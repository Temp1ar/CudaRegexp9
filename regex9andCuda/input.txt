  The Linux 3Dfx HOWTO
  Bernd Kreimeier (bk@gamers.org)
  v1.16, 6 February 1998

  This document describes 3Dfx graphics accelerator chip support for
  Linux. It lists some supported hardware, describes how to configure
  the drivers, and answers frequently asked questions.
  ______________________________________________________________________

  Table of Contents



  1. Introduction

     1.1 Contributors and Contacts
     1.2 Acknowledgments
     1.3 Revision History
     1.4 New versions of this document
     1.5 Feedback
     1.6 Distribution Policy

  2. Graphics Accelerator Technology

     2.1 Basics
     2.2 Hardware configuration
     2.3 A bit of Voodoo Graphics (tm) architecture

  3. Installation

     3.1 Installing the board
        3.1.1 Troubleshooting the hardware installation
        3.1.2 Configuring the kernel
        3.1.3 Configuring devices
     3.2 Setting up the Displays
        3.2.1 Single screen display solution
        3.2.2 Single screen dual cable setup
        3.2.3 Dual screen display solution
     3.3 Installing the Glide distribution
        3.3.1 Using the detect program
        3.3.2 Using the test programs

  4. Answers To Frequently Asked Questions

  5. FAQ: Requirements?

     5.1 What are the system requirements?
     5.2 Does it work with Linux-Alpha?
     5.3 Which 3Dfx chipsets are supported?
     5.4 Is the Voodoo Rush (tm) supported?
     5.5 Which boards are supported?
     5.6 How do boards differ?
     5.7 What about AGP?

  6. FAQ: Voodoo Graphics (tm)? 3Dfx?

     6.1 Who is 3Dfx?
     6.2 Who is Quantum3D?
     6.3 What is the Voodoo Graphics (tm)?
     6.4 What is the Voodoo Rush (tm)?
     6.5 What is the Voodoo 2 (tm)?
     6.6 What is VGA pass-though?
     6.7 What is Texelfx or TMU?
     6.8 What is a Pixelfx unit?
     6.9 What is SLI mode?
     6.10 Is there a single board SLI setup?
     6.11 How much memory? How many buffers?
     6.12 Does the Voodoo Graphics (tm) do 24 or 32 bit color?
     6.13 Does the Voodoo Graphics (tm) store 24 or 32 bit z-buffer per pixel?
     6.14 What resolutions does the Voodoo Graphics (tm) support?
     6.15 What texture sizes are supported?
     6.16 Does the Voodoo Graphics (tm) support paletted textures?
     6.17 What about overclocking?
     6.18 Where could I get additional info on Voodoo Graphics (tm)?

  7. FAQ: Glide? TexUS?

     7.1 What is Glide anyway?
     7.2 What is TexUS?
     7.3 Is Glide freeware?
     7.4 Where do I get Glide?
     7.5 Is the Glide source available?
     7.6 Is Linux Glide supported?
     7.7 Where could I post Glide questions?
     7.8 Where to send bug reports?
     7.9 Who is maintaining it?
     7.10 How can I contribute to Linux Glide?
     7.11 Do I have to use Glide?
     7.12 Should I program using the Glide API?
     7.13 What is the Glide current version?
     7.14 Does it support multiple Texelfx already?
     7.15 Is Linux Glide identical to DOS/Windows Glide?
     7.16 Where to I get information on Glide?
     7.17 Where to get some Glide demos?
     7.18 What is ATB?

  8. FAQ: Glide and XFree86?

     8.1 Does it run with XFree86?
     8.2 Does it only run full screen?
     8.3 What is the problem with AT3D/Voodoo Rush (tm) boards?
     8.4 What about GLX for XFree86?
     8.5 Glide and commerical X Servers?
     8.6 Glide and SVGA?
     8.7 Glide and GGI?

  9. FAQ: OpenGL/Mesa?

     9.1 What is OpenGL?
     9.2 Where to get additional information on OpenGL?
     9.3 Is Glide an OpenGL implementation?
     9.4 Is there an OpenGL driver from 3Dfx?
     9.5 Is there a commercial OpenGL for Linux and 3Dfx?
     9.6 What is Mesa?
     9.7 Does Mesa work with 3Dfx?
     9.8 How portable is Mesa with Glide?
     9.9 Where to get info on Mesa?
     9.10 Where to get information on Mesa Voodoo?
     9.11 Does Mesa support multitexturing?
     9.12 Does Mesa support single pass trilinear mipmapping?
     9.13 What is the Mesa "Window Hack"?
     9.14 How about GLUT?

  10. FAQ: But Quake?

     10.1 What about that 3Dfx GL driver for Quake?
     10.2 Is there a 3Dfx based glQuake for Linux?
     10.3 Does glQuake run in an XFree86 window?
     10.4 Known Linux Quake problems?
     10.5 Know Linux Quake security problems?
     10.6 Does LinuxQuake use multitexturing?
     10.7 Where can I get current information on Linux glQuake?

  11. FAQ: Troubleshooting?

     11.1 Has this hardware been tested?
     11.2 Failed to change I/O privilege?
     11.3 Does it work without root privilege?
     11.4 Displayed images looks awful (single screen)?
     11.5 The last frame is still there (single or dual screen)?
     11.6 Powersave kicks in (dual screen)?
     11.7 My machine seem to lock (X11, single screen)?
     11.8 My machine locks (single or dual screen)?
     11.9 My machine locks (used with S3 VGA board)?
     11.10 No address conflict, but locks anyway?
     11.11 Mesa runs, but does not access the board?
     11.12 Resetting dual board SLI?
     11.13 Resetting single board SLI?


  ______________________________________________________________________



  1.  Introduction

  This is the Linux 3Dfx HOWTO document. It is intended as a quick
  reference covering everything you need to know to install and
  configure 3Dfx support under Linux. Frequently asked questions
  regarding the 3Dfx support are answered, and references are given to
  some other sources of information on a variety of topics related to
  computer generated, hardware accelerated 3D graphics.

  This information is only valid for Linux on the Intel platform.  Some
  information may be applicable to other processor architectures, but I
  have no first hand experience or information on this. It is only
  applicable to boards based on 3Dfx technology, any other graphics
  accelerator hardware is beyond the scope of this document.



  1.1.  Contributors and Contacts

  This document would not have been possible without all the information
  contributed by other people - those involved in the Linux Glide port
  and the beta testing process, in the development of Mesa and the Mesa
  Voodoo drivers, or rewieving the document on behalf of 3Dfx and
  Quantum3D.  Some of them contributed entire sections to this document.

  Daryll Strauss daryll@harlot.rb.ca.us did the port, Paul J. Metzger
  pjm@rbd.com modified the Mesa Voodoo driver (written by David
  Bucciarelli tech.hmw@plus.it) for Linux, Brian Paul brianp@RA.AVID.COM
  integrated it with his famous Mesa library. With respect to Voodoo
  Graphics (tm) accelerated Mesa, additional thanks has to go to Henri
  Fousse, Gary McTaggart, and the maintainer of the 3Dfx Mesa for DOS,
  Charlie Wallace Charlie.Wallace@unistudios.com.  The folks at 3Dfx,
  notably Gary Sanders, Rod Hughes, and Marty Franz, provided valuable
  input, as did Ross Q. Smith of Quantum3D. The pages on the Voodoo
  Extreme and Operation 3Dfx websites provided useful info as well, and
  in some case I relied on the 3Dfx local Newsgroups. The Linux glQuake2
  port that uses Linux Glide and Mesa is maintained by Dave Kirsch
  zoid@idsoftware.com.  Thanks to all those who sent e-mail regarding
  corrections and updates, and special thanks to Mark Atkinson for
  reminding me of the dual cable setup.

  Thanks to the SGML-Tools package (formerly known as Linuxdoc-SGML),
  this HOWTO is available in several formats, all generated from a
  common source file. For information on SGML-Tools see its homepage at
  pobox.com/~cg/sgmltools.



  1.2.  Acknowledgments

  3Dfx, the 3Dfx Interactive logo, Voodoo Graphics (tm), and Voodoo Rush
  (tm) are registered trademarks of 3Dfx Interactive, Inc.  Glide,
  TexUS, Pixelfx and Texelfx are trademarks of 3Dfx Interactive, Inc.
  OpenGL is a registered trademark of Silicon Graphics. Obsidian is a
  trademark of Quantum3D.  Other product names are trademarks of the
  respective holders, and are hereby considered properly acknowledged.


  1.3.  Revision History


     Version 1.03
        First version for public release.

     Version 1.16
        Current version v1.16 6 February 1998.



  1.4.  New versions of this document

  You will find the most recent version of this document at
  www.gamers.org/dEngine/xf3D/.

  New versions of this document will be periodically posted to the
  comp.os.linux.answers newsgroup. They will also be uploaded to various
  anonymous ftp sites that archive such information including
  ftp://sunsite.unc.edu/pub/Linux/docs/HOWTO/.

  Hypertext versions of this and other Linux HOWTOs are available on
  many World-Wide-Web sites, including sunsite.unc.edu/LDP/. Most Linux
  CD-ROM distributions include the HOWTOs, often under the
  /usr/doc/directory, and you can also buy printed copies from several
  vendors.

  If you make a translation of this document into another language, let
  me know and I'll include a reference to it here.



  1.5.  Feedback

  I rely on you, the reader, to make this HOWTO useful. If you have any
  suggestions, corrections, or comments, please send them to me (
  bk@gamers.org), and I will try to incorporate them in the next
  revision.  Please add HOWTO 3Dfx to the Subject-line of the mail, so
  procmail will dump it in the appropriate folder.

  Before sending bug reports or questions, please read all of the
  information in this HOWTO, and send detailed information about the
  problem.

  If you publish this document on a CD-ROM or in hardcopy form, a
  complimentary copy would be appreciated. Mail me for my postal
  address. Also consider making a donation to the Linux Documentation
  Project to help support free documentation for Linux. Contact the
  Linux HOWTO coordinator, Tim Bynum (linux-howto@sunsite.unc.edu), for
  more information.



  1.6.  Distribution Policy

  Copyright (c) 1997, 1998 by Bernd Kreimeier.  This document may be
  distributed under the terms set forth in the LDP license at
  sunsite.unc.edu/LDP/COPYRIGHT.html.

  This HOWTO is free documentation; you can redistribute it and/or
  modify it under the terms of the LDP license.  This document is
  distributed in the hope that it will be useful, but without any
  warranty; without even the implied warranty of merchantability or
  fitness for a particular purpose.  See the LDP license for more
  details.



  2.  Graphics Accelerator Technology

  2.1.  Basics

  This section gives a very cursory overview of computer graphics
  accelerator technology, in order to help you understand the concepts
  used later in the document. You should consult e.g.  a book on OpenGL
  in order to learn more.


  2.2.  Hardware configuration

  Graphics accelerators come in different flavors: either as a separate
  PCI board that is able to pass through the video signal of a (possibly
  2D or video accelerated) VGA board, or as a PCI board that does both
  VGA and 3D graphics (effectively replacing older VGA controllers).
  The 3Dfx boards based on the Voodoo Graphics (tm) belong to the former
  category. We will get into this again later.


  If there is no address conflict, any 3D accelerator board could be
  present under Linux without interfering, but in order to access the
  accelerator, you will need a driver. A combined 2D/3D accelerator
  might behave differently.


  2.3.  A bit of Voodoo Graphics (tm) architecture

  Usually, accessing texture memory and frame/depth buffer is a major
  bottleneck. For each pixel on the screen, there are at least one
  (nearest), four (bi-linear), or eight (tri-linear mipmapped) read
  accesses to texture memory, plus a read/write to the depth buffer, and
  a read/write to frame buffer memory.

  The Voodoo Graphics (tm) architecture separates texture memory from
  frame/depth buffer memory by introducing two separate rendering
  stages, with two corresponding units (Pixelfx and Texelfx), each
  having a separate memory interface to dedicated memory. This gives an
  above-average fill rate, paid for restrictions in memory management
  (e.g. unused framebuffer memory can not be used for texture caching).

  Moreover, a Voodoo Graphics (tm) could use two TMU's (texture
  management or texelfx units), and finally, two Voodoo Graphics (tm)
  could be combined with a mechanism called Scan-Line Interleaving
  (SLI). SLI essentially means that each Pixelfx unit effectively
  provides only every other scanline, which decreases bandwidth impact
  on each Pixelfx' framebuffer memory.



  3.  Installation

  Configuring Linux to support 3Dfx accelerators involves the following
  steps:

  1. Installing the board.

  2. Installing the Glide distribution.

  3. Compiling, linking and/or running the application.

  The next sections will cover each of these steps in detail.


  3.1.  Installing the board

  Follow the manufacturer's instructions for installing the hardware or
  have your dealer perform the installation.  It should not be necessary
  to select settings for IRQ, DMA channel, either Plug&Pray (tm) or
  factory defaults should work. The add-on boards described here are
  memory mapped devices and do not use IRQ's. The only kind of conflict
  to avoid is memory overlap with other devices.

  As 3Dfx does not develop or sell any boards, do not contact them on
  any problems.


  3.1.1.  Troubleshooting the hardware installation

  To check the installation and the memory mapping, do cat /proc/pci.
  The output should contain something like

  ______________________________________________________________________
    Bus  0, device  12, function  0:
      VGA compatible controller: S3 Inc. Vision 968 (rev 0).
        Medium devsel.  IRQ 11.
        Non-prefetchable 32 bit memory at 0xf4000000.

    Bus  0, device   9, function  0:
      Multimedia video controller: Unknown vendor Unknown device (rev 2).
        Vendor id=121a. Device id=1.
        Fast devsel.  Fast back-to-back capable.
        Prefetchable 32 bit memory at 0xfb000000.
  ______________________________________________________________________


  for a Diamond Monster 3D used with a Diamond Stealth-64. Additionally
  a cat /proc/cpuinfo /proc/meminfo might be helpfull for tracking down
  conflicts and/or submitting a bug report.

  With current kernels, you will probably get a boot warning like

  ______________________________________________________________________
  Jun 12 12:31:52 hal kernel: Warning : Unknown PCI device (121a:1).
  Please read include/linux/pci.h
  ______________________________________________________________________


  which could be safely ignored. If you happen to have a board not very
  common, or have encountered a new revision, you should take the time
  to follow the advice in /usr/include/linux/pci.h and send all neces-
  sary information to linux-pcisupport@cao-vlsi.ibp.fr.

  If you experience any problems with the board, you should try to
  verify that DOS and/or Win95 or NT support works. You will probably
  not receive any useful response from a board manufacturer on a bug
  report or request regarding Linux. Having dealt with the Diamond
  support e-mail system, I would not expect useful responses for other
  operating systems either.


  3.1.2.  Configuring the kernel

  There is no kernel configuration necessary, as long as PCI support is
  enabled.  The Linux Kernel HOWTO
  <http://sunsite.unc.edu/mdw/HOWTO/Kernel-HOWTO.html> should be
  consulted for the details of building a kernel.



  3.1.3.  Configuring devices

  The current drivers do not (yet) require any special devices.  This is
  different from other driver developments (e.g. the sound drivers,
  where you will find a /dev/dsp and /dev/audio). The driver uses the
  /dev/mem device which should always be available. In consequence, you
  need to use setuid or root privileges to access the accelerator board.


  3.2.  Setting up the Displays

  There are two possible setups with add-on boards. You could either
  pass-through the video signal from your regular VGA board via the
  accelerator board to the display, or you could use two displays at the
  same time.  Rely to the manual provided by the board manufacturer for
  details. Both configurations have been tried with the Monster 3D
  board.


  3.2.1.  Single screen display solution

  This configuration allows you to check basic operations of the
  accelerator board - if the video signal is not transmitted to the
  display, hardware failure is possible.

  Beware that the video output signal might deteoriate significantly if
  passed through the video board. To a degree, this is inevitable.
  However, reviews have complained about below-average of the cables
  provided e.g. with the Monster 3D, and judging from the one I tested,
  this has not changed.

  There are other pitfalls in single screen configurations.  Switching
  from the VGA display mode to the accelerated display mode will change
  resolution and refresh rate as well, even if you are using 640x480
  e.g. with X11, too.  Moreover, if you are running X11, your
  application is responsible for demanding all keyboard and mouse
  events, or you might get stuck because of changed scope and exposure
  on the X11 display (that is effectively invisible when the accelerated
  mode is used) You could use SVGA console mode instead of X11.

  If you are going to use a single screen configuration and switch modes
  often, remember that your monitor hardware might not enjoy this kind
  of use.



  3.2.2.  Single screen dual cable setup

  Some high end monitors (e.g. the EIZO F-784-T) come with two
  connectors, one with 5 BNC connectors for RGB, HSync, VSync, the other
  e.g. a regular VGA or a 13W3 Sub-D VGA.  These displays usually also
  feature a front panel input selector to safely switch from one to the
  other. It is thus possible to use e.g. a VGA-to-BNC cable with your
  high end 2D card, and a VGA-to-13W3 Sub-D cable with your 3Dfx, and
  effectively run dual screen on one display.


  3.2.3.  Dual screen display solution

  The accelerator board does not need the VGA input signal.  Instead of
  routing the common video output through the accelerator board, you
  could attach a second monitor to its output, and use both at the same
  time. This solution is more expensive, but gives best results, as your
  main display will still be hires and without the signal quality losses
  involved in a pass-through solution. In addition, you could use X11
  and the accelerated full screen display in parallel, for development
  and debugging.

  A common problem is that the accelerator board will not provide any
  video signal when not used. In consequence, each time the graphics
  application terminates, the hardware screensave/powersave might kick
  in depending on your monitors configuration. Again, your hardware
  might not enjoy being treated like this. You should use

  ______________________________________________________________________
  setenv SST_DUALSCREEN 1
  ______________________________________________________________________


  to force continued video output in this setup.


  3.3.  Installing the Glide distribution

  The Glide driver and library are provided as a single compressed
  archive. Use tar and gzip to unpack, and follow the instructions in
  the README and INSTALL accompanying the distribution.  Read the
  install script and run it. Installation puts everything in
  /usr/local/glide/include,lib,bin and sets the ld.conf to look there.
  Where it installs and setting ld.conf are independent actions. If you
  skip the ld.conf step then you need the LD_LIBRARY_PATH.

  You will need to install the header files in a location available at
  compile time, if you want to compile your own graphics applications.
  If you do not want to use the installation as above (i.e. you insist
  on a different location), make sure that any application could access
  the shared libary at runtime, or you will get a response like can't
  load library 'libglide.so'.



  3.3.1.  Using the detect program

  There is a bin/detect program in the distribution (the source is not
  available). You have to run it as root, and you will get something
  like

  ______________________________________________________________________
  slot  vendorId   devId   baseAddr0  command  description
  ----  --------  ------  ----------  -------  -----------
    00    0x8086  0x122d  0x00000000   0x0006  Intel:430FX (Triton)
    07    0x8086  0x122e  0x00000000   0x0007  Intel:ISA bridge
    09    0x121a  0x0001  0xfb000008   0x0002  3Dfx:video multimedia adapter
    10    0x1000  0x0001  0x0000e401   0x0007  ???:SCSI bus controller
    11    0x9004  0x8178  0x0000e001   0x0017  Adaptec:SCSI bus controller
    12    0x5333  0x88f0  0xf4000000   0x0083  S3:VGA-compatible display co
  ______________________________________________________________________


  as a result. If you do not have root privileges, the program will bail
  out with

  ______________________________________________________________________
  Permission denied: Failed to change I/O privilege. Are you root?
  ______________________________________________________________________


  output might come handy for a bug report as well.



  3.3.2.  Using the test programs

  Within the Glide distribution, you will find a folder with test
  programs. Note that these test programs are under 3Dfx copyright, and
  are legally available for use only if you have purchased a board with
  a 3Dfx chipset. See the LICENSE file in the distribution, or their web
  site www.3dfx.com for details.

  It is recommend to compile and link the test programs even if there
  happen to be binaries in the distribution. Note that some of the
  programs will requires some files like alpha.3df from the distribution
  to be available in the same folder.  All test programs use the 640x480
  screen resolution. Some will request a veriety of single character
  inputs, others will just state Press A Key To Begin Test. Beware of
  loss of input scope if running X11 on the same screen at the same
  time.

  See the README.test for a list of programs, and other details.



  4.  Answers To Frequently Asked Questions

  The following section answers some of the questions that (will) have
  been asked on the Usenet news groups and mailing lists. The FAQ has
  been subdivided into several parts for convenience, namely

  o  FAQ: Requirements?

  o  FAQ: Voodoo Graphics (tm)? 3Dfx?

  o  FAQ: Glide?

  o  FAQ: Glide and SVGA?

  o  FAQ: Glide and XFree86?

  o  FAQ: Glide versus OpenGL/Mesa?

  o  FAQ: But Quake?

  o  FAQ: Troubleshooting?

     Each section lists several questions and answers, which will
     hopefully address most problems.



  5.  FAQ: Requirements?



  5.1.  What are the system requirements?

  A Linux PC, PCI 2.1 compliant, a monitor capable of 640x480, and a 3D
  accelerator board based on the 3Dfx Voodoo Graphics (tm). It will work
  on a P5 or P6, with or without MMX. The current version does not use
  MMX, but it has some optimized code paths for P6.

  At one point, some 3Dfx statements seemed to imply that using Linux
  Glide required using a RedHat distribution. Note that while Linux
  Glide has originally been ported in a RedHat 4.1 environment, it has
  been used and tested with many other Linux distributions, including
  homebrew, Slackware, and Debian 1.3.1.


  5.2.  Does it work with Linux-Alpha?

  There is currently no Linux Glide distribution available for any
  platform besides i586. As the Glide sources are not available for
  distribution, you will have to wait for the binary. Quantum3D has DEC
  Alpha support announced for 2H97. Please contact Daryll Strauss if you
  are interested in supporting this.

  There is also the issue of porting the the assembly modules. While
  there are alternative C paths in the code, the assembly module in
  Glide (essentially triangle setup) offered significant performance
  gains depending on the P5 CPU used.



  5.3.  Which 3Dfx chipsets are supported?

  Currently, the  3Dfx Voodoo Graphics (tm) chipset is supported under
  Linux. The Voodoo Rush (tm) chipset is not yet supported.


  5.4.  Is the Voodoo Rush (tm) supported?

  The current port of Glide to Linux does not support the Voodoo Rush
  (tm). An update is in the works.

  The problem is that at one point the Voodoo Rush (tm) driver code in
  Glide depended on Direct Draw. There was an SST96 based DOS portion in
  the library that could theoretically be used for Linux, as soon as all
  portions residing in the 2D/Direct Draw/D3D combo driver are replaced.

  Thus Voodoo Rush (tm) based boards like the Hercules Stingray 128/3D
  or Intergraph Intense Rush are not supported yet.



  5.5.  Which boards are supported?

  There are no officially supported boards, as 3Dfx does not sell any
  boards. This section does not attempt to list all boards, it will just
  give an overview, and will list only boards that have been found to
  cause trouble.

  It is important to recognize that Linux support for a given board does
  not only require a driver for the 3D accelerator component. If a board
  features its own VGA core as well, support by either Linux SVGA or
  XFree86 is required as well (see section about Voodoo Rush (tm)
  chipset).  Currently, an add-on solution is recommended, as it allows
  you to choose a regular graphics board well supported for Linux. There
  are other aspects discussed below.


  All Quantum3D Obsidian boards, independend of texture memory, frame
  buffer memory, number of Pixelfx and Texelfx units, and SLI should
  work. Same for all other Voodoo Graphics (tm) based boards, like
  Orchid Righteous 3D, Canopus Pure 3D, Flash 3D, and Diamond Monster
  3D.  Voodoo Rush (tm) based boards are not yet supported.

  Boards that are not based on 3Dfx chipsets (e.g. manufactured by S3,
  Matrox, 3Dlabs, Videologic) do not work with the 3Dfx drivers and are
  beyond the scope of this document.



  5.6.  How do boards differ?

  As the board manufacturers are using the same chipset, any differences
  are due to board design. Examples are quality of the pass-through
  cable and connectors (reportedly, Orchid provided better quality than
  Diamond), availability of a TV-compliant video signal output (Canopus
  Pure 3D), and, most notably, memory size on board.

  Most common were boards for games with 2MB texture cache and 2 MB
  framebuffer memory, however, the Canopus Pure3D comes with a maximal 4
  MB texture cache, which is an advantage e.g.  with games using
  dynamically changed textures, and/or illumation textures (Quake, most
  notably).  The memory architecture of a typical Voodoo Graphics (tm)
  board is described below, in a separate section.

  Quantum 3D offers the widest selection of 3Dfx-based boards, and is
  probably the place to go if you are looking for a high end Voodoo
  Graphics (tm) based board configuration.  Quantum 3D is addressing the
  visual simulation market, while most of the other vendors are only
  targetting the consumer-level PC-game market.



  5.7.  What about AGP?

  There is no Voodoo Graphics (tm) or Voodoo Rush (tm) AGP board that I
  am aware of. I am not aware of AGP support under Linux, and I do not
  know whether upcmong AGP boards using 3Dfx technology might possibly
  be supported with Linux.



  6.  FAQ: Voodoo Graphics (tm)? 3Dfx?

  6.1.  Who is 3Dfx?

  3Dfx is a San Jose based manufacturer of 3D graphics accelerator
  hardware for arcade games, game consoles, and PC boards.  Their
  official website is www.3dfx.com. 3Dfx does not sell any boards, but
  other companies do, e.g. Quantum3D.



  6.2.  Who is Quantum3D?

  Quantum3D started as a 3Dfx spin-off, manufacturing high end
  accelerator boards based on 3Dfx chip technology for consumer and
  business market, and supplying arcade game technology. See their home
  page at www.quantum3d.com for additional information. For general
  inquiries regarding Quantum3D, please send mail to info@quantum3d.


  6.3.  What is the Voodoo Graphics (tm)?

  The Voodoo Graphics (tm) is a chipset manufactured by 3Dfx. It is used
  in hardware acceleration boards for the PC.  See the HOWTO section on
  supported hardware.


  6.4.  What is the Voodoo Rush (tm)?

  The Voodoo Rush (tm) is a derivate of the Voodoo Graphics (tm) that
  has an interface to cooperate with a 2D VGA video accelerator,
  effectively supporting accelerated graphics in windows. This combo is
  currently not supported with Linux.


  6.5.  What is the Voodoo 2 (tm)?

  The Voodoo 2 (tm) is the successor of the Voodoo Graphics (tm)
  chipset, featuring several improvements. It is announced for late
  March 1998, and annoucements of Voodoo 2 (tm) based boards have been
  published e.g. by Quantum 3D, by Creative Labs, Orchid Technologies,
  and Diamond Multimedia.

  The Voodoo 2 (tm) is supposed to be backwards compatible.  However, a
  new version of Glide will have to be ported to Linux.



  6.6.  What is VGA pass-though?

  The Voodoo Graphics (tm) (but not the Voodoo Rush (tm)) boards are
  add-on boards, meant to be used with a regular 2D VGA video
  accelerator board. In short, the video output of your regular VGA
  board is used as input for the Voodoo Graphics (tm) based add-on
  board, which by default passes it through to the display also
  connected to the Voodoo Graphics (tm) board. If the Voodoo Graphics
  (tm) is used (e.g. by a game), it will disconnect the VGA input
  signal, switch the display to a 640x480 fullscreen mode with the
  refresh rate configured by SST variables and the application/driver,
  and generate the video signal itself. The VGA doesn't need to be aware
  of this, and won't be.

  This setup has several advantages: free choice of 2D VGA board, which
  is an issue with Linux, as XFree86 drivers aren't available for all
  chipsets and revisions, and a cost effective migration path to
  accelerated 3D graphics. It also has several disadvantages: an
  application using the Voodoo Graphics (tm) might not re-enable video
  output when crashing, and regular VGA video signal deteoriates in the
  the pass-through process.


  6.7.  What is Texelfx or TMU?

  Voodoo Graphics (tm) chipsets have two units. The first one interfaces
  the texture memory on the board, does the texture mapping, and
  ultimately generates the input for the second unit that interfaces the
  framebuffer. This one is called Texelfx, aka Texture Management Unit,
  aka TMU. The neat thing about this is that a board can use two Texelfx
  instead of only one, like some of the Quantum3D Obsidian boards did,
  effectively doubling the processing power in some cases, depending on
  the application.

  As each Texelfx can address 4MB texture memory, a dual Texelfx setup
  has an effective texture cache of up to 8MB.  This can be true even if
  only one Texelfx is actually needed by a particular application, as
  textures can be distributed to both Texelfx, which are used depending
  on the requested texture. Both Texelfx are used together to perform
  certain operations as trilinear filtering and illumination
  texture/lightmap passes (e.g. in glQuake) in a single pass instead of
  the two passes that are required with only one Texelfx. To actually
  exploit the theoretically available speedup and cache size increase, a
  Glide application has to use both Texelfx properly.

  The two Texelfx can not be used separately to each draw a textured
  triangle at the same time. A triangle is always drawn using whatever
  the current setup is, which can be to use both Texelfx for a single
  pass operation combining two textures, or one Texelfx for only a
  single texture. Each Texelfx can only access its own memory.



  6.8.  What is a Pixelfx unit?

  Voodoo Graphics (tm) chipsets have two units. The second one
  interfaces the framebuffer and ultimately generates the depth buffer
  and pixel color updates. This one is called Pixelfx. The neat thing
  here is that two Pixelfx units can cooperate in SLI mode, like with
  some of the Quantum3D Obsidian boards, effectively doubling the frame
  rate.



  6.9.  What is SLI mode?

  SLI means "Scanline Interleave". In this mode, two Pixelfx are
  connected and render in alternate turns, one handling odd, the other
  handling even scanlines of the actual output.  Inthis mode, each
  Pixelfx stores only half of the image and half of the depth buffer
  data in its own local framebuffer, effectively doubling the number of
  pixels.

  The Pixelfx in question can be on the same board, or on two boards
  properly connected. Some Quantum3D Obsidian boards support SLI with
  Voodoo Graphics (tm).

  As two cards can decode the same PCI addresses and receive the same
  data, there is not necessarily additional bus bandwidth required by
  SLI. On the other hand, texture data will have to be replicated on
  both boards, thus the amount of texture memory effectively stays the
  same.



  6.10.  Is there a single board SLI setup?

  There are now two types of Quantum3D SLI boards.  The intial setup
  used two boards, two PCI slots, and an interconnect (e.g. the Obsidian
  100-4440).  The later revision which performs identically is contained
  on one full-length PCI board (e.g.  Obsidian 100-4440SB). Thus a
  single board SLI solution is possible, and has been done.



  6.11.  How much memory? How many buffers?

  The most essential difference between different boards using the
  Voodoo Graphics (tm) chipset is the amount and organization of memory.
  Quantum3D used a three digit scheme to descibe boards. Here is a
  slightly modifed one (anticipating Voodoo 2 (tm)). Note that if you
  use more than one Texelfx, they need the same amount of texture cache
  memory each, and if you combine two Pixelfx, each needs the same
  amount of frame buffer memory.
  ______________________________________________________________________
      "SLI / Pixelfx / Texelfx1 / Texelfx2 "
  ______________________________________________________________________


  It means that a common 2MB+2MB board would be a 1/2/2/0 solution, with
  the minimally required total 4Mb of memory. A Canopus Pure 3D would be
  1/2/4/0, or 6MB. An Obsidian-2220 board with two Texelfx would be
  1/2/2/2, and an Obsidian SLI-2440 board would be 2/2/4/4.  A fully
  featured dual board solution (2 Pixelfx, each with 2 Texelfx and 4MB
  frame buffer, each Texelfx 4 MB texture cache) would be 2/4/4/4, and
  the total amount of memory would be SLI*(Pixelfx+Texelfx1+Texelfx2),
  or 24 MB.

  So there.


  6.12.  Does the Voodoo Graphics (tm) do 24 or 32 bit color?

  No. The Voodoo Graphics (tm) architecture uses 16bpp internally.  This
  is true for  Voodoo Graphics (tm), Voodoo Rush (tm) and Voodoo 2 (tm)
  alike. Quantum3D claims to implement 22-bpp effective color depth with
  an enhanced 16-bpp frame buffer, though.


  6.13.  Does the Voodoo Graphics (tm) store 24 or 32 bit z-buffer per
  pixel?

  No. The Voodoo Graphics (tm) architecture uses 16bpp internally for
  the depth buffer, too. This again is true for  Voodoo Graphics (tm),
  Voodoo Rush (tm) and Voodoo 2 (tm) alike. Again, Quantum3D claims that
  using the floating point 16-bits per pixel (bpp) depth buffering
  provides 22-bpp effective Z-buffer precision.


  6.14.  What resolutions does the Voodoo Graphics (tm) support?

  The Voodoo Graphics (tm) chipset supports up to 4 MB frame buffer
  memory. Presuming double buffering and a depth buffer, a 2MB
  framebuffer will support a resolution of 640x480.  With 4 MB frame
  buffer, 800x600 is possible.

  Unfortunately 960x720 is not supported. The Voodoo Graphics (tm)
  chipset requires that the amount of memory for a particular resolution
  must be such that the vertical and horizontal resolutions must be
  evenly divisible by 32. The video refresh controller, though can
  output any particular resolution, but the "virtual" size required for
  the memory footprint must be in dimensions evenly divisible by 32.
  So, 960x720 actually requires 960x736 amount of memory, and
  960x736x2x3 = 4.04MBytes.

  However, using two boards with SLI, or a dual Pixelfx SLI board means
  that each framebuffer will only have to store half of the image. Thus
  2 times 4 MB in SLI mode are good up to 1024x768, which is the maximum
  because of the overall hardware design. You will be able to do
  1024x768 tripled buffered with Z, but you will not be able to do e.g.
  1280x960 with double buffering.

  Note that triple buffering (no VSync synchonization required by the
  application), stereo buffering (for interfacing LCD shutters) and
  other more demanding setups will severely decrease the available
  resolution.



  6.15.  What texture sizes are supported?

  The maximum texture size for the Voodoo Graphics (tm) chipset is
  256x256, and you have to use powers of two. Note that for really small
  textures (e.g. 16x16) you are better off merging them into a large
  texture, and adjusting your effective texture coordinates
  appropriately.


  6.16.  Does the Voodoo Graphics (tm) support paletted textures?

  The Voodoo Graphics (tm) hardware and Glide support the palette
  extension to OpenGL. The most recent version of Mesa does support the
  GL_EXT_paletted_texture and GL_EXT_shared_texture_palette extensions.



  6.17.  What about overclocking?

  If you want to put aside considerations about warranty and
  overheating, and want to do overclocking to boost up performance even
  further, there is related info out on the web. The basic mechanism is
  to use Glide environment variables to adjust the clock.

  Note that the actual recommended clock is board dependend. While the
  default clock speed is 50 Mhz, the Diamond Monster 3D property sheet
  lets you set up a clock of 57 MHz. It all comes down to the design of
  a specific board, and which components are used with the Voodoo
  Graphics (tm) chipset - most notably access speed of the RAM in
  question. If you exceed the limits of your hardware, rendering
  artifacts will occur to say the least. Reportedly, 57 MHz usually
  works, while 60 MHz or more is already pushing it.

  Increasing the clock frequency also means increasing the waste heat
  disposed in the chips, in a nonlinear dependency (10% increase in
  frequency means a lot larger increase in heating). In consequence, for
  permanent overclocking you might want to educate yourself about ways
  to  add cooling fans to the board in a way that does not affect
  warranty. A very recommendable source is the "3Dfx Voodoo Heat Report"
  by Eric van Ballegoie, available on the web.



  6.18.  Where could I get additional info on Voodoo Graphics (tm)?

  There is a FAQ by 3Dfx, which should be available at their web site.
  You will find retail information at the following locations:
  www.3dfx.com and www.quantum3d.com.

  Inofficial sites that have good info are "Voodoo Extreme" at
  www.ve3d.com, and "Operation 3Dfx" at www.ve3d.com.



  7.  FAQ: Glide? TexUS?

  7.1.  What is Glide anyway?

  Glide is a proprietary API plus drivers to access 3D graphics
  accelerator hardware based on chipsets manufactured by 3Dfx. Glide has
  been developed and implemented for DOS, Windows, and Macintosh, and
  has been ported to Linux by Daryll Strauss.



  7.2.  What is TexUS?

  In the distribution is a libtexus.so, which is the 3Dfx Interactive
  Texture Utility Software.  It is an image processing libary and
  utility program for preparing images for use with the 3Dfx Interactive
  Glide library. Features of TexUS include file format conversion,
  MIPmap creation, and support for 3Dfx Interactive Narrow Channel
  Compression textures.

  The TexUS utility program texus reads images in several popular
  formats (TGA, PPM, RGT), generates MIPmaps, and writes the images as
  3Dfx Interactive textures files (see e.g. alpha.3df, as found in the
  distribution) or as an image file for inspection. For details on the
  parameters for texus, and the API, see the TexUS documentation.



  7.3.  Is Glide freeware?

  Nope. Glide is neither GPL'ed nor subject to any other public license.
  See LICENSE in the distribution for any details. Effectively, by
  downloading and using it, you agree to the End User License Agreement
  (EULA) on the 3Dfx web site. Glide is provided as binary only, and you
  should neither use nor distribute any files but the ones released to
  the public, if you have not signed an NDA. The Glide distribution
  including the test program sources are copyrighted by 3Dfx.

  The same is true for all the sources in the Glide distribution. In the
  words of 3Dfx: These are not public domain, but they can be freely
  distributed to owners of 3Dfx products only.  No card, No code!


  7.4.  Where do I get Glide?

  The entire 3Dfx SDK is available for download off their public web-
  site located at www.3dfx.com/software/download_glide.html. Anything
  else 3Dfx publicly released by 3Dfx is nearby on their website, too.

  There is also an FTP site, ftp.3dfx.com. The FTP has a longer timeout,
  and some of the larger files have been broken into 3 files (approx.
  3MB each).



  7.5.  Is the Glide source available?

  Nope. The Glide source is made available only based on a special
  agreement and NDA with 3Dfx.


  7.6.  Is Linux Glide supported?

  Currently, Linux Glide is unsupported. Basically, it is provided under
  the same disclaimers as the 3Dfx GL DLL (see below).

  However, 3Dfx definitely wants to provide as much support as possible,
  and is in the process of setting up some prerequisites. For the time
  being, you will have to rely on the 3Dfx newsgroup (see below).

  In addition, the Quantum3D web page claims that Linux support (for
  Obsidian) is planned for both Intel and AXP architecture systems in
  2H97.



  7.7.  Where could I post Glide questions?

  There are newsgroups currently available only on the NNTP server
  news.3dfx.com run by 3Dfx.  This USENET groups are dedicated to 3Dfx
  and Glide in general, and will mainly provide assistance for DOS,
  Win95, and NT. The current list includes:

  ______________________________________________________________________
  3dfx.events
  3dfx.games.glquake
  3dfx.glide
  3dfx.glide.linux
  3dfx.products
  3dfx.test
  ______________________________________________________________________


  and the 3dfx.oem.products.* group for specific boards, eg.
  3dfx.oem.products.quantum3d.obsidian.  Please use
  news.3dfx.com/3dfx.glide.linux for all Lnux Glide related questions.

  A mailing list dedicated to Linux Glide is in preparation for 1Q98.
  Send mail to majordomo@gamers.org, no subject, body of the message
  info linux-3dfx to get information about the posting guidelines, the
  hypermail archive and how to subscribe to the list or the digest.



  7.8.  Where to send bug reports?

  Currently, you should rely on the newsgroup (see above), that is
  news.3dfx.com/3dfx.glide.linux.  There is no official support e-mail
  set up yet.  For questions not specific to Linux Glide, make sure to
  use the other newsgroups.


  7.9.  Who is maintaining it?

  3Dfx will appoint an official maintainer soon.  Currently, inofficial
  maintainer of the Linux Glide port is Daryll Strauss. Please post bug
  reports in the newsgroup (above). If you are confident that you found
  a bug not previously reported, please mail to Daryll at
  daryll@harlot.rb.ca.us


  7.10.  How can I contribute to Linux Glide?

  You could submit precise bug reports. Providing sample programs to be
  included in the distribution is another possibility. A major
  contribution would be adding code to the Glide based Mesa Voodoo
  driver source.  See section on Mesa Voodoo below.



  7.11.  Do I have to use Glide?

  Yes. As of now, there is no other Voodoo Graphics (tm) driver
  available for Linux. At the lowest level, Glide is the only interface
  that talks directly to the hardware. However, you can write OpenGL
  code without knowing anything about Glide, and use Mesa with the Glide
  based Mesa Voodoo driver.  It helps to be aware of the involvement of
  Glide for recognizing driver limitations and bugs, though.



  7.12.  Should I program using the Glide API?

  That depends on the application you are heading for.  Glide is a
  proprietary API that is partly similar to OpenGL or Mesa, partly
  contains features only available as EXTensions to some OpenGL
  implementations, and partly contains features not available anywhere
  but within Glide.

  If you want to use the OpenGL API, you will need Mesa (see below).
  Mesa, namely the Mesa Voodoo driver, offers an API resembling the well
  documented and widely used OpenGL API. However, the Mesa Voodoo driver
  is in early alpha, and you will have to accept performance losses and
  lack of support for some features.

  In summary, the decision is up to you - if you are heading for maximum
  performance while accepting potential problems with porting to
  non-3Dfx hardware, Glide is not a bad choice. If you care about
  maintenance, OpenGL might be the best bet in the long run.



  7.13.  What is the Glide current version?

  The current version of Linux Glide is 2.4.  The next version will
  probably be identical to the current version for DOS/Windows, which is
  2.4.3, which comes in two distributions. Right now, various parts of
  Glide are different for Voodoo Rush (tm) (VR) and Voodoo Graphics (tm)
  (VG) boards. Thus you have to pick up separate distributions (under
  Windows) for VR and VG.  The same will be true for Linux. There will
  possibly be another chunk of code and another distribution for Voodoo
  2 (tm) (V2) boards.

  There is also a Glide 3.0 in preparation that will extend the API for
  use of triangle fans and triangle strips, and provide better state
  change optimization. Support for fans and strips will in some
  situations significantly reduce the amount of data sent ber triangle,
  and the Mesa driver will benefit from this, as the OpenGL API has
  separate modes for this. For a detailed explanation on this see e.g.
  the OpenGL documentation.



  7.14.  Does it support multiple Texelfx already?

  Multiple Texelfx/TMU's can be used for single pass trilinear
  mipmapping for improvement image quality without performance penalty
  in current Linux Glide already. You will need a board with two Texelfx
  (that is, one of the appropriate Quantum3D Obsidian boards). The
  application needs to specify the use of both Texelfx accordingly, it
  does not happen automatically.

  Note that because most applications are implemented for consumer
  boards with a single Texelfx, they might not query the presence of a
  second Texelfx, and thus not use it. This is not a flaw of Glide but
  of the application.



  7.15.  Is Linux Glide identical to DOS/Windows Glide?

  The publicly available version of Linux Glide should be identical to
  the respective DOS/Windows versions.  Delays in releasing the Linux
  port of newer DOS/Windows releases are possible.


  7.16.  Where to I get information on Glide?

  There is exhaustive information available from 3Dfx. You could
  download it from their home page at
  www.3dfx.com/software/download_glide.html.  These are for free,
  presuming you bought a 3Dfx hardware based board. Please read the
  licensing regulations.

  Basically, you should look for some of the following:

  o  Glide Release Notes

  o  Glide Programming Guide

  o  Glide Reference Manual

  o  Glide Porting Guide

  o  TexUs Texture Utility Software

  o  ATB Release Notes

  o  Installing and Using the Obsidian

     These are available as Microsoft Word documents, and part of the
     Windows Glide distribution, i.e.  the self-extracting archive file.
     Postscript copies for separate download should be available at
     www.3dfx.com as well. Note that the release numbers are not always
     in sync with those of Glide.



  7.17.  Where to get some Glide demos?

  You will find demo sources for Glide within the distribution (test
  programs), and on the 3Dfx home page. The problem with the latter is
  that some require ATB. To port these demos to Linux, the event
  handling has to be completely rewritten.

  In addition, you might find useful some of the OpenGL demo sources
  accompanying Mesa and GLUT. While the Glide API is different from the
  OpenGL API, they target the same hardware rendering pipeline.



  7.18.  What is ATB?

  Some of the 3Dfx demo programs for Glide depend not only on Glide but
  also on 3Dfx's proprietary Arcade Toolbox (ATB), which is available
  for DOS and Win32, but has not been ported for Linux. If you are a
  devleoper, the sources are available within the Total Immersion
  program, so porting ATB to Linux would be possible.



  8.  FAQ: Glide and XFree86?


  8.1.  Does it run with XFree86?

  Basically, the Voodoo Graphics (tm) hardware does not care about X.
  The X server will not even notice that the video signal generated by
  the VGA hardware does not reach the display in single screen
  configurations. If your application is not written X aware, Glide
  switching to full screen mode might cause problems (see
  troubleshooting section). If you do not want the overhead of writing
  an X11-aware application, you might want to use SVGA console mode
  instead.

  So yes, it does run with XFree86, but no, it is not cooperating if you
  don't write your application accordingly. You can use the Mesa "window
  hack", which will be significantly slower than fullscreen, but still a
  lot faster than software rendering (see section below).



  8.2.  Does it only run full screen?

  See above. The Voodoo Graphics (tm) hardware is not window environment
  aware, neither is Linux Glide. Again, the experimental Mesa "window
  hack" covered below will allow for pasting the Voodoo Graphics (tm)
  board framebuffer's content into an X11 window.



  8.3.  What is the problem with AT3D/Voodoo Rush (tm) boards?

  There is an inherent problem when using Voodoo Rush (tm) boards with
  Linux: Basically, these boards are meant to be VGA 2D/3D accelerator
  boards, either as a single board solution, or with a Voodoo Rush (tm)
  based daughterboard used transparently. The VGA component tied to the
  Voodoo Rush (tm) is a Alliance Semiconductor's ProMotion-AT3D
  multimedia accelerator.  To use this e.g. with XFree86 at all, you
  need a driver for the AT3D chipset.

  There is a mailing list on this, and a web site with FAQ at
  www.frozenwave.com/linux-stingray128.  Look there for most current
  info.  There is a SuSE maintained driver at
  ftp.suse.com/suse_update/special/xat3d.tgz.  Reportedly, the XFree86
  SVGA server also works, supporting 8, 16 and 32 bpp.  Official support
  will probably be in XFree86 4.0.  XFree86 decided to prepare an
  intermediate XFree86 3.3.2 release as well, which might already
  address the issues.

  The following XF86Config settings reportedly work.

  ______________________________________________________________________
  # device section settings
  Chipset "AT24"
  Videoram 4032

  # videomodes tested by Oliver Schaertel
  #  25.18  28.32  for 640 x 480   (70hz)
  #  61.60         for 1024 x 786  (60hz)
  #  120           for 1280 x 1024 (66hz)
  ______________________________________________________________________


  In summary, there is nothing prohibiting this except for the fact that
  the drivers in XFree86 are not yet finished.

  If you want a more technical explanation: Voodoo Rush (tm) support
  requires X server changes to support grabbing a buffer area in the
  video memory on the AT3D board, as the Voodoo Rush (tm) based boards
  need to store their back buffer and z buffer there. This  memory
  allocation and locking requirement is not a 3Dfx specific problem, it
  is also needed e.g. for support of TV capture cards, and is thus under
  active development for XFree86. This means changes at the device
  dependend X level (thus XAA), which are currently implemented as an
  extension to XFree86 DGA (Direct Graphics Access, an X11 extension
  proposal implemented in different ways by Sun and XFree86, that is not
  part of the final X11R6.1 standard and thus not portable). It might be
  part of an XFree86 GLX implementation later on. The currently
  distributed X servers assume they have full control of the
  framebuffer, and use anything that is not used by the visual region of
  the framebuffer as pixmap cache, e.g. for caching fonts.



  8.4.  What about GLX for XFree86?

  There are a couple of problems.

  The currently supported Voodoo Graphics (tm) hardware and the
  available revision of Linux Glide are full screen only, and not set up
  to share a framebuffer with a window environment. Thus GLX or other
  integration with X11 is not yet possible.

  The Voodoo Rush (tm) might be capable of cooperating with XFree86
  (that is, an SVGA compliant board will work with the XFree86 SVGA
  server), but it is not yet supported by Linux Glide, nor do S3 or
  other XFree86 servers support these boards yet.

  In addition, GLX is tied to OpenGL or, in the Linux case, to Mesa.
  The XFree86 team is currently working on integrating Mesa with their X
  Server. GLX is in beta, XFree86 3.3 has the hooks for GLX.  See Steve
  Parker's GLX pages at www.cs.utah.edu/~sparker/xfree86-3d/ for the
  most recent information.  Moreover, there is a joint effort by XFree86
  and SuSe, which includes a GLX, see www.suse.de/~sim/.  Currently,
  Mesa still uses its GLX emulation with Linux.



  8.5.  Glide and commerical X Servers?

  I have not received any mail regarding use of Glide and/or Mesa with
  commercial X Servers.  I would be interested to get confirmation on
  this, especially on Mesa and Glide with a commercial X Server that has
  GLX support.



  8.6.  Glide and SVGA?

  You should have no problems running Glide based applications either
  single or dual screen using VGA modes. It might be a good idea to set
  up the 640x480 resolution in the SVGA modes, too, if you are using a
  single screen setup.


  8.7.  Glide and GGI?

  A GGI driver for Glide is under development by Jon M. Taylor, but has
  not officially been released and was put on hold till completion of
  GGI 0.0.9. For information about GGI see synergy.caltech.edu/~ggi/.
  If you are adventurous, you might find the combination of XGGI (a GGI
  based X Server for XFree86) and GGI for Glide an interesting prospect.
  There is also a GGI driver interfacing the OpenGL API; tested with
  unaccelerated Mesa. Essentially, this means X11R6 running on a Voodoo
  Graphics (tm), using either Mesa or Glide directly.



  9.  FAQ: OpenGL/Mesa?



  9.1.  What is OpenGL?

  OpenGL is an immediate mode graphics programming API originally
  developed by SGI based on their previous proprietary Iris GL, and
  became in industry standard several years ago. It is defined and
  maintained by the Architectural Revision Board (ARB), an organization
  that includes members as SGI, IBM, and DEC, and Microsoft.

  OpenGL provides a complete feature set for 2D and 3D graphics
  operations in a pipelined hardware accelerated architecture for
  triangle and polygon rendering. In a broader sense, OpenGL is a
  powerful and generic toolset for hardware assisted computer graphics.



  9.2.  Where to get additional information on OpenGL?

  The official site for OpenGL maintained by the members of the ARB, is
  www.opengl.org,

  A most recommended site is Mark Kilgard's Gateway to OpenGL Info at
  reality.sgi.com/mjk_asd/opengl-links.html: it provides pointers to
  book, online manual pages, GLUT, GLE, Mesa, ports to several OS, tons
  of demos and tools.

  If you are interested in game programming using OpenGL, there is the
  OpenGL-GameDev-L@fatcity.com at Listserv@fatcity.com. Be warned, this
  is a high traffic list with very technical content, and you will
  probably prefer to use procmail to handle the 100 messages per day
  coming in. You cut down bandwidth using the SET OpenGL-GameDev-L
  DIGEST command. It is also not appropriate if you are looking for
  introductions.  The archive is handled by the ListServ software, use
  the INDEX OpenGL-GameDev-L and GET OpenGL-GameDev-L "filename"
  commands to get a preview before subscribing.



  9.3.  Is Glide an OpenGL implementation?

  No, Glide is a proprietary 3Dfx API which several features specific to
  the Voodoo Graphics (tm) and Voodoo Rush (tm). A 3Dfx OpenGL is in
  preparation (see below). Several Glide features would require
  EXTensions to OpenGL, some of which already found in other
  implementations (e.g. paletted textures).

  The closest thing to a hardware accelerated Linux OpenGL you could
  currently get is Brian Paul's Mesa along with David Bucciarelli's Mesa
  Voodoo driver (see below).



  9.4.  Is there an OpenGL driver from 3Dfx?

  Both the 3Dfx website and the Quantum3D website announced OpenGL for
  Voodoo Graphics (tm) to be available 4Q97.  The driver is currently in
  Beta, and accessible only to registered deverloper's under written
  Beta test agreement.

  A linux port has not been announced yet.



  9.5.  Is there a commercial OpenGL for Linux and 3Dfx?

  I am not aware of any third party commercial OpenGL that supports the
  Voodoo Graphics (tm). Last time I paid attention, neither MetroX nor
  XInside OpenGL did.



  9.6.  What is Mesa?

  Mesa is a free implementation of the OpenGL API, designed and written
  by Brian Paul, with contributions from many others. Its performance is
  competitive, and while it is not officially certified, it is an almost
  fully compliant OpenGL implementation conforming to the ARB
  specifications - more complete than some commercial products out,
  actually.



  9.7.  Does Mesa work with 3Dfx?

  The latest Mesa MesaVer; release works with Linux Glide 2.4. In fact,
  support was included in earlier versions, however, this driver is
  still under development, so be prepared for bugs and less than optimal
  performance. It is steadily improving, though, and bugs are usually
  fixed very fast.

  You will need to get the Mesa library archive from the
  iris.ssec.wisc.edu FTP site.  It is recommended to subscribe to the
  mailing list as well, especially when trying to track down bugs,
  hardware, or driver limitations. Make sure to get the most recent
  distribution. A Mesa-3.0 is in preparation.



  9.8.  How portable is Mesa with Glide?

  It is available for Linux and Win32, and any application based on Mesa
  will only have the usual system specific code, which should usually
  mean XWindows vs. Windows, or GLX vs. WGL. If you use e.g. GLUT or Qt,
  you should get away with any system specifics at all for virtually
  most applications. There are only a few issues (like sampling relative
  mouse movement) that are not adressed by the available portable GUI
  toolkits.

  Mesa/Glide is also available for DOS. The port which is 32bit DOS is
  maintained by Charlie Wallace and kept up to date with the main Mesa
  base. See www.geocities.com/~charlie_x/.for the most current releases.



  9.9.  Where to get info on Mesa?

  The Mesa home page is at www.ssec.wisc.edu/~brianp/Mesa.html.  There
  is an archive of the Mesa mailing list.  at www.iqm.unicamp.br/mesa/.
  This list is not specific to 3Dfx and Glide, but if you are interested
  in using 3Dfx hardware to accelerate Mesa, it is a good place to
  start.


  9.10.  Where to get information on Mesa Voodoo?

  For latest information on the Mesa Voodoo driver maintained by David
  Bucciarelli tech.hmw@plus.it see the home page at www-
  hmw.caribel.pisa.it/fxmesa/.

  9.11.  Does Mesa support multitexturing?

  Not yet (as of Mesa 2.6), but it is on the list.  In Mesa you will
  probably have to use the OpenGL EXT_multitexture extension once it is
  available. There is no final specification for multitextures in
  OpenGL, which is supposed to be part of the upcoming OpenGL 1.2
  revision. There might be a Glide driver specific implementation of the
  extension in upcoming Mesa releases, but as long as only certain
  Quantum3D Obsidian boards come with multiple TMU's, it is not a top
  priority. This will surely change once Voodoo 2 (tm) based boards are
  in widespread use.



  9.12.  Does Mesa support single pass trilinear mipmapping?

  Multiple TMU's should be used for single pass trilinear mipmapping for
  improvement image quality without performance penalty in current Linux
  Glide already. Mesa support is not yet done (as of Mesa 2.6), but is
  in preparation.



  9.13.  What is the Mesa "Window Hack"?

  The most recent revisions of Mesa contain an experimental feature for
  Linux XFree86. Basically, the GLX emulation used by Mesa copies the
  contents of the Voodoo Graphics (tm) board's most recently finished
  framebuffer content into video memory on each glXSwapBuffers call.
  This feature is also available with Mesa for Windows.

  This obviously puts some drain on the PCI, doubled by the fact that
  this uses X11 MIT SHM, not XFree86 DGA to access the video memory. The
  same approach could theoretically be used with e.g. SVGA. The major
  benefit is that you could use a Voodoo Graphics (tm) board for
  accelerated rendering into a window, and that you don't have to use
  the VGA passthrough mode (video output of the VGA board deteoriates in
  passing through, which is very visible with high end monitors like
  e.g. EIZO F784-T).

  Note that this experimental feature is NOT Voodoo Rush (tm) support by
  any means. It applies only to the Voodoo Graphics (tm) based boards.
  Moreover, you need to use a modified GLUT, as interfacing the window
  management system and handling the events appropriately has to be done
  by the application, it is not handled in the driver.

  Make really sure that you have enabled the following environment
  variables:

  ______________________________________________________________________
  export SST_VGA_PASS=1          # to stop video signal switching
  export SST_NOSHUTDOWN=1        # to stop video signal switching
  export MESA_GLX_FX="window"    # to initiate Mesa window mode
  ______________________________________________________________________


  If you manage to forget one of the SST variables, your VGA board will
  be shut off, and you will loose the display (but not the actual X). It
  is pretty hard to get that back being effectively blind.

  Finally, note that the libMesaGL.a (or .so) library can contain
  multiple client interfaces.  I.e. the GLX, OSMesa, and fxMesa (and
  even SVGAMesa) interfaces call all be compiled into the same
  libMesaGL.a. The client program can use any of them freely, even
  simultaneously if it's careful.



  9.14.  How about GLUT?

  Mark Kilgard's GLUT distribution is a very good place to get sample
  applications plus a lot of useful utilities.  You will find it at
  reality.sgi.com/mjk_asd/glut3/, and you should get it anyway. The
  current release is GLUT 3.6, and discussion on a GLUT 3.7 (aka
  GameGLUT) has begun. Note that Mark Kilgard has left SGI recently, so
  the archive might move some time this year - for the time being it
  will be kept at SGI.

  There is also a GLUT mailing list, glut@perp.com. Send mail to
  majordomo@perp.com, with the (on of the) following in the body of your
  email message:

  ______________________________________________________________________
     help
     info glut
     subscribe glut
     end
  ______________________________________________________________________



  As GLUT handles double buffers, windows, events, and other operations
  closely tied to hardware and operating system, using GLUT with Voodoo
  Graphics (tm) requires support, which is currently in development
  within GLX for Mesa. It already works for most cases.



  10.  FAQ: But Quake?

  10.1.  What about that 3Dfx GL driver for Quake?

  The 3Dfx Quake GL, aka mini-driver, aka miniport, aka Game GL, aka
  3Dfx GL alpha, implemented only a Quake-specific subset of OpenGL (see
  http://www.cs.unc.edu/~martin/3dfx.html for an inofficial list of
  supported code paths). It is not supported, and not updated anymore.
  It was a Win32 DLL (opengl32.dll) released by 3Dfx and was available
  for Windows only. This DLL is not, and will not be ported to Linux.


  10.2.  Is there a 3Dfx based glQuake for Linux?

  Yes. A Quake linuxquake v0.97 binary has been released based on Mesa
  with Glide. The Quake2 q2test binary for Linux and Voodoo Graphics
  (tm) has been made available as well.  A full Quake2 for Linux was
  released in January 1998, with linuxquake2-3.10. Dave "Zoid" Kirsch is
  the official maintainer of all Linux ports of Quake, Quakeworld, and
  Quake2, including all the recent Mesa based ports. Note that all Linux
  ports, including the Mesa based ones, are not officially supported by
  id Software.

  See ftp.idsoftware.com/idstuff/quake/unix/ for the latest releases.



  10.3.  Does glQuake run in an XFree86 window?

  A revision of Mesa and the Mesa-based Linux glQuake is in preparation.
  Mesa already does support this by GLX, but Linux glQuake does not use
  GLX.



  10.4.  Known Linux Quake problems?

  Here is an excerpt, as of January 7th, 1998. I omitted most stuff not
  specific to &3Dfx; hardware.

  o  You really should run Quake2 as root when using the SVGALib and/or
     GL renders. You don't have to run as root for the X11 refresh, but
     the modes on the mouse and sound devices must be read/writable by
     whatever user you run it as. Dedicated server requires no special
     permissions.

  o  X11 has some garbage on the screen when 'loading'. This is normal
     in 16bit color mode. X11 doesn't work in 24bit (TrueColor). It
     would be very slow in any case.

  o  Some people are experiencing crashes with the GL renderer. Make
     sure you install the libMesa that comes with Quake2! Older versions
     of libMesa don't work properly.

  o  If you are experience video 'lag' in the GL renderer (the frame
     rate feels like it's lagging behind your mouse movement) type
     "gl_finish 1" in the console. This forces update on a per frame
     basis.

  o  When running the GL renderer, make sure you have killed selection
     and/or gpm or the mouse won't work as they won't "release" it while
     Quake2 is running in GL mode.


  10.5.  Know Linux Quake security problems?

  As Dave Kirsch posted on January 28th, 1998: an exploit for Quake2
  under Linux has been published. Quake2 is using shared libraries.
  While the READMRE so far does not specifically mention it, note that
  Quake2 should not be setuid.

  If you want to use the ref_soft and ref_gl renderers, you should run
  Quake2  as root. Do not make the binary setuid. You can only run both
  those renderers at the console only, so being root is not that much of
  an issue.

  The X11 render does not need any root permissions (if /dev/dsp is
  writable by others for sound).  The dedicated server mode does not
  need to be root either, obviously.

  Problems such as root requirements for games has been sort of a sore
  spot in Linux for a number of years now. This is one of the goals that
  e.g. GGI is targetting to fix.  A ref_ggi might be supported in the
  near future.


  10.6.  Does LinuxQuake use multitexturing?

  To my understadnding, glQuake will use a multitexture EXTension if the
  OpenGL driver in question offers it.  The current Mesa implementation
  and the Glide driver for Linux do not yet support this extension, so
  for the time being the answer is no. See section on Mesa and
  multitexturing for details.
  10.7.  Where can I get current information on Linux glQuake?

  Try some of these sites: the "The Linux Quake Resource" at
  linuxquake.telefragged.com, or the "Linux Quake Page" at
  www.planetquake.com/threewave/linux/.  Alternatively, you could look
  for Linux Quake sites in the "SlipgateCentral" database at
  www.slipgatecentral.com.



  11.  FAQ: Troubleshooting?

  11.1.  Has this hardware been tested?

  See hardware requirements list above. I currently do not maintain a
  conclusive list of vendors and boards, as no particular board specific
  problems have been verified.  Currently, only 3Dfx and Quantum3D
  provide boards for testing to the developers, so Quantum3D consumer
  boards are a safe bet. Every other Voodoo Graphics (tm) based board
  should work, too. I have reports regarding the Orchid Righteous 3D,
  Guillemot Maxi 3D Gamer, and Diamond Monster 3D.

  If you are a board manufacturer who wants to make sure his Voodoo
  Graphics (tm), Voodoo Rush (tm) or Voodoo 2 (tm) boards work with
  upcoming releases of Linux, Xfree86, Linux Glide and/or Mesa, please
  contact me, and I will happily forward your request to the persons
  maintaining the drivers in question. If you are interested in support
  for Linux Glide on other then the PC platfrom, e.g. DEC Alpha, please
  contact the maintainer of Linux Glide Daryll Strauss, at
  daryll@harlot.rb.ca.us



  11.2.  Failed to change I/O privilege?

  You need to be root, or setuid your application to run a Glide based
  application.  For DMA, the driver accesses /dev/mem, which is not
  writeable for anybody but root, with good reasons. See the README in
  the Glide distribution for Linux.



  11.3.  Does it work without root privilege?

  There are compelling case where the setuid requirement is a problem,
  obviously. There are currently solutions in preparation, which require
  changes to the library internals itself.



  11.4.  Displayed images looks awful (single screen)?

  If you are using the analog pass through configuration, the common
  SVGA or X11 display might look pretty bad.  You could try to get a
  better connector cable than the one provided with the accelerator
  board (the ones delivered with the Diamond Monster 3D are reportedly
  worse then the one accompanying the Orchid Righteous 3D), but up to a
  degree there will inevitably be signal loss with an additional
  transmission added.

  If the 640x480 full screen image created by the accelerator board does
  look awful, this might indicate a real hardware problem. You will have
  to contact the board manufacturer, not 3Dfx for details, as the
  quality of the video signal has nothing to do with the accelerator -
  the board manufacturer chooses the RAMDAC, output drivers, and other
  components responsible.



  11.5.  The last frame is still there (single or dual screen)?

  You terminated your application with Ctrl-C, or it did not exit
  normally. The accelerator board will dutifully provide the current
  content of the framebuffer as a video signal unless told otherwise.



  11.6.  Powersave kicks in (dual screen)?

  When you application terminates in dual screen setups, the accelerator
  board does not provide video output any longer. Thus powersave kicks
  each time. To avoid this, use

  ______________________________________________________________________
  setenv SST_DUALSCREEN 1
  ______________________________________________________________________



  11.7.  My machine seem to lock (X11, single screen)?

  If you are running X when calling a Glide application, you probably
  moved the mouse out of the window, and the keyboard inputs do not
  reach the application anymore.

  If you application is supposed to run concurrently with X11, it is
  recommend to expose a full screen window, or use the XGrabPointer and
  XGrabServer functions to redirect all inputs to the application while
  the X server cannot access the display. Note that grabbing all input
  with XGrabPointer and XGrabServer does not qualify as well-behaved
  application, and that your program might block the entire system.

  If you experience this problem without running X, be sure that there
  is no hardware conflict (see below).


  11.8.  My machine locks (single or dual screen)?

  If the system definitely does not respond to any inputs (you are
  running two displays and know about the loss of focus), you might
  experience a more or less subtle hardware conflict.  See installation
  troubleshooting section for details.

  If there is no obvious address conflict, there might still be other
  problems (below). If you are writing your own code the most common
  reason for locking is that you didn't snap your vertices. See the
  section on snapping in the Glide documentation.


  11.9.  My machine locks (used with S3 VGA board)?

  It is possible you have a problem with memory region overlap specific
  to S3. There is some info and a patch to the so-called S3 problem in
  the 3Dfx web site, but these apply to Windows only. To my
  understanding, the cause of the problem is that some S3 boards (older
  revisions of Diamond Stealth S3 968) reserve more memory space than
  actually used, thus the Voodoo Graphics (tm) has to be mapped to a
  different location. However, this has not been reported as a problem
  with Linux, and might be Windows-specific.
  11.10.  No address conflict, but locks anyway?

  If you happen to use a motherboard with non-standard or incomplete PCI
  support, you could try to shuffle the boards a bit. I am running an
  ASUS TP4XE that has that non-standard modified "Media Slot", i.e. PCI
  slot4 with additional connector for ASUS-manufactured SCSI/Sound combo
  boards, and I experienced severe problems while running a Diamond
  Monster 3D in that slot. The system operates flawlessly since I put
  the board in one of the regular slots.



  11.11.  Mesa runs, but does not access the board?

  Be sure that you recompiled all the libraries (including the toolkits
  the demo programs use - remember that GLUT does not yet support Voodoo
  Graphics (tm)), and that you removed the older libraries, run
  ldconfig, and/or set your LD_LIBRARY_PATH properly.  Mesa supports
  several drivers in parallel (you could use X11 SHM, off screen
  rendering, and Mesa Voodoo at the same time), and you might have to
  create and switch contexts explicitely (see MakeCurrent function) if
  the Voodoo Graphics (tm) isn't chosen by default.



  11.12.  Resetting dual board SLI?

  If a Quantum 3D Obsidian board using in an SLI setup exits abruptly
  (i.e., the application crashes, or is aborted by user), the boards are
  left in an undefined state.  With the dual-board set, you can run a
  program called resetsli to reset them. Until you run the resetsli
  program, you will not be able to re-initialize the Obsidian board.



  11.13.  Resetting single board SLI?

  The resetsli program mentioned above does not yet work with a single
  board Obsidian SLI (e.g. the Obsidian 100-4440SB). You will have to
  reboot your system by reset in order to reset the board.



  4mb Laptop HOWTO
  Bruce Richardson <brichardson@lineone.net>

  25 March 2000

  How to put a "grown-up" Linux on a small-spec (4mb RAM, <=200mb hard
  disk) laptop.

  ______________________________________________________________________

  Table of Contents



  1. Introduction

     1.1 Why this document was written.
     1.2 What use is a small laptop?
     1.3 Why not just upgrade the laptop?
     1.4 What about 4mb desktop machines?
     1.5 What this document doesn't do.
     1.6 Where to find this document.
     1.7 Copyright

  2. The Laptops

     2.1 Basic Specifications
        2.1.1 Compaq Contura Aero
        2.1.2 Toshiba T1910
     2.2 The Problem
     2.3 The Solution

  3. Choices Made

     3.1 What to use to create the initial root partition?
     3.2 The Distribution
           3.2..1 But I don't like Slackware!
     3.3 Which installation method to use?
     3.4 Partition Layout
        3.4.1 Basic Requirement
        3.4.2 How complex a layout?
     3.5 Which components to install?

  4. The Pre-installation Procedure

     4.1 muLinux Preparation
     4.2 Prepare the installation root files.
     4.3 Create the partitions.
        4.3.1 Mini-Linuces and ext2 file-systems - an important note.
        4.3.2 Procedure

  5. The Installation

     5.1 Boot the machine
     5.2 Floppy/Parport CD-ROM Install
     5.3 Network/PCMCIA Install
        5.3.1 PCMCIA install on the Aero
     5.4 Set-up
        5.4.1 AddSwap
        5.4.2 Target
        5.4.3 Select
        5.4.4 Install
        5.4.5 Configure
        5.4.6 Exit
     5.5 Pre-reboot Configuration
     5.6 Post-reboot Configuration.
        5.6.1 Re-use the temporary root.
        5.6.2 Other configuration tweaks.

  6. Conclusion

  7. Appendix A:

     7.1 A - Base Linux System
           7.1..1 Packages considered for omission:
           7.1..2 Packages installed:
     7.2 AP - Non-X Applications
           7.2..1 Packages considered for inclusion:
           7.2..2 Packages installed:
     7.3 D - Development Tools
           7.3..1 Packages installed:
     7.4 E - Emacs
           7.4..1 Packages installed:
     7.5 F - FAQs and HOWTOs
           7.5..1 Packages installed:
     7.6 K - Kernel Source
           7.6..1 Packages Installed:
     7.7 N - Networking Tools and Apps
           7.7..1 Packages installed:
     7.8 Tetex
           7.8..1 Packages installed:
     7.9 Y - BSD Games Collection
           7.9..1 Packages installed:
     7.10 End result

  8. Appendix B: Resources relevant to this HOWTO



  ______________________________________________________________________

  1.  Introduction

  1.1.  Why this document was written.

  I got my hands on two elderly laptops, both with just 4mb RAM and
  small (<=200mb) hard drives. I wanted to install Linux on them. The
  documentation for this kind of laptop all recommends installing either
  a mini-Linux or an old (and therefor compact) version of one of the
  professional distributions.  I wanted to install an up-to-date
  professional distribution.

  1.2.  What use is a small laptop?

  Plenty. It isn't going to run X or be a development box (see ``Which
  components to install?'') but if you are happy at the console you have
  a machine that can do e-mail, networking, writing etc. Laptops also
  make excellent diagnostic/repair tools and the utilities for that will
  easily fit onto small laptops.

  1.3.  Why not just upgrade the laptop?

  Upgrading old laptops is not much cheaper than upgrading new ones.
  That's a lot to spend on an old machine, especially considering that
  the manufacturer isn't supporting it any more and spare parts are hard
  to find.

  1.4.  What about 4mb desktop machines?

  The procedure described in this document will work perfectly well on a
  desktop PC. On the other hand, upgrading a desktop machine is far
  easier and cheaper than upgrading a laptop. Even if you don't upgrade
  it, there are still simpler options. You could take out the hard disk,
  put it in a more powerful machine, install Linux, trim it to fit and
  then put the disk back in the old machine.

  1.5.  What this document doesn't do.

  This document is not a general HOWTO about installing Linux on laptops
  or even a specific HOWTO for either of the two machines mentioned
  here. It simply describes a way of squeezing a large Linux into a very
  small space, citing two specific machines as examples.



  1.6.  Where to find this document.

  The latest copy of this document can be found in several formats at
  http://website.lineone.net/~brichardson/linux/4mb_laptops/.

  1.7.  Copyright

  This document is copyright (c) Bruce Richardson 2000. It may be
  distributed under the terms set forth in the LDP license at
  sunsite.unc.edu/LDP/COPYRIGHT.html.

  This HOWTO is free documentation; you can redistribute it and/or
  modify it under the terms of the LDP license. This document is
  distributed in the hope that it will be useful, but without any
  warranty; without even the implied warranty of merchantability or
  fitness for a particular purpose. See the LDP license for more
  details.

  Toshiba and T1910 are trademarks of Toshiba Corporation. Compaq and
  Contura Aero are trademarks of Compaq Computer Corporation.

  2.  The Laptops

  This section describes the laptops that I have used this procedure on,
  the problems faced when installing Linux on them and the solutions to
  those problems (in outline).

  2.1.  Basic Specifications

  2.1.1.  Compaq Contura Aero


    25MHz 486SX CPU

    4mb RAM

    170mb Hard Disk

    1 PCMCIA Type II slot

    External PCMCIA 3.5" Floppy drive   (-- The PCMCIA floppy drive has
     a proprietary interface which is partly handled by the Aero's
     unique BIOS. The Linux PCMCIA drivers can't work with it. According
     to the PCMCIA-HOWTO, if the drive is connected when the laptop
     boots it will work as a standard drive and Card Services will
     ignore the socket but it is not hot-swappable. However, I found
     that the drive becomes inaccessible as soon as Card Services start
     unless there is a mounted disk in the drive. This has implications
     for the installation process - these are covered at the relevant
     points.  --)


  2.1.2.  Toshiba T1910


    33MHz 486SX CPU

    4mb RAM

    200 mb Hard Disk

    Internal 3.5" Floppy drive

    1 PCMCIA Type II/III slot


  2.2.  The Problem

  The small hard disks and the lack of an internal floppy on the Aero
  make the installation more tricky than normal but the real problem is
  the RAM. None of the current distributions has an installation disk
  that will boot in 4mb, not even if the whole hard disk is a swap
  partition.

  The standard installation uses a boot disk to uncompress a root-
  partition image (either from a second floppy or from CD-ROM) into a
  ram-disk. The root-image is around 4mb in size. That's all the RAM
  available in this scenario. Try it and it freezes while unpacking the
  root-image.

  2.3.  The Solution

  The answer is to eliminate the ram-disk. If you can mount root on a
  physical partition you will have enough memory to do the install.
  Since the uncompressed ram-disk is too big to fit on a floppy, the
  only place left is on the hard disk of the laptop. The steps are:


  1. Find something that will boot in 4mb ram and which can also create
     ext2 partitions.

  2. Use it to create a swap partition and a small ext2 partition on the
     laptop's hard disk.

  3. Uncompress the installation root-image and copy it onto the ext2
     partition.

  4. Boot the laptop from the installation boot-disk, pointing it at the
     ext2 partition on the hard disk.

  5. The installation should go more or less as normal from here.

  The only question was whether a distribution that wouldn't install
  (under normal circumstances) on the laptops would run on them. The
  short answer is "Yes".

  If you're an old Linux hand then that's all you need to know. If not,
  read on - some of the steps listed above aren't as simple as they
  look.

  3.  Choices Made

  This section describes the choices available, which options are
  practical, which ones I decided on and why.

  3.1.  What to use to create the initial root partition?

  The best tool for this is a mini-Linux. There's a wide selection of
  small Linuces available on the net, but most of them won't boot in 4mb
  RAM. I found two that will:


     SmallLinux  http://smalllinux.netpedia.net/
        SmallLinux will boot in as little as 2mb RAM but its root disk
        can't be taken out of the drive, which is a shame since
        otherwise it has everything we need (i.e. fdisk, mkswap and
        mkfs.ext2). SmallLinux can create the needed partitions but
        can't be used to copy the root partition.

     muLinux  http://sunsite.auc.dk/mulinux/
        muLinux will boot in 4mb but only in a limited single-user mode.
        In this mode fdisk and mkswap are available but mkfs.ext2 and
        the libraries needed to run it are on the /usr partition which
        is not available in maintenance mode. To use muLinux to do the
        whole pre-installation procedure the files needed to create ext2
        file-systems must be extracted from the usr disk image and
        copied onto a floppy.

  This gives the option of either using SmallLinux to create the
  partitions and muLinux to copy the root partition or using muLinux to
  do the whole job.  Since I had two laptops I tried both.

  3.2.  The Distribution

  It didn't take much time to choose Slackware. Apart from the fact that
  I like it but haven't used it much and want to learn more, I
  considered the following points:


    Slackware has possibly the most low-tech DIY install of all the
     major distributions.  It is also one of the most flexible, coming
     with a wide range of boot-disk kernels to suit many different
     machines. This makes it well suited to the kind of hacking about
     required in this scenario.

    Slackware supports all the methods listed in ``Which Installation
     method to use?''.

    Slackware is a distribution designed by one person. I'm sure
     Patrick Volkerding won't object if I say this means its
     configuration tools are simpler and more streamlined. In my opinion
     this makes the job of trimming the installation to fit cramped
     conditions easier.

  Version 7.0 was the latest version when I tried this so that's what I
  used.

  3.2.0.1.  But I don't like Slackware!

  You don't have to use it. I can't answer for all the distributions but
  I know that Debian, Red Hat and SuSE offer a range of installation
  methods and have an "expert" installation procedure   (-- Does Debian
  do any other kind?  --)

  which can be used here. Most of the steps in this document would apply
  to any of the distributions without change.

  If you haven't used the expert method with your preferred distribution
  before, do a trial run on a simple desktop machine to get the feel of
  it and to explore the options it offers.

  3.3.  Which installation method to use?


     Floppy Install
        This means churning out 15 floppies - which only gives you an
        absolute minimal install and requires a second stage to get the
        apps you want on. It's also very slow on such low-spec machines.
        This is a last resort if you can't make the others work.

     Parallel-port Install
        Where the parallel port has an IDE device, parallel cable or
        pocket ethernet adaptor   (-- A pocket lan adaptor installation
        onto these machines will be very slow.  --)

        attached. This would be a good choice for the Aero, leaving the
        PCMCIA slot free to run the floppy drive.

     PCMCIA Install
        As above, this could be a CD-ROM or network install.  This would
        be the best method for the T1910 - on the Aero it's a bit more
        awkward.

     ISA/PCI Ethernet Install
        Not an option for the laptops, obviously, but included in case
        your target machine is a desktop PC.


       The tools I had to hand dictated a PCMCIA network install. I
       will point out where steps differ for the other methods.
       Whichever method you choose, you need to have a higher-spec
       machine available - even if only to create the disks for a
       floppy install.


  3.4.  Partition Layout

  3.4.1.  Basic Requirement

  This procedure requires at least two Linux Native partitions in
  addition to a Swap partition. Since one of the ext2 partitions will be
  in use as temporary root during the installation it will not be
  available as a target partition and so should be small - though no
  smaller than 5mb. It makes sense to create for this a partition that
  you will re-use as /home after installation is complete.  Another
  option would be to re-create it as a DOS partition to give you a dual
  boot laptop.

  3.4.2.  How complex a layout?

  There isn't room to get too clever here. There is an argument for
  having a single ext2 partition and using a swap file to avoid wasting
  space but I would strongly urge creating a separate partition for
  /usr. If you have only one partition and something goes wrong with it
  you may well be faced with a complete re-installation. Separating /usr
  and having a small partition for / makes disaster recovery a more
  likely prospect. On both machines I created 4 partitions in total:


  1. A swap partition -- 16mb on the T1910, 20 on the Aero (I'm more
     likely to upgrade the memory on the Aero).

  2. /home (temporary root during installation) -- 10mb

  3. / -- 40mb on the T1910, 30mb on the Aero.

  4. /usr -- All the remainder.

  In addition, the Aero uses hda3 for a 2mb DOS partition containing
  configuration utilities. See the Aero FAQs for details.

  3.5.  Which components to install?

  The full glibc libraries alone would nearly fill the hard disks so
  there's no question of building a development machine. It looks as if
  a minimal X installation can be squeezed in but I'm sure it would
  crawl and I don't want it anyway.  I decide to install the following
  (for a full listing see ``Appendix A''):


    The core Linux utilities

    Assorted text apps from the ap1 file set:

    Info/FAQ/HOWTO documentation

    Basic networking utilities

    The BSD games

  This selection matches the kind of machine described in ``What use is
  a small laptop?''.

  4.  The Pre-installation Procedure

  This section covers creating a swap partition and a temporary root
  partition on the laptop's hard disk. Nothing here is Slackware-
  specific.

  4.1.  muLinux Preparation

  If you are going to use only muLinux to for this procedure then you
  need to prepare a disk with mkfs.ext2 and supporting libraries on it.
  From the muLinux setup files uncompress USR.bz2 and mount it as a loop
  file-system. If you are in the same directory as the USR file and you
  want to mount it as /tmpusr then the sequence for this is:


  ______________________________________________________________________
  losetup /dev/loop0 USR
  mount -t ext2 /dev/loop0 /tmpusr

  ______________________________________________________________________



  >From there copy mkfs.ext2, libext2fs.so.2, libcomerr.so.2 and
  libuuid.so.1 onto a floppy.

  4.2.  Prepare the installation root files.

  Select the root disk you want - I used the color one with no problems
  but the text one would be slightly faster in these low memory
  conditions. Uncompress the image and mount it as a loop device. The
  procedure is the same as in the above section but the root disk image
  is a minix file-system.

  Next you need 3 1722 floppies or 4 1440 floppies with ext2 file-
  systems - it's better with 1722 disks as you don't need to split the
  /lib directory.  Give one floppy twice the default number of inodes so
  it can take the /dev directory. That's 432 nodes for a 1722 disk or
  368 for a 1440. If you specify /dev/fd0H1722 or /dev/fd0H1440 then you
  don't have to give any other parameters so for a 1722 disk do


  ______________________________________________________________________
  mke2fs -N 432 /dev/fd0H1722

  ______________________________________________________________________



  If you have mounted the root image as /tmproot and the destination
  floppy as /floppy then cd to /tmproot. To copy the dev directory the
  command is



  ______________________________________________________________________
  cp -dpPR dev/* /floppy/

  ______________________________________________________________________



  For the other directories with files in (bin, etc, lib, mnt, sbin,
  usr, var) it's


  ______________________________________________________________________
  cp -dpPr directoryname/* /floppy/

  ______________________________________________________________________



  Don't bother with the empty ones (floppy, proc, root, tag, tmp)
  because you can simply create them on the laptop. boot and cdrom are
  soft links pointing to /mnt/boot and /var/log/mount respectively - you
  can also create them on the laptop.

  4.3.  Create the partitions.

  4.3.1.  Mini-Linuces and ext2 file-systems - an important note.

  To save space, small-Linux designers sometimes use older libc5
  librariesand where they do use up-to-date libc6 they leave out may of
  the options compiled into full distributions, including some optional
  features of the ext2 file-system.  This has two consequences:


    Trying to mount ext2 disks formatted using a modern Linux system
     can generate error messages if you mount them read-write. Be sure
     to use the -r option when mounting floppies on the laptops.

    It is not wise to use the mkfs.ext2 that comes with the mini-Linux
     to create file-systems on the partitions into which SlackWare will
     be installed. It should only be used to create the file-system on
     the temporary root partition. Once installation is complete this
     partition can be reformatted and re-used.

  4.3.2.  Procedure

  If installing on an Aero, make sure the floppy drive is inserted
  before switching on and do not remove it.


  1. Boot from the mini-Linux   (-- With muLinux, wait until the boot-
     process complains about the small memory space and offers the
     option of dropping into a shell - take that option and work in the
     limited single-user mode it gives you.  --)



  2. Use fdisk to create the partitions.

  3. Reboot on leaving fdisk (with muLinux you may simply have to turn
     off and on again at this point).

  4. Use mkswap on the swap partition and then activate it (this will
     make muLinux much happier).

  5. If using muLinux then mount the extra floppy created in ``muLinux
     Preparation'', copy mkfs.ext2 into /bin and the libraries into
     /lib.

  6. Use mkfs.ext2 to create the file-system on the temporary root
     partition.

  7. If you have been using SmallLinux, shut down and reboot using
     muLinux.  Don't forget to activate the swap partition again.

  8. muLinux will have mounted the boot floppy on /startup - unmount it
     to free the floppy drive.

  9. Now mount the temporary root partition and copy onto it the
     contents of the disks you created in ``Prepare the installation
     root files''. Do not be alarmed by the error messages: if, for
     example, you copy usr from the floppy to the temporary root
     partition by typing "cp -dpPr usr/* /tmproot/" then you'll get the
     error message "cp: sr: no such file or directory". Ignore this,
     nothing is wrong.

  10.
     cd to the temporary root partition and create the empty folders
     (floppy, proc, root, tag, tmp) and the soft links boot (pointing to
     mnt/boot) and cdrom (to var/log/mount).

  11.
     Unmount the temporary root partition - this syncs the disk.

  12.
     You can simply turn off the machine now.

  5.  The Installation

  This section does not give much detail on the Slackware installation
  process.  In fact, it assumes you are familiar with it. Instead, this
  section concentrates on those areas where special care or unusual
  steps are required.

  5.1.  Boot the machine

  Make a boot-disk from one of the images. I recommend you use bareapm.i
  on a laptop and bare.i on a desktop - unless you have a parallel-port
  IDE device (pportide.i). Boot the laptop from it. When the boot:
  prompt appears, type "mount root=/dev/hdax" where x is the temporary
  root partition. Log in as root.  Then activate the swap partition.

  5.2.  Floppy/Parport CD-ROM Install

  In both these cases, no extra work should be necessary to access the
  installation media. Simply run setup.

  5.3.  Network/PCMCIA Install

  Slackware has supplementary disks with tools for these and
  instructions for their use greet you when you log in. Use the network
  disk on a desktop PC with ethernet card or a laptop with pocket
  ethernet adaptor. Use the PCMCIA disk for PCMCIA install. Once your
  network adapter/PCMCIA socket has been identified, run setup.

  5.3.1.  PCMCIA install on the Aero

  The Slackware installation process runs the PCMCIA drivers from the
  supplementary floppy. Because the Aero has a PCMCIA floppy drive, this
  means you can't remove the floppy drive to insert the PCMCIA CD-
  ROM/ethernet card. The solution is simple: the Slackware PCMCIA setup
  routine creates /pcmcia and mounts the supplementary disk there, so

  1. Create the /pcmcia directory yourself

  2. Mount the supplementary disk to /mnt. Be sure to specify the type
     as vfat - if you don't, it'll be incorrectly identified as UMSDOS
     and long filenames will be mis-copied.

  3. cd /mnt;cp -dpPr ./* /pcmcia/

  4. Unmount the floppy.

  5. Run pcmcia. When the script complains that there is no disk in the
     drive simply hit Enter: Card Sevices will start. Connect your
     PCMCIA device and hit Enter.

  6. Run setup

  5.4.  Set-up

  The Slackware set-up program is straightforward. Start with the Keymap
  section and it'll take you forward step by step.

  5.4.1.  AddSwap

  You do need to do this step so it can put the correct entry in fstab
  but make sure it doesn't run mkswap - you're already using the
  partition.

  5.4.2.  Target

  In this section Slackware asks which partitions will be mounted as
  what and then formats them if you want.

  The safest bet here is to leave your temporary root partition out
  altogether and just edit fstab later once you know you don't need it
  for it's temporary purpose anymore. If you're going to reuse it as
  /home then it is OK to designate it as /home - obviously, don't format
  it now! If you intend to re-use it as a part of the directory
  structure that will have files placed in it during installation (/var,
  for example) then you absolutely must ignore it in this step: after
  the installation is complete you can move the files across.

  5.4.3.  Select

  Here you choose which general categories of software to install. I
  chose as follows:


    A - Base Linux System

    AP -Non-X applications

    F - FAQs and HOWTOs

    N - Networking tools and apps

    Y - BSD games collection

  I wouldn't recommend adding to this - if anything, prune it back to A,
  AP and N. That gives you a core Linux setup to which you can add
  according to your needs.

  5.4.4.  Install

  Choose the Expert installation method. This allows you to
  select/reject for installation individual packages from the categories
  you chose in the Selection step. ``Appendix A'' goes through the
  precise choices I made .

  This part takes about 3 hours for a PCMCIA network install. You are
  prompted to select individual packages before the installation of each
  category, so you can't just walk away and leave it to run through.

  5.4.5.  Configure

  Once the packages are all installed, you are prompted to do final
  configuration for your machine. This covers areas like networking,
  Lilo, selecting a kernel etc. Some points to look out for:


    If you did a PCMCIA install, don't accept the offer to configure
     your network with netconfig. This will ruin your pcmcia networking.
     Wait until you've rebooted and then edit /etc/pcmcia/network.opts

    This is the point where you should install a kernel. For a laptop
     the bareapm kernel is best, for a desktop simply the bare one.

  5.4.6.  Exit

  The set-up process is finished but you are not. Do not reboot yet!
  There is another vital step to complete.

  5.5.  Pre-reboot Configuration

  On a normal machine you would simply reboot once the installation is
  complete.  If you do that here you may have to wait 6 or 8 hours for a
  login prompt to appear and another half hour to get to the command
  prompt. Before rebooting you need to change or remove the elements
  that cause this slowdown. This involves editing config files so you
  need to be familiar with vi, ed or sed.

  At this stage your future root partition is still mounted as /mnt so
  remember to at that to the paths given here.


     /etc/passwd
        Edit this to change root's login shell to ash. ash really is the
        only practical login shell for 4mb RAM.

     /etc/rc.d/rc.modules
        Comment out the line 'depmod -a'. You only need to update module
        dependencies if you have changed your module configuration
        (recompiled or added new ones, for example). On a standard
        system it only takes a second or two and so it doesn't matter
        that it's needlessly performed each time. On a 4mb laptop it can
        take as much as 8 hours.  When you do change your module set-up
        you can simply uncomment this line and reboot. Alternatively,
        change this part of the script so that it will only run if you
        pass a parameter at the boot-prompt. For example:

        ________________________________________________________________
        if [ "NEWMODULES" == "1" ] ; then
            depmod -a
        fi

        ________________________________________________________________



     /etc/rc.d/rc.inet2
        This script starts network services like nfs.  You probably
        don't need these and certainly not at start-up. Rename this
        script to something like RC.inet2 - that will stop it from being
        run at boot and you can run it manually when you need it.

     /etc/rc.d/rc.pcmcia
        On the Aero you should also rename this script, otherwise you'll
        lose the use of your floppy drive on start-up. It's worth
        considering for any other small laptop as well - you can always
        run it manually before inserting a card.

  Once these changes have been made, you are ready to reboot.

  5.6.  Post-reboot Configuration.

  If you made the changes recommended in section ``Pre-reboot
  configuration'' then the boot process will only take a few minutes, as
  opposed to several hours. Login as root and check that everything is
  functioning properly.

  5.6.1.  Re-use the temporary root.

  Once you are sure the installation is solid you can reclaim the
  partition you used as the temporary root. Don't just delete the
  contents, reformat the filesystem. Remember, the mke2fs that came with
  the mini-Linux is out of date.

  If you intend to re-use this partition as /home, remember not to
  create any user accounts until you have completed this step.

  5.6.2.  Other configuration tweaks.

  In such a small RAM space, every little helps. Go through SlackWare's
  BSD-style init scripts in /etc/rc.d/ and comment out anything you
  don't need. Have a look at Todd Burgess' Small Memory mini-HOWTO
  http://eddie.cis.uoguelph.ca/~tburgess/ for more ideas.

  6.  Conclusion

  That's it all done. You now have a laptop with the core utilities in
  place and 50 to 70mb spare for whichever extras you need. Don't mess
  it up because it's a lot easier to modify an existing installation on
  such cramped old machines than it is to start from scratch again.

  7.  Appendix A:

  This appendix lists which packages (if any) from each category might
  be included in the installation and gives my reasons for including or
  omitting them. I made no attempt to install X so those categories are
  ignored.

  Although this appendix refers specifically to the Slackware
  distribution it can be used as a guide with any of the major
  distributions.

  7.1.  A - Base Linux System

  Most of the packages in this category are essential, even those that
  aren't listed as required by the Slackware set-up program. Because of
  this, I've listed those packages that I felt could reasonably be left
  out rather than all the non-compulsory packages that I installed.

  7.1.0.1.  Packages considered for omission:


     kernels (ide, scsi etc.)
        There's no need to install any of these, you get a chance to
        select a kernel at the very end of the installation process.

     aoutlibs
        This is only needed if you intend to run executables compiled in
        the old a.out format. Omitting it saves a lot of space. Omitted.

     bash1
        Bash2 (simply called bash in the Slackware package list) is
        required for the Slackware configuration scripts but there are a
        lot of scripts that need bash1. I included it.

     getty
        agetty is Slackware's default getty, this package contains getty
        and uugetty as alternatives. Only include it if you need their
        extra functionality. Omitted.

     gpm
        Personally, I find this very useful at the console (and the
        Aero's trackball is very handy) but it's not essential.
        Included.

     icbs2
        Not needed. Omitted.

     isapnp
        No use here. Omitted.

     loadlin
        Not needed with the setup described here - unless your old
        laptop has some peculiarity that requires a DOS driver to
        initialise some of its devices. Omitted.

     lpr
        You could argue that you can do your printing from whichever
        desktop is nearest but I always find it useful to be have
        printing capabilities on a laptop. Included.

     minicom
        Not a compulsory include but I want the laptop to do dial-up
        connection. Very handy. Included.

     pciutils
        Not needed on these old laptops. Omitted.

     quota
        Not vital but it can be used to set limits that stop you from
        overflowing the limited space available in these laptops.
        Included.

     tcsh
        I recommend using ash as your login shell. Only include this if
        you need it for scripts. Omitted.

     umsprogs
        You can leave this out and still be able to access UMSDOS
        floppies. Omitted.

     scsimods
        No use on these laptops. Omitted.

     sysklogd
        This can interfere with apmd but it does provide essential
        information. Included.

  7.1.0.2.  Packages installed:

  aaa_base, bash, bash1, bin, bzip2, cpio, cxxlibs, devs, e2fsprog,
  elflibs, elvis, etc, fileutils, find, floppy, fsmods, glibcso, gpm,
  grep, gzip, hdsetup, infozip, kbd, ldso, less, lilo, man, modules,
  modutils, pcmcia, sh_utils, shadow, sudo, sysklogd, sysvinit, tar,
  txtutils, util, zoneinfo

  Combined size: 33.4

  7.2.  AP - Non-X Applications

  None of these packages are, strictly speaking, essential - although
  ash is really required for sensible operation in 4mb. Leaving them all
  out could save the vital space for you to squeeze in your favourite
  app. I selected a minimal set of tools that I don't like to do
  without.

  7.2.0.1.  Packages considered for inclusion:


     apsfilter
        Not much point having printing if you can only print text files.
        Included.

     ash
        This is the shell for low-memory machines, only taking up 60k.
        Use it as the default login shell unless you like waiting 10
        seconds for the command prompt to reappear each time. Included.

     editors (jed, joe jove vim)
        elvis is the default Slackware editor and a required part of the
        installation. If, like me, you are a vi fan then that's all you
        need: installing vim would be wasteful duplication given the
        space restrictions. If you can't stand vi and need a more DOS-
        style editor then joe is small. Emacs fans with some self-
        discipline might consider jed or jove rather than pigging out on
        the full-size beast. Omitted.

     enscript
        If you already have apsfilter you don't really need this.
        Omitted.

     ghostscript
        Including the fonts this comes to about 7.5mb. One to leave
        until after the core installation, then consider if you need it.
        Omitted.

     groff
        Needed for the man pages. Included.

     ispell
        Not an essential butvery useful to the overenthusiastic touch-
        typist.  included.

     manpages
        Included!

     mc Slackware offers a lightweight compilation of mc but I'm happier
        at the command prompt. Omitted.

     quota
        Not necessary on what is not a multi-user machine but you
        may,like me, find it handy to stop you from forgetfully wasting
        the little space you have. Included.

     rpm
        Don't bother. If you do have an rpm that you would like to
        squeeze in, use rpm2tgz on a desktop machine to turn it into a
        tgz package - then you can use the standard Slackware
        installation tools. Omitted.

     sc A useful little spreadsheet packed very small. Included.

     sudo
        Not essential but I find it useful here: it's a cramped
        environment and an awkward reinstall if you mess things up -
        sudo helps create user profiles with the power to do the things
        you need without carelessly wiping your disk.  Included.

     texinfo
        Info documentation. Included.

     zsh
        Leave this out unless you're addicted to it or have scripts that
        must use it. Omitted.

  7.2.0.2.  Packages installed:

  apsfilter,ash, diff, groff, ispell, manpages, quota, sc, sudo, texinfo

  Combined size: 8.1 mb

  7.3.  D - Development Tools

  You could fit C or C++ into this space but the glibc library package
  is too big, so some pruning would be needed. Do the main installation
  first and then try it.

  There is room for Perl and Python.

  7.3.0.1.  Packages installed:

  None

  7.4.  E - Emacs

  I don't use Emacs and so saved myself some space. On the other hand,
  if you are an Emacs fan then you probably use it for e-mail, news and
  coding so you'll claim some of that space back by omitting other
  packages.

  If you do want Emacs it might be an idea to leave this out while doing
  the core installation. Once the laptop is up you can try fitting in
  what you want/need at your leisure.

  7.4.0.1.  Packages installed:

  None.

  7.5.  F - FAQs and HOWTOs

  If you know it all you don't need these. I installed the lot.

  7.5.0.1.  Packages installed:

  howto, manyfaqs, mini

  Combined size: 12.4 mb

  7.6.  K - Kernel Source

  You can just squeeze it in. If all you want to do is read the source,
  go ahead.


  7.6.0.1.  Packages Installed:

  None

  7.7.  N - Networking Tools and Apps

  These packages were selected to provide core networking tools, dial-up
  capability, e-mail, web and news.

  7.7.0.1.  Packages installed:

  dip, elm, fetchmail, mailx, lynx, netmods, netpipes, ppp, procmail,
  trn, tcpip1, tcpip2, uucp, wget

  Combined size: 15.1 mb

  7.8.  Tetex

  Another set that will barely squeeze in. I can't say how it would run
  in the space available.

  7.8.0.1.  Packages installed:

  None

  7.9.  Y - BSD Games Collection

  I'm addicted to several of these. If I really need that last 5mb they
  can go.

  7.9.0.1.  Packages installed:

  bsdgames

  Combined size: 5.4 mb

  7.10.  End result

  In total the installed packages plus kernel took up about 75mb of disk
  space of which 19.5mb was in the root partition and 55.5 in /usr. On
  the Aero that left 39mb in /usr, 74mb on the T1910.

  8.  Appendix B: Resources relevant to this HOWTO


     Linux Laptop HOWTO
        http://www.snafu.de/~wehe/Laptop-HOWTO.html

     Small Memory mini-HOWTO
        http://eddie.cis.uoguelph.ca/~tburgess/

     Linux on Laptops
        http://www.cs.utexas.edu/users/kharker/linux-laptop/ HOWTOs and
        installation FAQs for a wide range of machines.

     Linux T1910 FAQ
        http://members.tripod.com/~Cyberpvnk/linux.htm

     Linux Contura Aero FAQ
        http://domen.uninett.no/~hta/linux/aero-faq.html

     Contura Aero FAQ
        http://www.reed.edu/~pwilk/aero/aero.faq Comprehensive FAQ on
        all aspects of the Contura Aero compiled by the moderators of
        the Aero mailing list. Good Linux section .



  GNU/Linux AI & Alife HOWTO
  by John Eikenberry
  v1.4, 23 June 2000

  This howto mainly contains information about, and links to, various AI
  related software libraries, applications, etc. that work on the
  GNU/Linux platform. All of it is (at least) free for personal use. The
  new master page for this document is http://zhar.net/gnu-linux/howto/
  ______________________________________________________________________

  Table of Contents


  1. Introduction

     1.1 Purpose
     1.2 Where to find this software
     1.3 Updates and comments
     1.4 Copyright/License

  2. Traditional Artificial Intelligence

     2.1 AI class/code libraries
     2.2 AI software kits, applications, etc.

  3. Connectionism

     3.1 Connectionist class/code libraries
     3.2 Connectionist software kits/applications

  4. Evolutionary Computing

     4.1 EC class/code libraries
     4.2 EC software kits/applications

  5. Alife & Complex Systems

     5.1 Alife & CS class/code libraries
     5.2 Alife & CS software kits, applications, etc.

  6. Autonomous Agents

  7. Programming languages



  ______________________________________________________________________

  1.  Introduction



  1.1.  Purpose


  The GNU/Linux OS has evolved from its origins in hackerdom to a full
  blown UNIX, capable of rivaling any commercial UNIX.  It now provides
  an inexpensive base to build a great workstation.  It has shed its
  hardware dependencies, having been ported to DEC Alphas, Sparcs,
  PowerPCs, and many others.  This potential speed boost along with its
  networking support will make it great for workstation clusters.  As a
  workstation it allows for all sorts of research and development,
  including artificial intelligence and artificial life.



  The purpose of this Mini-Howto is to provide a source to find out
  about various software packages, code libraries, and anything else
  that will help someone get started working with (and find resources
  for) artificial intelligence, artificial life, etc.  All done with
  GNU/Linux specifically in mind.



  1.2.  Where to find this software


  All this software should be available via the net (ftp || http).  The
  links to where to find it will be provided in the description of each
  package.  There will also be plenty of software not covered on these
  pages (which is usually platform independent) located on one of the
  resources listed on the links section of the Master Site (given
  above).



  1.3.  Updates and comments



  If you find any mistakes, know of updates to one of the items below,
  or have problems compiling and of the applications, please mail me at:
  jae@NOSPAM-zhar.net and I'll see what I can do.


  If you know of any AI/Alife applications, class libraries, etc. Please
  email me about them. Include your name, ftp and/or http sites where
  they can be found, plus a brief overview/commentary on the software
  (this info would make things a lot easier on me... but don't feel
  obligated ;).


  I know that keeping this list up to date and expanding it will take
  quite a bit of work. So please be patient (I do have other projects).
  I hope you will find this document helpful.


  1.4.  Copyright/License

  Copyright (c) 1996-2000 John A. Eikenberry

  LICENSE

  This document may be reproduced and distributed in whole or in part,
  in any medium physical or electronic, provided that this license
  notice is displayed in the reproduction. Commercial redistribution is
  permitted and encouraged. Thirty days advance notice, via email to the
  author, of redistribution is appreciated, to give the authors time to
  provide updated documents.

  A. REQUIREMENTS OF MODIFIED WORKS

  All modified documents, including translations, anthologies, and
  partial documents, must meet the following requirements:



    The modified version must be labeled as such.

    The person making the modifications must be identified.


    Acknowledgement of the original author must be retained.

    The location of the original unmodified document be identified.

    The original author's name(s) may not be used to assert or imply
     endorsement of the resulting document without the original author's
     permission.

  In addition it is requested (not required) that:


    The modifications (including deletions) be noted.

    The author be notified by email of the modification in advance of
     redistribution, if an email address is provided in the document.

  As a special exception, anthologies of LDP documents may include a
  single copy of these license terms in a conspicuous location within
  the anthology and replace other copies of this license with a
  reference to the single copy of the license without the document being
  considered "modified" for the purposes of this section.

  Mere aggregation of LDP documents with other documents or programs on
  the same media shall not cause this license to apply to those other
  works.

  All translations, derivative documents, or modified documents that
  incorporate this document may not have more restrictive license terms
  than these, except that you may require distributors to make the
  resulting document available in source format.



  2.

  Traditional Artificial Intelligence

  Traditional AI is based around the ideas of logic, rule systems,
  linguistics, and the concept of rationality.  At its roots are
  programming languages such as Lisp and Prolog.  Expert systems are the
  largest successful example of this paradigm.  An expert system
  consists of a detailed knowledge base and a complex rule system to
  utilize it.  Such systems have been used for such things as medical
  diagnosis support and credit checking systems.



  2.1.  AI class/code libraries


  These are libraries of code or classes for use in programming within
  the artificial intelligence field.  They are not meant as stand alone
  applications, but rather as tools for building your own applications.



     ACL2

       Web site: www.telent.net/cliki/ACL2

        ACL2 (A Computational Logic for Applicative Common Lisp) is a
        theorem prover for industrial applications. It is both a
        mathematical logic and a system of tools for constructing proofs
        in the logic.  ACL2 works with GCL (GNU Common Lisp).
     AI Search II

       WEB site: www.bell-labs.com/topic/books/ooai-book/

        Submitted by: Peter M. Bouthoorn


        Basically, the library offers the programmer a set of search
        algorithms that may be used to solve all kind of different
        problems. The idea is that when developing problem solving
        software the programmer should be able to concentrate on the
        representation of the problem to be solved and should not need
        to bother with the implementation of the search algorithm that
        will be used to actually conduct the search. This idea has been
        realized by the implementation of a set of search classes that
        may be incorporated in other software through C++'s features of
        derivation and inheritance.  The following search algorithms
        have been implemented:


        - depth-first tree and graph search.  - breadth-first tree and
        graph search.  - uniform-cost tree and graph search.  - best-
        first search.  - bidirectional depth-first tree and graph
        search.  - bidirectional breadth-first tree and graph search.  -
        AND/OR depth tree search.  - AND/OR breadth tree search.


        This library has a corresponding book, "Object-Oriented
        Artificial Instelligence, Using C++".



     Chess In Lisp (CIL)

       FTP site: chess.onenet.net/pub/chess/uploads/projects/


        The CIL (Chess In Lisp) foundation is a Common Lisp
        implementaion of all the core functions needed for development
        of chess applications.  The main purpose of the CIL project is
        to get AI researchers interested in using Lisp to work in the
        chess domain.



     DAI

       Web site: starship.skyport.net/crew/gandalf/DNET/AI


        A library for the Python programming language that provides an
        object oriented interface to the CLIPS expert system tool. It
        includes an interface to COOL (CLIPS Object Oriented Language)
        that allows:

       Investigate COOL classes

       Create and manipulate with COOL instances

       Manipulate with COOL message-handler's

       Manipulate with Modules

     Nyquist

       Web site:
        www.cs.cmu.edu/afs/cs.cmu.edu/project/music/web/music.html


        The Computer Music Project at CMU is developing computer music
        and interactive performance technology to enhance human musical
        experience and creativity. This interdisciplinary effort draws
        on Music Theory, Cognitive Science, Artificial Intelligence and
        Machine Learning, Human Computer Interaction, Real-Time Systems,
        Computer Graphics and Animation, Multimedia, Programming
        Languages, and Signal Processing. A paradigmatic example of
        these interdisciplinary efforts is the creation of interactive
        performances that couple human musical improvisation with
        intelligent computer agents in real-time.



     PDKB

       Web site: lynx.eaze.net/~pdkb/web/

       SourceForge site: sourceforge.net/project/?group_id=1449

        Public Domain Knowledge Bank (PDKB) is an Artificial
        Intelligence Knowledge Bank of common sense rules and facts. It
        is based on the Cyc Upper Ontology and the MELD language.



     Python Fuzzy Logic Module

       FTP site: ftp://ftp.csh.rit.edu/pub/members/retrev/

        A simple python module for fuzzy logic. The file is 'fuz.tar.gz'
        in this directory. The author plans to also write a simple
        genetic algorithm and a neural net library as well. Check the
        00_index file in this directory for release info.



     Screamer

       Web site: www.cis.upenn.edu/~screamer-tools/home.html


        Screamer is an extension of Common Lisp that adds support for
        nondeterministic programming. Screamer consists of two levels.
        The basic nondeterministic level adds support for backtracking
        and undoable side effects.  On top of this nondeterministic
        substrate, Screamer provides a comprehensive constraint
        programming language in which one can formulate and solve mixed
        systems of numeric and symbolic constraints. Together, these two
        levels augment Common Lisp with practically all of the
        functionality of both Prolog and constraint logic programming
        languages such as CHiP and CLP(R).  Furthermore, Screamer is
        fully integrated with Common Lisp. Screamer programs can coexist
        and interoperate with other extensions to Common Lisp such as
        CLOS, CLIM and Iterate.



     ThoughtTreasure

       Web site: www.signiform.com/tt/htm/tt.htm

        ThoughtTreasure is a project to create a database of commonsense
        rules for use in any application. It consists of a database of a
        little over 100K rules and a C API to integrate it with your
        applications. Python, Perl, Java and TCL wrappers are already
        available.



  2.2.

  AI software kits, applications, etc.


  These are various applications, software kits, etc. meant for research
  in the field of artificial intelligence. Their ease of use will vary,
  as they were designed to meet some particular research interest more
  than as an easy to use commercial package.



     ASA - Adaptive Simulated Annealing

       Web site: www.ingber.com/#ASA-CODE

       FTP site: ftp.ingber.com/


        ASA (Adaptive Simulated Annealing) is a powerful global
        optimization C-code algorithm especially useful for nonlinear
        and/or stochastic systems.


        ASA is developed to statistically find the best global fit of a
        nonlinear non-convex cost-function over a D-dimensional space.
        This algorithm permits an annealing schedule for 'temperature' T
        decreasing exponentially in annealing-time k, T = T_0 exp(-c
        k^1/D).  The introduction of re-annealing also permits
        adaptation to changing sensitivities in the multi-dimensional
        parameter-space. This annealing schedule is faster than fast
        Cauchy annealing, where T = T_0/k, and much faster than
        Boltzmann annealing, where T = T_0/ln k.



     Babylon

       FTP site: ftp.gmd.de/gmd/ai-research/Software/Babylon/


        BABYLON is a modular, configurable, hybrid environment for
        developing expert systems. Its features include objects, rules
        with forward and backward chaining, logic (Prolog) and
        constraints. BABYLON is implemented and embedded in Common Lisp.

     CLEARS

       Web site: www.coli.uni-sb.de/~clears/


        The CLEARS system is an interactive graphical environment for
        computational semantics. The tool allows exploration and
        comparison of different semantic formalisms, and their
        interaction with syntax. This enables the user to get an idea of
        the range of possibilities of semantic construction, and also
        where there is real convergence between theories.



     CLIG

       Web site: www.ags.uni-sb.de/~konrad/clig.html


        CLIG is an interactive, extendible grapher for visualizing
        linguistic data structures like trees, feature structures,
        Discourse Representation Structures (DRS), logical formulas etc.
        All of these can be freely mixed and embedded into each other.
        The grapher has been designed both to be stand-alone and to be
        used as an add-on for linguistic applications which display
        their output in a graphical manner.



     CLIPS

       Web site: www.jsc.nasa.gov/~clips/CLIPS.html

       FTP site: cs.cmu.edu/afs/cs.cmu.edu/project/ai-
        repository/ai/areas/expert/systems/clips



        CLIPS is a productive development and delivery expert system
        tool which provides a complete environment for the construction
        of rule and/or object based expert systems.


        CLIPS provides a cohesive tool for handling a wide variety of
        knowledge with support for three different programming
        paradigms: rule-based, object-oriented and procedural.  Rule-
        based programming allows knowledge to be represented as
        heuristics, or "rules of thumb," which specify a set of actions
        to be performed for a given situation. Object-oriented
        programming allows complex systems to be modeled as modular
        components (which can be easily reused to model other systems or
        to create new components).  The procedural programming
        capabilities provided by CLIPS are similar to capabilities found
        in languages such as C, Pascal, Ada, and LISP.



     EMA-XPS - A Hybrid Graphic Expert System Shell

       Web site: wmwap1.math.uni-wuppertal.de:80/EMA-XPS/


        EMA-XPS is a hybrid graphic expert system shell based on the
        ASCII-oriented shell Babylon 2.3 of the German National Research
        Center for Computer Sciences (GMD). In addition to Babylon's AI-
        power (object oriented data representation, forward and backward
        chained rules - collectible into sets, horn clauses, and
        constraint networks) a graphic interface based on the X11 Window
        System and the OSF/Motif Widget Library has been provided.



     FOOL & FOX

       FTP site: ntia.its.bldrdoc.gov/pub/fuzzy/prog/


        FOOL stands for the Fuzzy Organizer OLdenburg. It is a result
        from a project at the University of Oldenburg. FOOL is a
        graphical user interface to develop fuzzy rulebases.  FOOL will
        help you to invent and maintain a database that specifies the
        behavior of a fuzzy-controller or something like that.


        FOX is a small but powerful fuzzy engine which reads this
        database, reads some input values and calculates the new control
        value.



     FUF and SURGE

       Web site: www.dfki.de/lt/registry/generation/fuf.html

       FTP site: ftp.cs.columbia.edu/pub/fuf/

        FUF is an extended implementation of the formalism of functional
        unification grammars (FUGs) introduced by Martin Kay specialized
        to the task of natural language generation. It adds the
        following features to the base formalism:

       Types and inheritance.

       Extended control facilities (goal freezing, intelligent
        backtracking).

       Modular syntax.

        These extensions allow the development of large grammars which
        can be processed efficiently and can be maintained and
        understood more easily.  SURGE is a large syntactic realization
        grammar of English written in FUF. SURGE is developed to serve
        as a black box syntactic generation component in a larger
        generation system that encapsulates a rich knowledge of English
        syntax. SURGE can also be used as a platform for exploration of
        grammar writing with a generation perspective.



     The Grammar Workbench

       Web site: www.cs.kun.nl/agfl/GWB.html



        The Grammar Workbench, or GWB for short, is an environment for
        the comfortable development of Affix Grammars in the AGFL-
        formalism. Its purposes are:


       to allow the user to input, inspect and modify a grammar;

       to perform consistency checks on the grammar;

       to compute grammar properties;

       to generate example sentences;

       to assist in performing grammar transformations.



     GSM Suite

       Web site: www.slip.net/~andrewm/gsm/


        The GSM Suite is a set of programs for using Finite State
        Machines in a graphical fashion. The suite consists of programs
        that edit, compile, and print state machines. Included in the
        suite is an editor program, gsmedit, a compiler, gsm2cc, that
        produces a C++ implementation of a state machine, a PostScript
        generator, gsm2ps, and two other minor programs. GSM is licensed
        under the GNU Public License and so is free for your use under
        the terms of that license.



     Illuminator

       Web site:
        documents.cfar.umd.edu/resources/source/illuminator.html


        Illuminator is a toolset for developing OCR and Image
        Understanding applications.  Illuminator has two major parts: a
        library for representing, storing and retrieving OCR
        information, heretofore called dafslib, and an X-Windows "DAFS"
        file viewer, called illum. Illuminator and DAFS lib were
        designed to supplant existing OCR formats and become a standard
        in the industry. They particularly are extensible to handle more
        than just English.

        The features of this release:

       5 magnification levels for images

       flagged characters and words

       unicode support -- American, British, French, German, Greek,
        Italian, MICR, Norwegian, Russian, Spanish, Swedish, keyboards

       reads DAFS, TIFF's, PDA's (image only)

       save to DAFS, ASCII/UTF or Unicode

       Entity Viewer - shows properties, character choices, bounding
        boxes image fragment for a selected entity, change type, change
        content, hierarchy mode



     Jess, the Java Expert System Shell


       Web site: herzberg.ca.sandia.gov/jess/


        Jess is a clone of the popular CLIPS expert system shell written
        entirely in Java. With Jess, you can conveniently give your
        applets the ability to 'reason'. Jess is compatible with all
        versions of Java starting with version 1.0.2. Jess implements
        the following constructs from CLIPS: defrules, deffunctions,
        defglobals, deffacts, and deftemplates.



     learn

       FTP site: sunsite.unc.edu/pub/Linux/apps/cai/


        Learn is a vocable learning program with memory model.



     Otter: An Automated Deduction System

       Web site: www-unix.mcs.anl.gov/AR/otter/


        Our current automated deduction system  Otter is designed to
        prove theorems stated in first-order logic with equality.
        Otter's inference rules are based on resolution and
        paramodulation, and it includes facilities for term rewriting,
        term orderings, Knuth-Bendix completion, weighting, and
        strategies for directing and restricting searches for proofs.
        Otter can also be used as a symbolic calculator and has an
        embedded equational programming system.



     NICOLE

       Web site: nicole.sourceforge.net

        It is an attempt to simulate a conversation by learning how
        words are related to other words. A Human communicates with
        NICOLE via the keyboard and NICOLE responds back with its own
        sentences which are automatically generated, based on what
        NICOLE has stored in it's database.  Each new sentence that has
        been typed in, and NICOLE doesn't know about it, it is included
        into NICOLE's database, thus extending the knowledge base of
        NICOLE.



     PVS

       Web site: pvs.csl.sri.com/

        PVS is a verification system: that is, a specification language
        integrated with support tools and a theorem prover. It is
        intended to capture the state-of-the-art in mechanized formal
        methods and to be sufficiently rugged that it can be used for
        significant applications. PVS is a research prototype: it
        evolves and improves as we develop or apply new capabilities,
        and as the stress of real use exposes new requirements.

     RIPPER

       Web site: www.research.att.com/~wcohen/ripperd.html


        Ripper is a system for fast effective rule induction. Given a
        set of data, Ripper will learn a set of rules that will predict
        the patterns in the data. Ripper is written in ASCI C and comes
        with documentation and some sample problems.



     SNePS

       Web site: www.cs.buffalo.edu/pub/sneps/WWW/

       FTP site: ftp.cs.buffalo.edu/pub/sneps/

        The long-term goal of The SNePS Research Group is the design and
        construction of a natural-language-using computerized cognitive
        agent, and carrying out the research in artificial intelligence,
        computational linguistics, and cognitive science necessary for
        that endeavor. The three-part focus of the group is on knowledge
        representation, reasoning, and natural-language understanding
        and generation. The group is widely known for its development of
        the SNePS knowledge representation/reasoning system, and Cassie,
        its computerized cognitive agent.



     Soar

       Web site: bigfoot.eecs.umich.edu/~soar/

       FTP site: cs.cmu.edu/afs/cs/project/soar/public/Soar6/


        Soar has been developed to be a general cognitive architecture.
        We intend ultimately to enable the Soar architecture to:

       work on the full range of tasks expected of an intelligent
        agent, from highly routine to extremely difficult, open-ended
        problems

       represent and use appropriate forms of knowledge, such as
        procedural, declarative, episodic, and possibly iconic

       employ the full range of problem solving methods

       interact with the outside world and

       learn about all aspects of the tasks and its performance on
        them.

        In other words, our intention is for Soar to support all the
        capabilities required of a general intelligent agent.
        http://wwwis.cs.utwente.nl:8080/ tcm/index.html



     TCM


       Web site: wwwis.cs.utwente.nl:8080/~tcm/index.html

       FTP site: ftp.cs.vu.nl/pub/tcm/


        TCM (Toolkit for Conceptual Modeling) is our suite of graphical
        editors. TCM contains graphical editors for Entity-Relationship
        diagrams, Class-Relationship diagrams, Data and Event Flow
        diagrams, State Transition diagrams, Jackson Process Structure
        diagrams and System Network diagrams, Function Refinement trees
        and various table editors, such as a Function-Entity table
        editor and a Function Decomposition table editor.  TCM is easy
        to use and performs numerous consistency checks, some of them
        immediately, some of them upon request.



     WEKA

       Web site: lucy.cs.waikato.ac.nz/~ml/


        WEKA (Waikato Environment for Knowledge Analysis) is an state-
        of-the-art facility for applying machine learning techniques to
        practical problems. It is a comprehensive software "workbench"
        that allows people to analyse real-world data. It integrates
        different machine learning tools within a common framework and a
        uniform user interface. It is designed to support a "simplicity-
        first" methodology, which allows users to experiment
        interactively with simple machine learning tools before looking
        for more complex solutions.



  3.  Connectionism

  Connectionism is a technical term for a group of related techniques.
  These techniques include areas such as Artificial Neural Networks,
  Semantic Networks and a few other similar ideas. My present focus is
  on neural networks (though I am looking for resources on the other
  techniques). Neural networks are programs designed to simulate the
  workings of the brain. They consist of a network of small
  mathematical-based nodes, which work together to form patterns of
  information.  They have tremendous potential and currently seem to be
  having a great deal of success with image processing and robot
  control.



  3.1.  Connectionist class/code libraries


  These are libraries of code or classes for use in programming within
  the Connectionist field.  They are not meant as stand alone
  applications, but rather as tools for building your own applications.



     ANSI-C Neural Networks

       Web site: www.geocities.com/CapeCanaveral/1624/

        This site contains ANSC-C source code for 8 types of neural
        nets, including:

       Adaline Network

       Backpropagation

       Hopfield Model

       (BAM) Bidirectional Associative Memory

       Boltzmann Machine

       Counterpropagation

       (SOM) Self-Organizing Map

       (ART1) Adaptive Resonance Theory

        They were designed to help turn the theory of a particular
        network model into the design for a simulator implementation ,
        and to help with embeding an actual application into a
        particular network model.



     BELIEF

       Web site: www.cs.cmu.edu/afs/cs/project/ai-
        repository/ai/areas/reasonng/probabl/belief/


        BELIEF is a Common Lisp implementation of the Dempster and Kong
        fusion and propagation algorithm for Graphical Belief Function
        Models and the Lauritzen and Spiegelhalter algorithm for
        Graphical Probabilistic Models. It includes code for
        manipulating graphical belief models such as Bayes Nets and
        Relevance Diagrams (a subset of Influence Diagrams) using both
        belief functions and probabilities as basic representations of
        uncertainty. It uses the Shenoy and Shafer version of the
        algorithm, so one of its unique features is that it supports
        both probability distributions and belief functions.  It also
        has limited support for second order models (probability
        distributions on parameters).


     bpnn.py

       Web site: www.enme.ucalgary.ca/~nascheme/python/

        A simple back-propogation ANN in Python.



     CONICAL

       Web site: strout.net/conical/

        CONICAL is a C++ class library for building simulations common
        in computational neuroscience. Currently its focus is on
        compartmental modeling, with capabilities similar to GENESIS and
        NEURON. A model neuron is built out of compartments, usually
        with a cylindrical shape. When small enough, these open-ended
        cylinders can approximate nearly any geometry. Future classes
        may support reaction-diffusion kinetics and more. A key feature
        of CONICAL is its cross-platform compatibility; it has been
        fully co-developed and tested under Unix, DOS, and Mac OS.



     IDEAL

       Web site: www.rpal.rockwell.com/ideal.html



        IDEAL is a test bed for work in influence diagrams and Bayesian
        networks. It contains various inference algorithms for belief
        networks and evaluation algorithms for influence diagrams. It
        contains facilities for creating and editing influence diagrams
        and belief networks.


        IDEAL is written in pure Common Lisp and so it will run in
        Common Lisp on any platform. The emphasis in writing IDEAL has
        been on code clarity and providing high level programming
        abstractions. It thus is very suitable for experimental
        implementations which need or extend belief network technology.


        At the highest level, IDEAL can be used as a subroutine library
        which provides belief network inference and influence diagram
        evaluation as a package. The code is documented in a detailed
        manual and so it is also possible to work at a lower level on
        extensions of belief network methods.


        IDEAL comes with an optional graphic interface written in CLIM.
        If your Common Lisp also has CLIM, you can run the graphic
        interface.



     Matrix Class

       FTP site: ftp.cs.ucla.edu/pub/


        A simple, fast, efficient C++ Matrix class designed for
        scientists and engineers. The Matrix class is well suited for
        applications with complex math algorithms. As an demonstration
        of the Matrix class, it was used to implement the backward error
        propagation algorithm for a multi-layer feed-forward artificial
        neural network.



     nunu

       Web site: ruby.ddiworld.com/jreed/web/software/nn.html


        nunu is a multi-layered, scriptable, back-propagation neural
        network.  It is build to be used for intensive computation
        problems scripted in shell scripts. It is written in C++ using
        the STL. nn is based on material from the "Introduction to the
        Theory of Neural Computation" by John Hertz, Anders Krogh, and
        Richard G. Palmer, chapter 6.
     Pulcinella

       Web site: iridia.ulb.ac.be/pulcinella/Welcome.html


        Pulcinella is written in CommonLisp, and appears as a library of
        Lisp functions for creating, modifying and evaluating valuation
        systems. Alternatively, the user can choose to interact with
        Pulcinella via a graphical interface (only available in Allegro
        CL). Pulcinella provides primitives to build and evaluate
        uncertainty models according to several uncertainty calculi,
        including probability theory, possibility theory, and Dempster-
        Shafer's theory of belief functions; and the possibility theory
        by Zadeh, Dubois and Prade's. A User's Manual is available on
        request.



     S-ElimBel

       Web site (???): www.spaces.uci.edu/thiery/elimbel/


        S-ElimBel is an algorithm that computes the belief in a Bayesian
        network, implemented in MIT-Scheme. This algorithm has the
        particularity of being rather easy to understand. Moreover, one
        can apply it to any kind of Bayesian network - it being singly
        connected or muliply connected. It is, however, less powerful
        than the standard algorithm of belief propagation.  Indeed, the
        computation has to be reconducted entirely for each new evidence
        added to the network. Also, one needs to run the algorithm as
        many times as one has nodes for which the belief is wanted.



     Software for Flexible Bayesian Modeling

       Web site: www.cs.utoronto.ca/~radford/fbm.software.html


        This software implements flexible Bayesian models for regression
        and classification applications that are based on multilayer
        perceptron neural networks or on Gaussian processes.  The
        implementation uses Markov chain Monte Carlo methods.  Software
        modules that support Markov chain sampling are included in the
        distribution, and may be useful in other applications.



     Spiderweb2

       Web site: www.cs.nyu.edu/~klap7794/spiderweb2.html


        A C++ artificial neual net library.  Spiderweb2 is a complete
        rewrite of the original Spiderweb library, it has grown into a
        much more flexible and object-oriented system. The biggest
        change is that each neuron object is responsible for its own
        activations and updates, with the network providing only the
        scheduling aspect. This is a very powerful change, and it allows
        easy modification and experimentation with various network
        architectures and neuron types.
     Symbolic Probabilistic Inference (SPI)

       FTP site: ftp.engr.orst.edu/pub/dambrosi/spi/

       Paper (ijar-94.ps): ftp.engr.orst.edu/pub/dambrosi/


        Contains Common Lisp function libraries to implement SPI type
        baysean nets.  Documentation is very limited.  Features:

       Probabilities, Local Expression Language Utilities, Explanation,
        Dynamic Models, and a TCL/TK based GUI.



     TresBel

       FTP site: iridia.ulb.ac.be/pub/hongxu/software/


        Libraries containing (Allegro) Common Lisp code for Belief
        Functions (aka. Dempster-Shafer evidential reasoning) as a
        representation of uncertainty. Very little documentation. Has a
        limited GUI.



     Various (C++) Neural Networks

       Web site: www.dontveter.com/nnsoft/nnsoft.html


        Example neural net codes from the book, The       Pattern
        Recognition Basics of AI.  These are simple example codes of
        these various neural nets. They work well as a good starting
        point for simple experimentation and for learning what the code
        is like behind the simulators. The types of networks available
        on this site are: (implemented in C++)



       The Backprop Package

       The Nearest Neighbor Algorithms

       The Interactive Activation Algorithm

       The Hopfield and Boltzman machine Algorithms

       The Linear Pattern Classifier

       ART I

       Bi-Directional Associative Memory

       The Feedforward Counter-Propagation Network



  3.2.

  Connectionist software kits/applications


  These are various applications, software kits, etc. meant for research
  in the field of Connectionism. Their ease of use will vary, as they
  were designed to meet some particular research interest more than as
  an easy to use commercial package.



     Aspirin - MIGRAINES
        (am6.tar.Z on ftp site)

       FTP site: sunsite.unc.edu/pub/academic/computer-science/neural-
        networks/programs/Aspirin/


        The software that we are releasing now is for creating, and
        evaluating, feed-forward networks such as those used with the
        backpropagation learning algorithm. The software is aimed both
        at the expert programmer/neural network researcher who may wish
        to tailor significant portions of the system to his/her precise
        needs, as well as at casual users who will wish to use the
        system with an absolute minimum of effort.



     DDLab

       Web site: www.santafe.edu/~wuensch/ddlab.html

       FTP site: ftp.santafe.edu/pub/wuensch/

        DDLab is an interactive graphics program for research into the
        dynamics of finite binary networks, relevant to the study of
        complexity, emergent phenomena, neural networks, and aspects of
        theoretical biology such as gene regulatory networks. A network
        can be set up with any architecture between regular CA (1d or
        2d) and "random Boolean networks" (networks with arbitrary
        connections and heterogeneous rules). The network may also have
        heterogeneous neighborhood sizes.



     GENESIS

       Web site: www.bbb.caltech.edu/GENESIS/

       FTP site: genesis.bbb.caltech.edu/pub/genesis/


        GENESIS (short for GEneral NEural SImulation System) is a
        general purpose simulation platform which was developed to
        support the simulation of neural systems ranging from complex
        models of single neurons to simulations of large networks made
        up of more abstract neuronal components. GENESIS has provided
        the basis for laboratory courses in neural simulation at both
        Caltech and the Marine Biological Laboratory in Woods Hole, MA,
        as well as several other institutions. Most current GENESIS
        applications involve realistic simulations of biological neural
        systems. Although the software can also model more abstract
        networks, other simulators are more suitable for backpropagation
        and similar connectionist modeling.
     JavaBayes

       Web site: www.cs.cmu.edu/People/javabayes/index.html/


        The JavaBayes system is a set of tools, containing a graphical
        editor, a core inference engine and a parser.  JavaBayes can
        produce:

       the marginal distribution for any variable in a network.

       the expectations for univariate functions (for example, expected
        value for variables).

       configurations with maximum a posteriori probability.

       configurations with maximum a posteriori expectation for
        univariate functions.



     Jbpe

       Web site: cs.felk.cvut.cz/~koutnij/studium/jbpe.html


        Jbpe is a back-propagation neural network editor/simulator.

        Features

       Standart back-propagation networks creation.

       Saving network as a text file, which can be edited and loaded
        back.

       Saving/loading binary file

       Learning from a text file (with structure specified below),
        number of learning periods / desired network energy can be
        specified as a criterion.

       Network recall



     Neural Network Generator

       Web site: www.idsia.ch/~rafal/research.html

       FTP site:  >ftp.idsia.ch/pub/rafal


        The Neural Network Generator is a genetic algorithm for the
        topological optimization of feedforward neural networks. It
        implements the Semantic Changing Genetic Algorithm and the Unit-
        Cluster Model. The Semantic Changing Genetic Algorithm is an
        extended genetic algorithm that allows fast dynamic adaptation
        of the genetic coding through population analysis. The Unit-
        Cluster Model is an approach to the construction of modular
        feedforward networks with a ''backbone'' structure.


        NOTE: To compile this on Linux requires one change in the
        Makefiles.  You will need to change '-ltermlib' to '-ltermcap'.
     Neureka ANS (nn/xnn)

       Web site: www.bgif.no/neureka/

       FTP site: ftp.ii.uib.no/pub/neureka/



        nn is a high-level neural network specification language. The
        current version is best suited for feed-forward nets, but
        recurrent models can and have been implemented, e.g. Hopfield
        nets, Jordan/Elman nets, etc.  In nn, it is easy to change
        network dynamics. The nn compiler can generate C code or
        executable programs (so there must be a C compiler available),
        with a powerful command line interface (but everything may also
        be controlled via the graphical interface, xnn). It is possible
        for the user to write C routines that can be called from inside
        the nn specification, and to use the nn specification as a
        function that is called from a C program. Please note that no
        programming is necessary in order to use the network models that
        come with the system (`netpack').


        xnn is a graphical front end to networks generated by the nn
        compiler, and to the compiler itself. The xnn graphical
        interface is intuitive and easy to use for beginners, yet
        powerful, with many possibilities for visualizing network data.


        NOTE: You have to run the install program that comes with this
        to get the license key installed. It gets put (by default) in
        /usr/lib. If you (like myself) want to install the package
        somewhere other than in the /usr directory structure (the
        install program gives you this option) you will have to set up
        some environmental variables (NNLIBDIR & NNINCLUDEDIR are
        required). You can read about these (and a few other optional
        variables) in appendix A of the documentation (pg 113).



     NEURON

       Web site: www.neuron.yale.edu/neuron.html

       FTP site: ftp.neuron.yale.edu/neuron/unix/

        NEURON is an extensible nerve modeling and simulation program.
        It allows you to create complex nerve models by connecting
        multiple one-dimensional sections together to form arbitrary
        cell morphologies, and allows you to insert multiple membrane
        properties into these sections (including channels, synapses,
        ionic concentrations, and counters). The interface was designed
        to present the neural modeler with a intuitive environment and
        hide the details of the numerical methods used in the
        simulation.



     PDP++

       Web site: www.cnbc.cmu.edu/PDP++/

       FTP site (US): cnbc.cmu.edu/pub/pdp++/

       FTP site (Europe): unix.hensa.ac.uk/mirrors/pdp++/


        As the field of Connectionist modeling has grown, so has the
        need for a comprehensive simulation environment for the
        development and testing of Connectionist models. Our goal in
        developing PDP++ has been to integrate several powerful software
        development and user interface tools into a general purpose
        simulation environment that is both user friendly and user
        extensible. The simulator is built in the C++ programming
        language, and incorporates a state of the art script interpreter
        with the full expressive power of C++. The graphical user
        interface is built with the Interviews toolkit, and allows full
        access to the data structures and processing modules out of
        which the simulator is built. We have constructed several useful
        graphical modules for easy interaction with the structure and
        the contents of neural networks, and we've made it possible to
        change and adapt many things. At the programming level, we have
        set things up in such a way as to make user extensions as
        painless as possible. The programmer creates new C++ objects,
        which might be new kinds of units or new kinds of processes;
        once compiled and linked into the simulator, these new objects
        can then be accessed and used like any other.



     RNS

       Web site: www.cs.cmu.edu/afs/cs/project/ai-
        repository/ai/areas/neural/systems/rns/

        RNS (Recurrent Network Simulator) is a simulator for recurrent
        neural networks. Regular neural networks are also supported. The
        program uses a derivative of the back-propagation algorithm, but
        also includes other (not that well tested) algorithms.

        Features include

       freely choosable connections, no restrictions besides memory or
        CPU constraints

       delayed links for recurrent networks

       fixed values or thresholds can be specified for weights

       (recurrent) back-propagation, Hebb, differential Hebb, simulated
        annealing and more

       patterns can be specified with bits, floats, characters,
        numbers, and random bit patterns with Hamming distances can be
        chosen for you

       user definable error functions

       output results can be used without modification as input



     Simple Neural Net (in Python)

       Web site: starship.python.net/crew/amk/unmaintained/


        Simple neural network code, which implements a class for 3-level
        networks (input, hidden, and output layers). The only learning
        rule implemented is simple backpropagation. No documentation (or
        even comments) at all, because this is simply code that I use to
        experiment with. Includes modules containing sample datasets
        from Carl G. Looney's NN book. Requires the Numeric extensions.



     SCNN

       Web site: apx00.physik.uni-frankfurt.de/e_ag_rt/SCNN/


        SCNN is an universal simulating system for Cellular Neural
        Networks (CNN).  CNN are analog processing neural networks with
        regular and local interconnections, governed by a set of
        nonlinear ordinary differential equations. Due to their local
        connectivity, CNN are realized as VLSI chips, which operates at
        very high speed.



     Semantic Networks in Python

       Web site: strout.net/info/coding/python/ai/index.html


        The semnet.py module defines several simple classes for building
        and using semantic networks.  A semantic network is a way of
        representing knowledge, and it enables the program to do simple
        reasoning with very little effort on the part of the programmer.


        The following classes are defined:

       Entity: This class represents a noun; it is something which can
        be related to other things, and about which you can store facts.

       Relation: A Relation is a type of relationship which may exist
        between two entities.  One special relation, "IS_A", is
        predefined because it has special meaning (a sort of logical
        inheritance).

       Fact: A Fact is an assertion that a relationship exists between
        two entities.


        With these three object types, you can very quickly define
        knowledge about a set of objects, and query them for logical
        conclusions.



     SNNS

       Web site: www.informatik.uni-stuttgart.de/ipvr/bv/projekte/snns/

       FTP site: ftp.informatik.uni-stuttgart.de/pub/SNNS/

        Stuttgart Neural Net Simulator (version 4.1).  An awesome neural
        net simulator. Better than any commercial simulator I've seen.
        The simulator kernel is written in C (it's fast!). It supports
        over 20 different network architectures, has 2D and 3D X-based
        graphical representations, the 2D GUI has an integrated network
        editor, and can generate a separate NN program in C. SNNS is
        very powerful, though a bit difficult to learn at first. To help
        with this it comes with example networks and tutorials for many
        of the architectures.  ENZO, a supplementary system allows you
        to evolve your networks with genetic algorithms.


        There is a debian package of SNNS available. So just get it (and
        use alien to convert it to RPM if you need to).



     SPRLIB/ANNLIB

       Web site: www.ph.tn.tudelft.nl/~sprlib/


        SPRLIB (Statistical Pattern Recognition Library) was developed
        to support the easy construction and simulation of pattern
        classifiers. It consist of a library of functions (written in C)
        that can be called from your own program. Most of the well-known
        classifiers are present (k-nn, Fisher, Parzen, ....), as well as
        error estimation and dataset generation routines.


        ANNLIB (Artificial Neural Networks Library) is a neural network
        simulation library based on the data architecture laid down by
        SPRLIB. The library contains numerous functions for creating,
        training and testing feed-forward networks.  Training algorithms
        include back-propagation, pseudo-Newton, Levenberg-Marquardt,
        conjugate gradient descent, BFGS.... Furthermore, it is possible
        - due to the datastructures' general applicability - to build
        Kohonen maps and other more exotic network architectures using
        the same data types.



     TOOLDIAG

       Web site: www.inf.ufes.br/~thomas/www/home/tooldiag.html

       FTP site: ftp.inf.ufes.br/pub/tooldiag/

        TOOLDIAG is a collection of methods for statistical pattern
        recognition. The main area of application is classification. The
        application area is limited to multidimensional continuous
        features, without any missing values. No symbolic features
        (attributes) are allowed. The program in implemented in the 'C'
        programming language and was tested in several computing
        environments.



  4.

  Evolutionary Computing

  Evolutionary computing is actually a broad term for a vast array of
  programming techniques, including genetic algorithms, complex adaptive
  systems, evolutionary programming, etc.  The main thrust of all these
  techniques is the idea of evolution. The idea that a program can be
  written that will evolve toward a certain goal.  This goal can be
  anything from solving some engineering problem to winning a game.



  4.1.

  EC class/code libraries


  These are libraries of code or classes for use in programming within
  the evolutionary computation field.  They are not meant as stand alone
  applications, but rather as tools for building your own applications.



     daga

       Web site: GARAGe.cps.msu.edu/software/software-index.html


        daga is an experimental release of a 2-level genetic algorithm
        compatible with the GALOPPS GA software. It is a meta-GA which
        dynamically evolves a population of GAs to solve a problem
        presented to the lower-level GAs. When multiple GAs (with
        different operators, parameter settings, etc.) are
        simultaneously applied to the same problem, the ones showing
        better performance have a higher probability of surviving and
        "breeding" to the next macro-generation (i.e., spawning new
        "daughter"-GAs with characteristics inherited from the parental
        GA or GAs.  In this way, we try to encourage good problem-
        solving strategies to spread to the whole population of GAs.



     EO

       Web site: geneura.ugr.es/~jmerelo/EO.html

        EO is a templates-based, ANSI-C++ compliant evolutionary
        computation library. It contains classes for any kind of
        evolutionary computation (specially genetic algorithms) you
        might come up to. It is component-based, so that if you don't
        find the class you need in it, it is very easy to subclass
        existing abstract or concrete class.



     FORTRAN GA

       Web site: www.staff.uiuc.edu/~carroll/ga.html


        This program is a FORTRAN version of a genetic algorithm driver.
        This code initializes a random sample of individuals with
        different parameters to be optimized using the genetic algorithm
        approach, i.e.  evolution via survival of the fittest.  The
        selection scheme used is tournament selection with a shuffling
        technique for choosing random pairs for mating.  The routine
        includes binary coding for the individuals, jump mutation, creep
        mutation, and the option for single-point or uniform crossover.
        Niching (sharing) and an option for the number of children per
        pair of parents has been added.  More recently, an option for
        the use of a micro-GA has been added.



     GAGS

       Web site: kal-el.ugr.es/gags.html

       FTP site: kal-el.ugr.es/GAGS/


        Genetic Algorithm  application generator and class library
        written mainly in C++.  As a class library, and among other
        thing, GAGS includes:

       A chromosome hierarchy with variable length chromosomes.
        Genetic operators: 2-point crossover, uniform crossover, bit-
        flip mutation, transposition (gene interchange between 2 parts
        of the chromosome), and variable-length operators: duplication,
        elimination, and random addition.

       Population level operators include steady state, roulette wheel
        and tournament selection.

       Gnuplot wrapper: turns gnuplot into a iostreams-like class.

       Easy sample file loading and configuration file parsing.

        As an application generator (written in PERL), you only need to
        supply it with an ANSI-C or C++ fitness function, and it creates
        a C++ program that uses the above library to 90% capacity,
        compiles it, and runs it, saving results and presenting fitness
        thru gnuplot.



     GAlib: Matthew's Genetic Algorithms Library

       Web Site: lancet.mit.edu/ga/

       FTP site: lancet.mit.edu/pub/ga/

       Register GAlib at: lancet.mit.edu/ga/Register.html


        GAlib contains a set of C++ genetic algorithm objects.  The
        library includes tools for using genetic algorithms to do
        optimization in any C++ program using any representation and
        genetic operators.  The documentation includes an extensive
        overview of how to implement a genetic algorithm as well as
        examples illustrating customizations to the GAlib classes.



     GALOPPS

       Web site: GARAGe.cps.msu.edu/software/software-index.html

       FTP site: garage.cps.msu.edu/pub/GA/galopps/


        GALOPPS is a flexible, generic GA, in 'C'.  It was based upon
        Goldberg's Simple Genetic Algorithm (SGA) architecture, in order
        to make it easier for users to learn to use and extend.


        GALOPPS extends the SGA capabilities several fold:

       (optional) A new Graphical User Interface, based on TCL/TK, for
        Unix users, allowing easy running of GALOPPS 3.2 (single or
        multiple subpopulations) on one or more processors.  GUI
        writes/reads "standard" GALOPPS input and master files, and
        displays graphical output (during or after run) of user-selected
        variables.

       5 selection methods: roulette wheel, stochastic remainder
        sampling, tournament selection, stochastic universal sampling,
        linear-ranking-then-SUS.

       Random or superuniform initialization of "ordinary" (non-
        permutation) binary or non-binary chromosomes; random
        initialization of permutation-based chromosomes; or user-
        supplied initialization of arbitrary types of chromosomes.

       Binary or non-binary alphabetic fields on value-based
        chromosomes, including different user-definable field sizes.

       3 crossovers for value-based representations: 1-pt, 2-pt, and
        uniform, all of which operate at field boundaries if a non-
        binary alphabet is used.

       4 crossovers for order-based reps: PMX, order-based, uniform
        order-based, and cycle.

       4 mutations: fast bitwise, multiple-field, swap and random
        sublist scramble.

       Fitness scaling: linear scaling, Boltzmann scaling, sigma
        truncation, window scaling, ranking.

       Plus a whole lot more....



     GAS

       Web site: starship.skyport.net/crew/gandalf

       FTP site: ftp.coe.uga.edu/users/jae/ai


        GAS means "Genetic Algorithms Stuff".

        GAS is freeware.

        Purpose of GAS is to explore and exploit artificial evolutions.
        Primary implementation language of GAS is Python.  The GAS
        software package is meant to be a Python framework for applying
        genetic algorithms. It contains an example application where it
        is tried to breed Python program strings.  This special problem
        falls into the category of Genetic Programming (GP), and/or
        Automatic Programming.  Nevertheless, GAS tries to be useful for
        other applications of Genetic Algorithms as well.



     GECO

       FTP site: ftp://ftp.aic.nrl.navy.mil/pub/galist/src/


        GECO (Genetic Evolution through Combination of Objects), an
        extendible object-oriented tool-box for constructing genetic
        algorithms (in Lisp).  It provides a set of extensible classes
        and methods designed for generality. Some simple examples are
        also provided to illustrate the intended use.



     GPdata

       FTP site: ftp.cs.bham.ac.uk/pub/authors/W.B.Langdon/gp-code/

       Documentation (GPdata-icga-95.ps): cs.ucl.ac.uk/genetic/papers/


        GPdata-3.0.tar.gz (C++) contains a version of Andy Singleton's
        GP-Quick version 2.1 which has been extensively altered to
        support:

       Indexed memory operation (cf. teller)

       multi tree programs

       Adfs

       parameter changes without recompilation

       populations partitioned into demes

       (A version of) pareto fitness

        This ftp site also contains a small C++ program (ntrees.cc) to
        calculate the number of different there are of a given length
        and given function and terminal set.



     gpjpp Genetic Programming in Java

       [Dead Link] Web site: http://www.turbopower.com/~kimk/gpjpp.asp

       Anyone who knows where to find gpjpp, please let me know.


        gpjpp is a Java package I wrote for doing research in genetic
        programming. It is a port of the gpc++ kernel written by Adam
        Fraser and Thomas Weinbrenner. Included in the package are four
        of Koza's standard examples: the artificial ant, the hopping
        lawnmower, symbolic regression, and the boolean multiplexer.
        Here is a partial list of its features:

       graphic output of expression trees

       efficient diversity checking

       Koza's greedy over-selection option for large populations

       extensible GPRun class that encapsulates most details of a
        genetic programming test

       more robust and efficient streaming code, with automatic
        checkpoint and restart built into the GPRun class
       an explicit complexity limit that can be set on each GP

       additional configuration variables to allow more testing without
        recompilation

       support for automatically defined functions (ADFs)

       tournament and fitness proportionate selection

       demetic grouping

       optional steady state population

       subtree crossover

       swap and shrink mutation



     GP Kernel

       Web site (???): www.emk.e-technik.th-
        darmstadt.de/~thomasw/gp.html

        The GP kernel is a C++ class library that can be used to apply
        genetic programming techniques to all kinds of problems. The
        library defines a class hierarchy. An integral component is the
        ability to produce automatically defined functions as found in
        Koza's "Genetic Programming II". Technical documentation
        (postscript format) is included. There is also a short
        introduction into genetic programming.


        Functionality includes; Automatically defined functions (ADFs),
        tournament and fitness proportionate selection, demetic
        grouping, optional steady state genetic programming kernel,
        subtree crossover, swap and shrink mutation, a way of changing
        every parameter of the system without recompilation, capacity
        for multiple populations, loading and saving of populations and
        genetic programs, standard random number generator, internal
        parameter checks.



     lil-gp

       Web site: GARAGe.cps.msu.edu/software/software-index.html#lilgp

       FTP site: garage.cps.msu.edu/pub/GA/lilgp/


     patched lil-gp *

       Web site: www.cs.umd.edu/users/seanl/gp/


        lil-gp is a generic 'C' genetic programming tool. It was written
        with a number of goals in mind: speed, ease of use and support
        for a number of options including:

       Generic 'C' program that runs on UNIX workstations

       Support for multiple population experiments, using arbitrary and
        user settable topologies for exchange, for a single processor
        (i.e., you can do multiple population gp experiments on your
        PC).

       lil-gp manipulates trees of function pointers which are
        allocated in single, large memory blocks for speed and to avoid
        swapping.

        * The patched lil-gp kernel is strongly-typed, with
        modifications on multithreading, coevolution, and other tweaks
        and features.



     PGAPack
        Parallel Genetic Algorithm Library

       Web site: www.mcs.anl.gov/~levine/PGAPACK/

       FTP site: ftp.mcs.anl.gov/pub/pgapack/


        PGAPack is a general-purpose, data-structure-neutral, parallel
        genetic algorithm library. It is intended to provide most
        capabilities desired in a genetic algorithm library, in an
        integrated, seamless, and portable manner. Key features are in
        PGAPack V1.0 include:

       Callable from Fortran or C.

       Runs on uniprocessors, parallel computers, and workstation
        networks.

       Binary-, integer-, real-, and character-valued native data
        types.

       Full extensibility to support custom operators and new data
        types.

       Easy-to-use interface for novice and application users.

       Multiple levels of access for expert users.

       Parameterized population replacement.

       Multiple crossover, mutation, and selection operators.

       Easy integration of hill-climbing heuristics.

       Extensive debugging facilities.

       Large set of example problems.

       Detailed users guide.



     PIPE

       Web site: www.idsia.ch/~rafal/research.html

       FTP site: ftp.idsia.ch/pub/rafal

        Probabilistic Incremental Program Evolution (PIPE) is a novel
        technique for automatic program synthesis. The software is
        written in C. It
       is easy to install (comes with an automatic installation tool).

       is easy to use: setting up PIPE_V1.0 for different problems
        requires a minimal amount of programming. User-written,
        application- independent program parts can easily be reused.

       is efficient: PIPE_V1.0 has been tuned to speed up performance.

       is portable: comes with source code (optimized for SunOS 5.5.1).

       is extensively documented(!) and contains three example
        applications.

       supports statistical evaluations: it facilitates running
        multiple experiments and collecting results in output files.

       includes testing tool for testing generalization of evolved
        programs.

       supports floating point and integer arithmetic.

       has extensive output features.

       For lil-gp users: Problems  set up for lil-gp 1.0 can be easily
        ported to PIPE_v1.0.  The  testing tool can also be used to
        process programs evolved by lil-gp 1.0.



     Sugal

       Web site: www.trajan-software.demon.co.uk/sugal.htm

        Sugal [soo-gall] is the SUnderland Genetic ALgorithm system. The
        aim of Sugal is to support research and implementation in
        Genetic Algorithms on a common software platform. As such, Sugal
        supports a large number of variants of Genetic Algorithms, and
        has extensive features to support customization and extension.



  4.2.

  EC software kits/applications


  These are various applications, software kits, etc. meant for research
  in the field of evolutionary computing. Their ease of use will vary,
  as they were designed to meet some particular research interest more
  than as an easy to use commercial package.



     ADATE

       Web site: www-ia.hiof.no/~rolando/adate_intro.html


        ADATE (Automatic Design of Algorithms Through Evolution) is a
        system for automatic programming i.e., inductive inference of
        algorithms, which may be the best way to develop artificial and
        general intelligence.


        The ADATE system can automatically generate non-trivial and
        novel algorithms. Algorithms are generated through large scale
        combinatorial search that employs sophisticated program
        transformations and heuristics. The ADATE system is particularly
        good at synthesizing symbolic, functional programs and has
        several unique qualities.



     esep & xesep

       Web site(esep): www.iit.edu/~linjinl/esep.html

       Web site(xesep): www.iit.edu/~linjinl/xesep.html


        This is a new scheduler, called Evolution Scheduler, based on
        Genetic Algorithms and Evolutionary Programming. It lives with
        original Linux priority scheduler.This means you don't have to
        reboot to change the scheduling policy. You may simply use the
        manager program esep to switch between them at any time, and
        esep itself is an all-in-one for scheduling status, commands,
        and administration. We didn't intend to remove the original
        priority scheduler; instead, at least, esep provides you with
        another choice to use a more intelligent scheduler, which
        carries out natural competition in an easy and effective way.


        Xesep is a graphical user interface to the esep (Evolution
        Scheduling and Evolving Processes). It's intended to show users
        how to start, play, and feel the Evolution Scheduling and
        Evolving Processes, including sub-programs to display system
        status, evolving process status, queue status, and evolution
        scheduling status periodically in as small as one mini-second.



     Corewars

       Web site: corewars.sourceforge.net/

       SourceForge site: sourceforge.net/project/?group_id=3054

        Corewars is a game which simulates a virtual machine with a
        number of programs. Each program tries to crash the others. The
        program that lasts the longest time wins. A number of sample
        programs are provided and new programs can be written by the
        player. Screenshots are available at the Corewars homepage.



     Corewar VM

       Web site: www.jedi.claranet.fr/


        This is a virtual machine written in Java (so it is a virtual
        machine for another virtual machine !) for a Corewar game.



     FSM-Evolver

       Web site (???): pages.prodigy.net/czarneckid


        A Java (jdk-v1.0.2+) code library that is used to evolve finite
        state machines. The problem included in the package is the
        Artificial Ant problem. You should be able to compile the .java
        files and then run: java ArtificialAnt.



     GPsys

       Web site: www.cs.ucl.ac.uk/staff/A.Qureshi/gpsys.html


        GPsys (pronounced gipsys) is a Java (requires Java 1.1 or later)
        based Genetic Programming system developed by Adil Qureshi.  The
        software includes documentation, source and executables.


        Feature Summary:

       Steady State engine

       ADF support

       Strongly Typed

        1. supports generic functions and terminals

        2. has many built-in primitives

        3. includes indexed memory

       Save/Load feature

        1. can save/load current generation to/from a file

        2. data stored in GZIP compression format to minimise disk
           requirements

        3. uses serialisable objects for efficiency

       Fully Documented

       Example Problems

        1. Lawnmower (including GUI viewer)

        2. Symbolic Regression

       Totally Parameterised

       Fully Object Oriented and Extensible

       High Performance

       Memory Efficient



     JGProg

       Web site: www.linuxstart.com/~groovyjava/JGProg/

        Genetic Programming (JGProg) is an open-source Java
        implementation of a strongly-typed Genetic Programming
        experimentation platform. Two example "worlds" are provided, in
        which a population evolves and solves the problem.



  5.

  Alife & Complex Systems



  Alife takes yet another approach to exploring the mysteries of
  intelligence.  It has many aspects similar to EC and Connectionism,
  but takes these ideas and gives them a meta-level twist. Alife
  emphasizes the development of intelligence through emergent behavior
  of complex adaptive systems.  Alife stresses the social or group based
  aspects of intelligence. It seeks to understand life and survival. By
  studying the behaviors of groups of 'beings' Alife seeks to discover
  the way intelligence or higher order activity emerges from seemingly
  simple individuals. Cellular Automata and Conway's Game of Life are
  probably the most commonly known applications of this field. Complex
  Systems (abbreviated CS) are very similar to alife in the way the are
  approached, just more general in definition (ie.  alife is a type of
  complex system).  Usually complex system software takes the form of a
  simulator.



  5.1.

  Alife & CS class/code libraries



  These are libraries of code or classes for use in programming within
  the artificial life field.  They are not meant as stand alone
  applications, but rather as tools for building your own applications.



     CASE

       Web site: www.iu.hioslo.no/~cell/

       FTP site: ftp.iu.hioslo.no/pub/

        CASE (Cellular Automaton Simulation Environment) is a C++
        toolkit for visualizing discrete models in two dimensions: so-
        called cellular automata. The aim of this project is to create
        an integrated framework for creating generalized cellular
        automata using the best, standardized technology of the day.



     John von Neumann Universal Constructor

       Web site: alife.santafe.edu/alife/software/jvn.html

       FTP site: alife.santafe.edu/pub/SOFTWARE/jvn/


        The universal constructor of John von Neumann is an extension of
        the logical concept of universal computing machine.  In the
        cellular environment proposed by von Neumann both computing and
        constructive universality can be achieved.  Von Neumann proved
        that in his cellular lattice both a Turing machine and a machine
        capable of producing any other cell assembly, when fed with a
        suitable program, can be embedded. He called the latter machine
        a ''universal constructor'' and showed that, when provided with
        a program containing its own description, this is capable of
        self-reproducing.



     Swarm

       Web site:  www.santafe.edu/projects/swarm

       FTP site:  ftp.santafe.edu/pub/swarm


        The swarm Alife simulation kit. Swarm is a simulation
        environment which facilitates development and experimentation
        with simulations involving a large number of agents behaving and
        interacting within a dynamic environment.  It consists of a
        collection of classes and libraries written in Objective-C and
        allows great flexibility in creating simulations and analyzing
        their results.  It comes with three demos and good
        documentation.


        Swarm 1.0 is out. It requires libtclobjc and BLT 2.1 (both
        available at the swarm site).



  5.2.

  Alife & CS software kits, applications, etc.


  These are various applications, software kits, etc. meant for research
  in the field of artificial life. Their ease of use will vary, as they
  were designed to meet some particular research interest more than as
  an easy to use commercial package.



     Avida

       Web site: http://www.krl.caltech.edu/avida/home/software.html

       Web site: www.krl.caltech.edu/avida/pubs/nature99/

        The computer program avida is an auto-adaptive genetic system
        designed primarily for use as a platform in Artificial Life
        research. The avida system is based on concepts similar to those
        employed by the tierra program, that is to say it is a
        population of self-reproducing strings with a Turing-complete
        genetic basis subjected to Poisson-random mutations. The
        population adapts to the combination of an intrinsic fitness
        landscape (self-reproduction) and an externally imposed
        (extrinsic) fitness function provided by the researcher. By
        studying this system, one can examine evolutionary adaptation,
        general traits of living systems (such as self-organization),
        and other issues pertaining to theoretical or evolutionary
        biology and dynamic systems.



     BugsX

       FTP site: ftp.de.uu.net/pub/research/ci/Alife/packages/bugsx/


        Display and evolve biomorphs. It is a program which draws the
        biomorphs based on parametric plots of Fourier sine and cosine
        series and let's you play with them using the genetic algorithm.



     The Cellular Automata Simulation System

       Web site: www.cs.runet.edu/~dana/ca/cellular.html


        The system consists of a compiler for the Cellang cellular
        automata programming language, along with the corresponding
        documentation, viewer, and various tools. Cellang has been
        undergoing refinement for the last several years (1991-1995),
        with corresponding upgrades to the compiler.  Postscript
        versions of the tutorial and language reference manual are
        available for those wanting more detailed information. The most
        important distinguishing features of Cellang, include support
        for:

       any number of dimensions;

       compile time specification of each dimensions size; cell
        neighborhoods of any size (though bounded at compile time) and
        shape;

       positional and time dependent neighborhoods;

       associating multiple values (fields), including arrays, with
        each cell;

       associating a potentially unbounded number of mobile agents [
        Agents are mobile entities based on a mechanism of the same name
        in the Creatures system, developed by Ian Stephenson
        (ian@ohm.york.ac.uk).] with each cell; and

       local interactions only, since it is impossible to construct
        automata that contain any global control or references to global
        variables.



     Cyphesis

       Web site: www.worldforge.org/website/servers/cyphesis/

        Cyphesis will be the AI Engine, or more plainly, the
        intelligence behind Worldforge (WF).  Cyphesis will aims to
        achieve 'live' virtual worlds. Animals will have young, prey on
        each other and eventually die. Plants grow, flower, bear fruit
        and even die just as they do in real life. When completed, NPCs
        in Cyphesis will do all sorts of interesting things like attempt
        to acomplish ever-changing goals that NPCs set for themselves,
        gossip to PCs and other NPCs, live, die and raise children.
        Cyphesis aims to make NPCs act just like you and me.



     dblife & dblifelib

       FTP site: ftp.cc.gatech.edu/ac121/linux/games/amusements/life/


        dblife: Sources for a fancy Game of Life program for X11 (and
        curses).  It is not meant to be incredibly fast (use xlife for
        that:-).  But it IS meant to allow the easy editing and viewing
        of Life objects and has some powerful features.  The related
        dblifelib package is a library of Life objects to use with the
        program.


        dblifelib: This is a library of interesting Life objects,
        including oscillators, spaceships, puffers, and other weird
        things.  The related dblife package contains a Life program
        which can read the objects in the Library.



     Drone

       Web site: pscs.physics.lsa.umich.edu/Software/Drone/


        Drone is a tool for automatically running batch jobs of a
        simulation program. It allows sweeps over arbitrary sets of
        parameters, as well as multiple runs for each parameter set,
        with a separate random seed for each run. The runs may be
        executed either on a single computer or over the Internet on a
        set of remote hosts. Drone is written in Expect (an extension to
        the Tcl scripting language) and runs under Unix. It was
        originally designed for use with the Swarm agent-based
        simulation framework, but Drone can be used with any simulation
        program that reads parameters from the command line or from an
        input file.


     EBISS

       Web site: www.ebiss.org/english/

        EBISS is a multi-disciplinary, open, collaborative project aimed
        at investigating social problems by means of computational
        modeling and social simulations. During the past four years we
        have been developing SARA, a multi-agent gaming simulation
        platform providing for easy construction of simulations and
        gamings.

        We believe that in order to have a break-through in the
        difficult task of understanding real-world complex social
        problems, we need to gather researchers and experts with
        different backgrounds not only in discussion forums, but in a
        tighter cooperative task of building and sharing common
        experimental platforms.



     EcoLab

       Web site: parallel.acsu.unsw.edu.au/rks/ecolab.html


        EcoLab is a system that implements an abstract ecology model. It
        is written as a set of Tcl/Tk commands so that the model
        parameters can easily be changed on the fly by means of editing
        a script. The model itself is written in C++.



     Game Of Life (GOL)

       Web site: www.arrakeen.demon.co.uk/downloads.html

       FTP site: metalab.unc.edu/pub/Linux/science/ai/life


        GOL is a simulator for conway's game of life (a simple cellular
        automata), and other simple rule sets. The emphasis here is on
        speed and scale, in other words you can setup large and fast
        simulations.



     gLife

       Web site: glife.sourceforge.net

       SourceForge site: sourceforge.net/project/?group_id=748

        This program is similiar to "Conway's Game of Life" but yet it
        is very different. It takes "Conway's Game of Life" and applies
        it to a society (human society). This means there is a very
        different (and much larger) ruleset than in the original game.
        Things need to be taken into account such as the terrain, age,
        sex, culture, movement, etc



     Grany-3

       Web site: altern.org/gcottenc/html/grany.html

        Grany-3 is a full-featured cellular automaton simulator, made in
        C++ with Gtk--, flex++/bison++, doxygen and gettext, useful to
        granular media physicists.



     Langton's Ant

       Web site: theory.org/software/ant/

        Langton's Ant is an example of a finite-state cellular automata.
        The ant (or ants) start out on a grid. Each cell is either black
        or white.  If the ant is on a black square, it turns right 90
        and moves forward one unit. If the ant is on a white square, it
        turns left 90 and moves forward one unit.  And when the ant
        leaves a square, it inverts the color. The neat thing about
        Langton's Ant is that no matter what pattern field you start it
        out on, it eventually builds a "road," which is a series of 117
        steps that repeat indefinitely, each time leaving the ant
        displaced one pixel vertically and horizontally.



     LEE

       Web site: dollar.biz.uiowa.edu/~fil/LEE/

       FTP site: dollar.biz.uiowa.edu/pub/fil/LEE/


        LEE (Latent Energy Environments) is both an Alife model and a
        software tool to be used for simulations within the framework of
        that model. We hope that LEE will help understand a broad range
        of issues in theoretical, behavioral, and evolutionary biology.
        The LEE tool described here consists of approximately 7,000
        lines of C code and runs in both Unix and Macintosh platforms.



     Net-Life & ZooLife

       FTP site: ftp.coe.uga.edu/users/jae/alife/

        *(netlife-2.0.tar.gz contains both Net-Life and ZooLife)


        Net-Life is a simulation of artificial-life, with neural
        "brains" generated via slightly random techniques. Net-Life uses
        artificial neural nets and evolutionary algorithms to breed
        artificial organisms that are similar to single cell organisms.
        Net-life uses asexual reproduction of its fittest individuals
        with a chance of mutation after each round to eventually evolve
        successful life-forms.


        ZooLife is a simulation of artificial-life. ZooLife uses
        probabilistic methods and evolutionary algorithms to breed
        artificial organisms that are similar to plant/animal zoo
        organisms.  ZooLife uses asexual reproduction with a chance of
        mutation.



     POSES++

       Web site: www.tu-chemnitz.de/ftp-
        home/pub/Local/simulation/poses++/www/index.html

        The POSES++ software tool supports the development and
        simulation of models.  Regarding the simulation technique models
        are suitable reproductions of real or planned systems for their
        simulative investigation.

        In all industrial sectors or branches POSES++ can model and
        simulate any arbitrary system which is based on a discrete and
        discontinuous behaviour. Also continuous systems can mostly be
        handled like discrete systems e.g., by quantity discretion and
        batch processing.



     Primordial Soup

       Web site: alife.santafe.edu/alife/software/psoup.html


        Primordial Soup is an artificial life program. Organisms in the
        form of computer software loops live in a shared memory space
        (the "soup") and self-reproduce. The organisms mutate and
        evolve, behaving in accordance with the principles of Darwinian
        evolution.


        The program may be started with one or more organisms seeding
        the soup. Alternatively, the system may be started "sterile",
        with no organisms present. Spontaneous generation of self-
        reproducing organisms has been observed after runs as short as
        15 minutes.



     Tierra

       Web site: www.hip.atr.co.jp/~ray/tierra/tierra.html

       FTP site: alife.santafe.edu/pub/SOFTWARE/Tierra/

       Alternate

       FTP site: ftp.cc.gatech.edu/ac121/linux/science/biology/


        Tierra's written in the C programming language. This source code
        creates a virtual computer and its operating system, whose
        architecture has been designed in such a way that the executable
        machine codes are evolvable. This means that the machine code
        can be mutated (by flipping bits at random) or recombined (by
        swapping segments of code between algorithms), and the resulting
        code remains functional enough of the time for natural (or
        presumably artificial) selection to be able to improve the code
        over time.



     TIN

       FTP site: ftp.coe.uga.edu/users/jae/alife/


        This program simulates primitive life-forms, equipped with some
        basic instincts and abilities, in a 2D environment consisting of
        cells.  By mutation new generations can prove their success, and
        thus passing on "good family values".


        The brain of a TIN can be seen as a collection of processes,
        each representing drives or impulses to behave a certain way,
        depending on the state/perception of the environment ( e.g.
        presence of food, walls, neighbors, scent traces) These behavior
        process currently are : eating, moving, mating, relaxing,
        tracing others, gathering food and killing. The process with the
        highest impulse value takes control, or in other words: the tin
        will act according to its most urgent need.



     XLIFE

       FTP site: ftp.cc.gatech.edu/ac121/linux/games/amusements/life/


        This program will evolve patterns for John Horton Conway's game
        of Life.  It will also handle general cellular automata with the
        orthogonal neighborhood and up to 8 states (it's possible to
        recompile for more states, but very expensive in memory).
        Transition rules and sample patterns are provided for the
        8-state automaton of E. F. Codd, the Wireworld automaton, and a
        whole class of `Prisoner's Dilemma' games.



     Xtoys

       Web site: penguin.phy.bnl.gov/www/xtoys.html

        xtoys contains a set of cellular automata simulators for X
        windows.  Programs included are:

       xising --- a two dimensional Ising model simulator,

       xpotts --- the two dimensional Potts model,

       xautomalab ---  a totalistic cellular automaton simulator,

       xsand --- for the Bak, Tang, Wiesenfeld sandpile model,

       xwaves --- demonstrates three different wave equations,

       schrodinger --- play with the Scrodinger equation in an
        adjustable potential.



  6.

  Autonomous Agents


  Also known as intelligent software agents or just agents, this area of
  AI research deals with simple applications of small programs that aid
  the user in his/her work. They can be mobile (able to stop their
  execution on one machine and resume it on another) or static (live in
  one machine). They are usually specific to the task (and therefore
  fairly simple) and meant to help the user much as an assistant would.
  The most popular (ie. widely known) use of this type of application to
  date are the web robots that many of the indexing engines (eg.
  webcrawler) use.
     AgentK

       FTP site: ftp.csd.abdn.ac.uk/pub/wdavies/agentk


        This package synthesizes two well-known agent paradigms: Agent-
        Oriented Programming, Shoham (1990), and the Knowledge Query &
        Manipulation Language, Finin (1993). The initial implementation
        of AOP, Agent-0, is a simple language for specifying agent
        behaviour. KQML provides a standard language for inter-agent
        communication. Our integration (which we have called Agent-K)
        demonstrates that Agent-0 and KQML are highly compatible. Agent-
        K provides the possibility of inter-operable (or open) software
        agents, that can communicate via KQML and which are programmed
        using the AOP approach.



     Agent

       FTP site: www.cpan.org/modules/by-
        category/23_Miscellaneous_Modules/Agent/


        The Agent is a prototype for an Information Agent system. It is
        both platform and language independent, as it stores contained
        information in simple packed strings. It can be packed and
        shipped across any network with any format, as it freezes itself
        in its current state.



     D'Agent (was AGENT TCL)

       Web site: agent.cs.dartmouth.edu/software/agent2.0/

       FTP site: ftp.cs.dartmouth.edu/pub/agents/


        A transportable agent is a program that can migrate from machine
        to machine in a heterogeneous network.  The program chooses when
        and where to migrate.  It can suspend its execution at an
        arbitrary point, transport to another machine and resume
        execution on the new machine.  For example, an agent carrying a
        mail message migrates first to a router and then to the
        recipient's mailbox.  The agent can perform arbitrarily complex
        processing at each machine in order to ensure that the message
        reaches the intended recipient.



     Aglets Workbench

       Web site: www.trl.ibm.co.jp/aglets/


        An aglet is a Java object that can move from one host on the
        Internet to another.  That is, an aglet that executes on one
        host can suddenly halt execution, dispatch to a remote host, and
        resume execution there. When the aglet moves, it takes along its
        program code as well as its state (data). A built-in security
        mechanism makes it safe for a computer to host untrusted aglets.
        The Java Aglet API (J-AAPI) is a proposed public standard for
        interfacing aglets and their environment. J-AAPI contains
        methods for initializing an aglet, message handling, and
        dispatching, retracting, deactivating/activating, cloning, and
        disposing of the aglet. J-AAPI is simple, flexible, and stable.
        Application developers can write platform-independent aglets and
        expect them to run on any host that supports J-AAPI.



     A.L.I.C.E.

       Web site: www.alicebot.org

        The ALICE software implements AIML (Artificial Intelligence
        Markup Language), a non-standard evolving markup language for
        creating chat robots. The primary design feature of AIML is
        minimalism. Compared with other chat robot languages, AIML is
        perhaps the simplest. The pattern matching language is very
        simple, for example permitting only one wild-card ('*') match
        character per pattern. AIML is an XML language, implying that it
        obeys certain grammatical meta-rules. The choice of XML syntax
        permits integration with other tools such as XML editors.
        Another motivation for XML is its familiar look and feel,
        especially to people with HTML experience.



     Ara

       Web site: www.uni-kl.de/AG-Nehmer/Projekte/Ara/index_e.html


        Ara is a platform for the portable and secure execution of
        mobile agents in heterogeneous networks. Mobile agents in this
        sense are programs with the ability to change their host machine
        during execution while preserving their internal state. This
        enables them to handle interactions locally which otherwise had
        to be performed remotely. Ara's specific aim in comparison to
        similar platforms is to provide full mobile agent functionality
        while retaining as much as possible of established programming
        models and languages.



     Bee-gent

       Web site: www2.toshiba.co.jp/beegent/index.htm

        Bee-gent is a new type of development framework in that it is a
        100% pure agent system. As opposed to other systems which make
        only some use of agents, Bee-gent completely "Agentifies" the
        communication that takes place between software applications.
        The applications become agents, and all messages are carried by
        agents. Thus, Bee-gent allows developers to build flexible open
        distributed systems that make optimal use of existing
        applications.


     Bots

       Web site: utenti.tripod.it/Claudio1977/bots.html

        Another AI-robot battle simulation.  Utilizing probablistic
        logic as a machine learning technique.  Written in C++ (with C++
        bots).

     Cadaver

       Web site: www.erikyyy.de/cadaver/

        Cadaver is a simulated world of cyborgs and nature in realtime.
        The battlefield consists of forests, grain, water, grass,
        carcass (of course) and lots of other things. The game server
        manages the game and the rules.  You start a server and connect
        some clients.  The clients communicate with the server using a
        very primitive protocol.  They can order cyborgs to harvest
        grain, attack enemies or cut forest.  The game is not intended
        to be played by humans!  There is too much to control.  Only for
        die-hards: Just telnet to the server and you can enter commands
        by hand.  Instead the idea is that you write artificial
        intelligence clients to beat the other artificial intelligences.
        You can choose a language (and operating system) of your choice
        to do that task.  It is enough to write a program that
        communicates on standard input and standard output channels.
        Then you can use programs like "socket" to connect your clients
        to the server.  It is NOT needed to write TCP/IP code, although
        i did so :) The battle shall not be boring, and so there is the
        so called spyboss client that displays the action graphically on
        screen.



     Dunce

       Web site: www.boswa.com/boswabits/

        Dunce is a simple chatterbot (conversational AI) and a language
        for programming such chatterbots. It uses a basic regex pattern
        matching and a semi-neural rule/response firing mechanism (with
        excitement/decay cycles).

        Dunce is listed about halfway down the page.



     FishMarket

       Web site: www.iiia.csic.es/Projects/fishmarket/

        FM - The FishMarket project conducted at the Artificial
        Intelligence Research Institute (IIIA-CSIC) attempts to
        contribute in that direction by developing FM, an agent-mediated
        electronic auction house which has been evolved into a test-bed
        for electronic auction markets. The framework, conceived and
        implemented as an extension of FM96.5 (a Java-based version of
        the Fishmarket auction house), allows to define trading
        scenarios based on fish market auctions (Dutch auctions). FM
        provides the framework wherein agent designers can perform
        controlled experimentation in such a way that a multitude of
        experimental market scenarios--that we regard as tournament
        scenarios due to the competitive nature of the domain-- of
        varying degrees of realism and complexity can be specified,
        activated, and recorded; and trading (buyer and seller)
        heterogeneous (human and software) agents compared, tuned and
        evaluated.



     Hive

       Web site: www.hivecell.net/

        Hive is a Java software platform for creating distributed
        applications.  Using Hive, programmers can easily create systems
        that connect and use data from all over the Internet. At its
        heart, Hive is an environment for distributed agents to live,
        communicating and moving to fulfill applications. We are trying
        to make the Internet alive.



     Jade

       Web site: sharon.cselt.it/projects/jade/

        JADE (Java Agent DEvelopment Framework) is a software framework
        fully implemented in Java language. It simplifies the
        implementation of multi-agent systems through a middle-ware that
        claims to comply with the FIPA specifications and through a set
        of tools that supports the debugging and deployment phase. The
        agent platform can be distributed across machines (which not
        even need to share the same OS) and the configuration can be
        controlled via a remote GUI. The configuration can be even
        changed at run-time by moving agents from one machine to another
        one, as and when required.



     JAFMAS

       Web site: www.ececs.uc.edu/~abaker/JAFMAS


        JAFMAS provides a framework to guide the coherent development of
        multiagent systems along with a set of classes for agent
        deployment in Java. The framework is intended to help beginning
        and expert developers structure their ideas into concrete agent
        applications. It directs development from a speech-act
        perspective and supports multicast and directed communication,
        KQML or other speech-act performatives and analysis of
        multiagent system coherency and consistency.


        Only four of the provided Java classes must be extended for any
        application. Provided examples of the N-Queens and Supply Chain
        Integration use only 567 and 1276 lines of additional code
        respectively for implementation.



     JAM Agent

       Web site: members.home.net/marcush/IRS/

        JAM supports both top-down, goal-based reasoning and bottom-up
        data-driven reasoning. JAM selects goals and plans based on
        maximal priority if metalevel reasoning is not used, or user-
        developed metalevel reasoning plans if they exist. JAM's
        conceptualization of goals and goal achievement is more
        classically defined (UMPRS is more behavioral performance-based
        than truly goal-based) and makes the distinction between plans
        to achieve goals and plans that simply encode behaviors. Goal-
        types implemented include achievement (attain a specified world
        state), maintenance (re-attain a specified world state), and
        performance. Execution of multiple simultaneous goals are
        supported, with suspension and resumption capabilities for each
        goal (i.e., intention) thread. JAM plans have explicit
        precondition and runtime attributes that restrict their
        applicability, a postcondition attribute, and a plan attributes
        section for specifying plan/domain-specific plan features.
        Available plan constructs include: sequencing, iteration,
        subgoaling, atomic (i.e., non-interruptable) plan segments, n-
        branch deterministic and non-deterministic conditional
        execution, parallel execution of multiple plan segments, goal-
        based or world state-based synchronization, an explicit failure-
        handling section, and Java primitive function definition through
        building it into JAM as well as the invocation of predefined
        (i.e., legacy) class members via Java's reflection capabilities
        without having to build it into JAM.



     JATLite

       Web site: java.stanford.edu/java_agent/html/


        JATLite is providing a set of java packages which makes easy to
        build multi-agent systems using Java. JATLite provides only
        light-weight, small set of packages so that the developers can
        handle all the packages with little efforts. For flexibility
        JATLite provides four different layers from abstract to Router
        implementation. A user can access any layer we are providing.
        Each layer has a different set of assumptions. The user can
        choose an appropriate layer according to the assumptions on the
        layer and user's application. The introduction page contains
        JATLite features and the set of assumptions for each layer.



     JATLiteBeans

       Web site: waitaki.otago.ac.nz/JATLiteBean/


       Improved, easier-to-use interface to JATLite features including
        KQML message parsing, receiving, and sending.


       Extensible architecture for message handling and agent "thread
        of control" management


       Useful functions for parsing of simple KQML message content


       JATLiteBean supports automatic advertising of agent capabilities
        to facilitator agents


       Automatic, optional, handling of the "forward" performative


       Generic configuration file parser



       KQML syntax checker



     Java(tm) Agent Template

       Web site: cdr.stanford.edu/ABE/JavaAgent.html


        The JAT provides a fully functional template, written entirely
        in the Java language, for constructing software agents which
        communicate peer-to-peer with a community of other agents
        distributed over the Internet. Although portions of the code
        which define each agent are portable, JAT agents are not
        migratory but rather have a static existence on a single host.
        This behavior is in contrast to many other "agent" technologies.
        (However, using the Java RMI, JAT agents could dynamically
        migrate to a foreign host via an agent resident on that host).
        Currently, all agent messages use KQML as a top-level protocol
        or message wrapper. The JAT includes functionality for
        dynamically exchanging "Resources", which can include Java
        classes (e.g. new languages and interpreters, remote services,
        etc.), data files and information inlined into the KQML
        messages.



     Java-To-Go

       Web site: ptolemy.eecs.berkeley.edu/dgm/javatools/java-to-go/


        Java-To-Go is an experimental infrastructure that assists in the
        development and experimentation of mobile agents and agent-based
        applications for itinerative computing (itinerative computing:
        the set of applications that requires site-to-site computations.
        The main emphasis here is on a easy-to-setup environment that
        promotes quick experimentation on mobile agents.



     Kafka

       Web site: www.fujitsu.co.jp/hypertext/free/kafka/


        Kafka is yet another agent library designed for constructing
        multi-agent based distributed applications. Kafka is a flexible,
        extendable, and easy-to-use java class library for programmers
        who are familiar with distributed programming. It is based on
        Java's RMI and has the following added features:

       Runtime Reflection: Agents can modify their behaviour (program
        codes) at runtime. The behaviour of the agent is represented by
        an abstract class Action. It is useful for remote maintenance or
        installation services.

       Remote Evaluation: Agents can receive and evaluate program codes
        (classes) with or without the serialized object. Remote
        evaluation is a fundamental function of a mobile agent and is
        thought to be a push model of service delivery.

       Distributed Name Service: Agents have any number of logical
        names that don't contain the host name. These names can be
        managed by the distributed directories.

       Customizable security policy: a very flexible, customizable,
        3-layered security model is implemented in Kafka.

       100% Java and RMI compatible: Kafka is written completely in
        Java. Agent is a Java RMI server object itself. So, agents can
        directly communicate with other RMI objects.



     Khepera Simulator

       Web site: diwww.epfl.ch/lami/team/michel/khep-sim/


        Khepera Simulator is a public domain software package written by
        Olivier MICHEL during the preparation of his Ph.D. thesis, at
        the Laboratoire I3S, URA 1376 of CNRS and University of Nice-
        Sophia Antipolis, France. It allows to write your own controller
        for the mobile robot Khepera using C or C++ languages, to test
        them in a simulated environment and features a nice colorful X11
        graphical interface. Moreover, if you own a Khepera robot, it
        can drive the real robot using the same control algorithm. It is
        mainly oriented toward to researchers studying autonomous
        agents.


     lyntin

       Web site: lyntin.sourceforge.net/

        Lyntin is an extensible Mud client and framework for the
        creation of autonomous agents, or bots, as well as mudding in
        general. Lyntin is centered around Python, a dynamic, object-
        oriented, and fun programming language and based on TinTin++ a
        lovely mud client.



     Mole

       Web site: mole.informatik.uni-stuttgart.de/


        Mole is an agent system supporting mobile agents programmed in
        Java.  Mole's agents consist of a cluster of objects, which have
        no references to the outside, and as a whole work on tasks given
        by the user or another agent. They have the ability to roam a
        network of "locations" autonomously. These "locations" are an
        abstraction of real, existing nodes in the underlying network.
        They can use location-specific resources by communicating with
        dedicated agents representing these services. Agents are able to
        use services provided by other agents and to provide services as
        well.



     Penguin!

       FTP site: www.perl.org/CPAN/modules/by-
        category/23_Miscellaneous_Modules/Penguin/FSG/
        Penguin is a Perl 5 module. It provides you with a set of
        functions which allow you to:

       send encrypted, digitally signed Perl code to a remote machine
        to be executed.

       receive code and, depending on who signed it, execute it in an
        arbitrarily secure, limited compartment.

        The combination of these functions enable direct Perl coding of
        algorithms to handle safe internet commerce, mobile information-
        gathering agents, "live content" web browser helper apps,
        distributed load-balanced computation, remote software update,
        distance machine administration, content-based information
        propagation, Internet-wide shared-data applications, network
        application builders, and so on.



     RealTimeBattle

       Web site: www.lysator.liu.se/realtimebattle/

        RealTimeBattle is a programming game, in which robots controlled
        by programs are fighting each other. The goal is to destroy the
        enemies, using the radar to examine the environment and the
        cannon to shoot.


       Game progresses in real time, with the robot programs running as
        child processes to RealTimeBattle.

       The robots communicate with the main program using the standard
        input and output.

       Robots can be constructed in almost any programming language.

       Maximum number of robots can compete simultaneously.

       A simple messaging language is used for communication, which
        makes it easy to start constructing robots.

       Robots behave like real physical object.

       You can create your own arenas.

       Highly configurable.



     Remembrance Agents

       Web site: rhodes.www.media.mit.edu/people/rhodes/RA/

        Remembrance Agents are a set of applications that watch over a
        user's shoulder and suggest information relevant to the current
        situation.  While query-based memory aids help with direct
        recall, remembrance agents are an augmented associative memory.
        For example, the word-processor version of the RA continuously
        updates a list of documents relevant to what's being typed or
        read in an emacs buffer.  These suggested documents can be any
        text files that might be relevant to what you are currently
        writing or reading.  They might be old emails related to the
        mail you are currently reading, or abstracts from papers and
        newspaper articles that discuss the topic of your writing.
     SimRobot

       Web site: www.informatik.uni-bremen.de/~simrobot/

       FTP site: ftp.uni-bremen.de/pub/ZKW/INFORM/simrobot/


        SimRobot is a program for simulation of sensor based robots in a
        3D environment. It is written in C++, runs under UNIX and X11
        and needs the graphics toolkit XView.

       Simulation of robot kinematics

       Hierarchically built scene definition via a simple definition
        language

       Various sensors built in: camera, facette eye, distance
        measurement, light sensor, etc.

       Objects defined as polyeders

       Emitter abstractly defined; can be interpreted e.g. as light or
        sound

       Camera images computed according to the raytracing or Z-buffer
        algorithms known from computer graphics

       Specific sensor/motor software interface for communicating with
        the simulation

       Texture mapping onto the object surfaces: bitmaps in various
        formats

       Comprehensive visualization of the scene: wire frame w/o hidden
        lines, sensor and actor values

       Interactive as well as batch driven control of the agents and
        operation in the environment

       Collision detection

       Extendability with user defined object types

       Possible socket communication to e.g. the Khoros image
        processing software



     Sulawesi

       Web site: wearables.essex.ac.uk/sulawesi/

        A framework called Sulawesi has been designed and implemented to
        tackle what has been considered to be important challenges in a
        wearable user interface. The ability to accept input from any
        number of modalities, and perform if necessary a translation to
        any number of modal outputs. It does this primarily through a
        set of proactive agents to act on the input.



     TclRobots


       FTP site: ftp.neosoft.com/pub/tcl/sorted/games/tclrobots-2.0/

       Redhat Patch: ftp.coe.uga.edu/users/jae/ai/tclrobots-
        redhat.patch

       RPMs (search at): http://rufus.w3.org/


        TclRobots is a programming game, similar to 'Core War'.  To play
        TclRobots, you must write a Tcl program that controls a robot.
        The robot's mission is to survive a battle with other robots.
        Two, three, or four robots compete during a battle, each running
        different programs (or possibly the same program in different
        robots.)  Each robot is equipped with a scanner, cannon, drive
        mechanism.  A single match continues until one robot is left
        running.  Robots may compete individually, or combine in a team
        oriented battle.  A tournament can be run with any number of
        robot programs, each robot playing every other in a round-robin
        fashion, one-on-one.  A battle simulator is available to help
        debug robot programs.


        The TclRobots program provides a physical environment, imposing
        certain game parameters to which all robots must adhere.
        TclRobots also provides a view on a battle, and a controlling
        user interface.  TclRobots requirements: a wish interpreter
        built from Tcl 7.4 and Tk 4.0.



     TKQML

       Web site: www.csee.umbc.edu/tkqml/

        TKQML is a KQML application/addition to Tcl/Tk, which allows Tcl
        based systems to communicate easily with a powerful agent
        communication language.



     The Tocoma Project

       Web site: www.tacoma.cs.uit.no/


        An agent is a process that may migrate through a computer
        network in order to satisfy requests made by clients. Agents are
        an attractive way to describe network-wide computations.


        The TACOMA project focuses on operating system support for
        agents and how agents can be used to solve problems
        traditionally addressed by operating systems. We have
        implemented a series of prototype systems to support agents.


        TACOMA Version 1.2 is based on UNIX and TCP. The system supports
        agents written in C, Tcl/Tk, Perl, Python, and Scheme (Elk). It
        is implemented in C. This TACOMA version has been in public
        domain since April 1996.


        We are currently focusing on heterogeneity, fault-tolerance,
        security and management issues. Also, several TACOMA
        applications are under construction. We implemented StormCast
        4.0, a wide-area network weather monitoring system accessible
        over the internet, using TACOMA and Java. We are now in the
        process of evaluating this application, and plan to build a new
        StormCast version to be completed by June 1997.



     UMPRS Agent

       Web site: members.home.net/marcush/IRS/

        UMPRS supports top-down, goal-based reasoning and selects goals
        and plans based on maximal priority. Execution of multiple
        simultaneous goals are supported, with suspension and resumption
        capabilities for each goal (i.e., intention) thread. UMPRS plans
        have an integrated precondition/runtime attribute that constrain
        their applicability.  Available plan constructs include:
        sequencing, iteration, subgoaling, atomic (i.e., non-
        interruptable) blocks, n-branch deterministic conditional
        execution, explicit failure-handling section, and C++ primitive
        function definition.



     Virtual Secretary Project (ViSe)
        (Tcl/Tk)

       Web site: www.cs.uit.no/DOS/Virt_Sec


        The motivation of the Virtual Secretary project is to construct
        user-model-based intelligent software agents, which could in
        most cases replace human for secretarial tasks, based on modern
        mobile computing and computer network. The project includes two
        different phases: the first phase (ViSe1) focuses on information
        filtering and process migration, its goal is to create a secure
        environment for software agents using the concept of user
        models; the second phase (ViSe2) concentrates on agents'
        intelligent and efficient cooperation in a distributed
        environment, its goal is to construct cooperative agents for
        achieving high intelligence. (Implemented in Tcl/TclX/Tix/Tk)



     VWORLD

       Web site: zhar.net/gnu-linux/projects/vworld/


        Vworld is a simulated environment for research with autonomous
        agents written in prolog. It is currently in something of an
        beta stage. It works well with SWI-prolog, but should work with
        Quitnus-prolog with only a few changes.  It is being designed to
        serve as an educational tool for class projects dealing with
        prolog and autonomous agents. It comes with three demo worlds or
        environments, along with sample agents for them. There are two
        versions now. One written for SWI-prolog and one written for
        LPA-prolog. Documentation is roughly done (with a
        student/professor framework in mind), and a graphical interface
        is planned.

     WebMate

       Web site: www.cs.cmu.edu/~softagents/webmate/


        WebMate is a personal agent for World-Wide Web browsing and
        searching. It accompanies you when you travel on the internet
        and provides you what you want.

        Features include:

       Searching enhancement, including parallel search, searching
        keywords refinement using our relevant keywords extraction
        technology, relevant feedback, etc.

       Browsing assistant, including learning your current interesting,
        recommending you new URLs according to your profile and selected
        resources, monitoring bookmarks of Netscape or IE, sending the
        current browsing page to your friends, etc.

       Offline browsing, including downloading the following pages from
        the current page for offline browsing.

       Filtering HTTP header, including recording http header and all
        the transactions between your browser and WWW servers, etc.

       Checking the HTML page to find the errors or dead links,  etc.

       Programming in Java, independent of operating system, runing in
        multi-thread.



     Zeus

       Web site: www.labs.bt.com/projects/agents/zeus/

        The construction of multi-agent systems involves long
        development times and requires solutions to some considerable
        technical difficulties. This has motivated the development of
        the ZEUS toolkit, which provides a library of software
        components and tools that facilitate the rapid design,
        development and deployment of agent system



  7.

  Programming languages


  While any programming language can be used for artificial
  intelligence/life research, these are programming languages which are
  used extensively for, if not specifically made for, artificial
  intelligence programming.



     Allegro CL


       Web site: www.franz.com

        Franz Inc's free linux version of their lisp development
        environment. You can download it or they will mail you a CD free
        (you don't even have to pay for shipping). It is generally
        considered to be one of the better lisp platforms.



     APRIL

       Web site: sourceforge.net/project/?group_id=3173

        APRIL is a symbolic programming language that is designed for
        writing mobile, distributed and agent-based systems especially
        in an Internet environment. It has advanced features such as a
        macro sub-language, asynchronous message sending and receiving,
        code mobility, pattern matching, higher-order functions and
        strong typing. The language is compiled to byte-code which is
        then interpreted by the APRIL runtime-engine.  APRIL now
        requires the InterAgent Communications Model (ICM) to be
        installed before it can be installed. [Ed. ICM can be found at
        the same web site]



     B-Prolog

       Web site: www.sci.brooklyn.cuny.edu/~zhou/bprolog.html

       Web site: www.cad.mse.kyutech.ac.jp/people/zhou/bprolog.html

        B-Prolog is a compact and complete CLP system that runs Prolog
        and CLP(FD) programs. An emulator-based system, B-Prolog has a
        performance comparable with SICStus-Prolog.


       In addition to Edinburgh-style programs, B-Prolog accepts
        canonical-form programs that can be compiled into more compact
        and faster code than standard Prolog programs.

       B-Prolog includes an interpreter and provides an interactive
        interface through which users can consult, list, compile, load,
        debug and run programs. The command editor facilitates reuse old
        commands.

       B-Prolog provides a bi-directional interface with C and Java.>
        resources in C and Java such as Graphics and sockets, and also
        makes it possible for a Prolog program to be embadded in a C and
        Java applications.

       B-Prolog supports most of the built-ins in ISO Prolog.

       B-Prolog supports the delaying (co-routining) mechanism, which
        can be used to implement concurrency, test-and-generate search
        algorithms, and most importantly constraint propagation
        algorithms.

       B-Prolog has an efficient constraint compiler for constraints>
        over finite-domains and Booleans.

       B-Prolog supports the tabling mechanism, which has proven
        effective for applications including parsing, problem solving,
        theorem proving, and deductive databases.


     DHARMI

       Web site: http://megazone.bigpanda.com/~wolf/DHARMI/

        DHARMI is a high level spatial, tinker-toy like language who's
        components are transparently administered by a background
        process called the Habitat. As the name suggests, the language
        was designed to make modelling prototypes and handle living
        data. Programs can be modified while running. This is
        accomplished by blurring the distinction between source code,
        program, and data.



     ECoLisp

       Web site (???): www.di.unipi.it/~attardi/software.html


        ECoLisp (Embeddable Common Lisp) is an implementation of Common
        Lisp designed for being embeddable into C based applications.
        ECL uses standard C calling conventions for Lisp compiled
        functions, which allows C programs to easily call Lisp functions
        and viceversa. No foreign function interface is required: data
        can be exchanged between C and Lisp with no need for conversion.
        ECL is based on a Common Runtime Support (CRS) which provides
        basic facilities for memory managment, dynamic loading and
        dumping of binary images, support for multiple threads of
        execution. The CRS is built into a library that can be linked
        with the code of the application. ECL is modular: main modules
        are the program development tools (top level, debugger, trace,
        stepper), the compiler, and CLOS. A native implementation of
        CLOS is available in ECL: one can configure ECL with or without
        CLOS. A runtime version of ECL can be built with just the
        modules which are required by the application. The ECL compiler
        compiles from Lisp to C, and then invokes the GCC compiler to
        produce binaries.



     ESTEREL

       Web site: www-sop.inria.fr/meije/esterel/

        Esterel is both a programming language, dedicated to programming
        reactive systems, and a compiler which translates Esterel
        programs into finite-state machines. It is particularly well-
        suited to programming reactive systems, including real-time
        systems and control automata.

        Only the binary is available for the language compiler. :P



     Gdel

       Web page: www.cs.bris.ac.uk/~bowers/goedel.html

        Gdel is a declarative, general-purpose programming language in
        the family of logic programming languages.  It is a strongly
        typed language, the type system being based on many-sorted logic
        with parametric polymorphism.  It has a module system.  Gdel
        supports infinite precision integers, infinite precision
        rationals, and also floating-point numbers.  It can solve
        constraints over finite domains of integers and also linear
        rational constraints. It supports processing of finite sets.  It
        also has a flexible computation rule and a pruning operator
        which generalizes the commit of the concurrent logic programming
        languages.  Considerable emphasis is placed on Gdel's meta-
        logical facilities which provide significant support for meta-
        programs that do analysis, transformation, compilation,
        verification, debugging, and so on.



     LIFE

       Web page: www.isg.sfu.ca/life

        LIFE (Logic, Inheritance, Functions, and Equations) is an
        experimental programming language proposing to integrate three
        orthogonal programming paradigms proven useful for symbolic
        computation.  From the programmer's standpoint, it may be
        perceived as a language taking after logic programming,
        functional programming, and object-oriented programming.  From a
        formal perspective, it may be seen as an instance (or rather, a
        composition of three instances) of a Constraint Logic
        Programming scheme due to Hoehfeld and Smolka refining that of
        Jaffar and Lassez.



     CLisp (Lisp)

       Web page: clisp.sourceforge.net

       FTP site: clisp.cons.org/pub/lisp/clisp/source

        CLISP is a Common Lisp implementation by Bruno Haible and
        Michael Stoll.  It mostly supports the Lisp described by Common
        LISP: The Language (2nd edition) and the ANSI Common Lisp
        standard.  CLISP includes an interpreter, a byte-compiler, a
        large subset of CLOS (Object-Oriented Lisp) , a foreign language
        interface and, for some machines, a screen editor.


        The user interface language (English, German, French) is chosen
        at run time.  Major packages that run in CLISP include CLX &
        Garnet.  CLISP needs only 2 MB of memory.



     CMU Common Lisp

       Web page: www.cons.org/cmucl/

       Old Web page: www.mv.com/users/pw/lisp/index.html

       FTP site: ftp2.cons.org/pub/languages/lisp/cmucl/release/

       Linux Installation: www.telent.net/lisp/howto.html


        CMU Common Lisp is a public domain "industrial strength" Common
        Lisp programming environment. Many of the X3j13 changes have
        been incorporated into CMU CL. Wherever possible, this has been
        done so as to transparently allow the use of either CLtL1 or
        proposed ANSI CL. Probably the new features most interesting to
        users are SETF functions, LOOP and the WITH-COMPILATION-UNIT
        macro.



     GCL (Lisp)

       FTP site: ftp.ma.utexas.edu/pub/gcl/


        GNU Common Lisp (GCL) has a compiler and interpreter for Common
        Lisp.  It used to be known as Kyoto Common Lisp.  It is very
        portable and extremely efficient on a wide class of
        applications.  It compares favorably in performance with
        commercial Lisps on several large theorem-prover and symbolic
        algebra systems. It supports the CLtL1 specification but is
        moving towards the proposed ANSI definition.  GCL compiles to C
        and then uses the native optimizing C compilers (e.g., GCC).  A
        function with a fixed number of args and one value turns into a
        C function of the same number of args, returning one value, so
        GCL is maximally efficient on such calls.  It has a conservative
        garbage collector which allows great freedom for the C compiler
        to put Lisp values in arbitrary registers.


        It has a source level Lisp debugger for interpreted code, with
        display of source code in an Emacs window.  Its profiling tools
        (based on the C profiling tools) count function calls and the
        time spent in each function.



     GNU Prolog

       Web site: pauillac.inria.fr/~diaz/gnu-prolog/

       Web site: www.gnu.org/software/prolog/prolog.html


        GNU Prolog is a free Prolog compiler with constraint solving
        over finite domains developed by Daniel Diaz.

        GNU Prolog accepts Prolog+constraint programs and produces
        native binaries (like gcc does from a C source). The obtained
        executable is then stand-alone. The size of this executable can
        be quite small since GNU Prolog can avoid to link the code of
        most unused built-in predicates. The performances of GNU Prolog
        are very encouraging (comparable to commercial systems).

        Beside the native-code compilation, GNU Prolog offers a
        classical interactive interpreter (top-level) with a debugger.

        The Prolog part conforms to the ISO standard for Prolog with
        many extensions very useful in practice (global variables, OS
        interface, sockets,...).

        GNU Prolog also includes an efficient constraint solver over
        Finite Domains (FD). This opens contraint logic pogramming to
        the user combining the power of constraint programming to the
        declarativity of logic programming.


     Mercury

       Web page: www.cs.mu.oz.au/research/mercury/


        Mercury is a new, purely declarative logic programming language.
        Like Prolog and other existing logic programming languages, it
        is a very high-level language that allows programmers to
        concentrate on the problem rather than the low-level details
        such as memory management.  Unlike Prolog, which is oriented
        towards exploratory programming, Mercury is designed for the
        construction of large, reliable, efficient software systems by
        teams of programmers. As a consequence, programming in Mercury
        has a different flavor than programming in Prolog.



     Mozart

       Web page: www.mozart-oz.org/


        The Mozart system provides state-of-the-art support in two
        areas: open distributed computing and constraint-based
        inference. Mozart implements Oz, a concurrent object-oriented
        language with dataflow synchronization.  Oz combines concurrent
        and distributed programming with logical constraint-based
        inference, making it a unique choice for developing multi-agent
        systems. Mozart is an ideal platform for both general-purpose
        distributed applications as well as for hard problems requiring
        sophisticated optimization and inferencing abilities. We have
        developed applications in scheduling and time-tabling, in
        placement and configuration, in natural language and knowledge
        representation, multi-agent systems and sophisticated
        collaborative tools.



     SWI Prolog

       Web page: www.swi.psy.uva.nl/projects/SWI-Prolog/

       FTP site: swi.psy.uva.nl/pub/SWI-Prolog/


        SWI is a free version of prolog in the Edinburgh Prolog family
        (thus making it very similar to Quintus and many other
        versions).  With: a large library of built in predicates, a
        module system, garbage collection, a two-way interface with the
        C language, plus many other features. It is meant as a
        educational language, so it's compiled code isn't the fastest.
        Although it similarity to Quintus allows for easy porting.


        XPCE is freely available in binary form for the Linux version of
        SWI-prolog.  XPCE is an object oriented X-windows GUI
        development package/environment.



     Kali Scheme


       Web site: www.neci.nj.nec.com/PLS/Kali.html


        Kali Scheme is a distributed implementation of Scheme that
        permits efficient transmission of higher-order objects such as
        closures and continuations. The integration of distributed
        communication facilities within a higher-order programming
        language engenders a number of new abstractions and paradigms
        for distributed computing. Among these are user-specified load-
        balancing and migration policies for threads, incrementally-
        linked distributed computations, agents, and parameterized
        client-server applications. Kali Scheme supports concurrency and
        communication using first-class procedures and continuations. It
        integrates procedures and continuations into a message-based
        distributed framework that allows any Scheme object (including
        code vectors) to be sent and received in a message.



     RScheme

       Web site:www.rscheme.org

       FTP site: ftp.rscheme.org/pub/rscheme/


        RScheme is an object-oriented, extended version of the Scheme
        dialect of Lisp. RScheme is freely redistributable, and offers
        reasonable performance despite being extraordinarily portable.
        RScheme can be compiled to C, and the C can then compiled with a
        normal C compiler to generate machine code. By default, however,
        RScheme compiles to bytecodes which are interpreted by a
        (runtime) virtual machine. This ensures that compilation is fast
        and keeps code size down. In general, we recommend using the
        (default) bytecode code generation system, and only compiling
        your time-critical code to machine code. This allows a nice
        adjustment of space/time tradeoffs.  (see web site for details)



     Scheme 48

       Web site: www.neci.nj.nec.com/homepages/kelsey/


        Scheme 48 is a Scheme implementation based on a virtual machine
        architecture. Scheme 48 is designed to be straightforward,
        flexible, reliable, and fast. It should be easily portable to
        32-bit byte-addressed machines that have POSIX and ANSI C
        support.  In addition to the usual Scheme built-in procedures
        and a development environment, library software includes support
        for hygienic macros (as described in the Revised^4 Scheme
        report), multitasking, records, exception handling, hash tables,
        arrays, weak pointers, and FORMAT.  Scheme 48 implements and
        exploits an experimental module system loosely derived from
        Standard ML and Scheme Xerox.  The development environment
        supports interactive changes to modules and interfaces.



     SCM (Scheme)

       Web site: www-swiss.ai.mit.edu/~jaffer/SCM.html

       FTP site: swiss-ftp.ai.mit.edu:/archive/scm/


        SCM conforms to the Revised^4 Report on the Algorithmic Language
        Scheme and the IEEE P1178 specification. Scm is written in C. It
        uses the following utilities (all available at the ftp site).

       SLIB (Standard Scheme Library) is a portable Scheme library
        which is intended to provide compatibility and utility functions
        for all standard Scheme implementations, including SCM, Chez,
        Elk, Gambit, MacScheme, MITScheme, scheme->C, Scheme48, T3.1,
        and VSCM, and is available as the file slib2c0.tar.gz. Written
        by Aubrey Jaffer.

       JACAL is a symbolic math system written in Scheme, and is
        available as the file jacal1a7.tar.gz.

       Interfaces to standard libraries including REGEX string regular
        expression matching and the CURSES screen management package.

       Available add-on packages including an interactive debugger,
        database, X-window graphics, BGI graphics, Motif, and Open-
        Windows packages.

       A compiler (HOBBIT, available separately) and dynamic linking of
        compiled modules.



     Shift

       Web site: www.path.berkeley.edu/shift/

        Shift is a programming language for describing dynamic networks
        of hybrid automata.  Such systems consist of components which
        can be created, interconnected and destroyed as the system
        evolves. Components exhibit hybrid behavior, consisting of
        continuous-time phases separated by discrete-event transitions.
        Components may evolve independently, or they may interact
        through their inputs, outputs and exported events. The
        interaction network itself may evolve.



     YAP Prolog

       Web site: www.ncc.up.pt/~vsc/Yap/

        YAP is a high-performance Prolog compiler developed at
        LIACC/Universidade do Porto. Its Prolog engine is based in the
        WAM (Warren Abstract Machine), with several optimizations for
        better performance. YAP follows the Edinburgh tradition, and is
        largely compatible with DEC-10 Prolog, Quintus Prolog, and
        especially with C-Prolog. Work on the more recent version of YAP
        strives at several goals:


       Portability: The whole system is now written in C. YAP compiles
        in popular 32 bit machines, such as Suns and Linux PCs, and in a
        64 bit machines, the Alphas running OSF Unix and Linux.
       Performance: We have optimised the emulator to obtain
        performance comparable to or better than well-known Prolog
        systems. In fact, the current version of YAP performs better
        than the original one, written in assembly language.

       Robustness: We have tested the system with a large array of
        Prolog applications.

       Extensibility: YAP was designed internally from the beginning to
        encapsulate manipulation of terms. These principles were used,
        for example, to implement a simple and powerful C-interface. The
        new version of YAP extends these principles to accomodate
        extensions to the unification algorithm, that we believe will be
        useful to implement extensions such as constraint programming.

       Completeness: YAP has for a long time provided most builtins
        expected from a Edinburgh Prolog implementation.  These include
        I/O functionality, data-base operations, and modules. Work on
        YAP aims now at being compatible with the Prolog standard.

       Openess: We would like to make new development of YAP open to
        the user community.

       Research: YAP has been a vehicle for research within and outside
        our group. Currently research is going on on parallelisation and
        tabulation, and we have started work to support constraint
        handling.



  Linux AX25-HOWTO, Amateur Radio.
  Terry Dawson, VK2KTJ, terry@perf.no.itg.telstra.com.au
  v1.5, 17 October 1997

  The Linux Operating System is perhaps the only operating system in the
  world that can boast native and standard support for the AX.25 packet
  radio protocol utilised by Amateur Radio Operators worldwide. This
  document aims to describe how to install and configure this support.
  ______________________________________________________________________

  Table of Contents



  1. Introduction.

     1.1 Changes from the previous version
     1.2 Where to obtain new versions of this document.
     1.3 Other related documentation.

  2. The Packet Radio Protocols and Linux.

     2.1 How it all fits together.

  3. The AX.25/NetRom/Rose software components.

     3.1 Finding the kernel, tools and utility packages.
        3.1.1 The kernel source:
        3.1.2 The network tools:
        3.1.3 The AX25 utilities:

  4. Installing the AX.25/NetRom/Rose software.

     4.1 Compiling the kernel.
        4.1.1 A word about Kernel modules
        4.1.2 What's new in 2.0.*+ModuleXX or 2.1.* Kernels ?
     4.2 The network configuration tools.
        4.2.1 A patch kit that adds Rose support and fixes some bugs.
        4.2.2 Building the standard net-tools release.
     4.3 The AX.25 user and utility programs.

  5. A note on callsigns, addresses and things before we start.

     5.1 What are all those T1, T2, N2 and things ?
     5.2 Run time configurable parameters

  6. Configuring an AX.25 port.

     6.1 Creating the AX.25 network device.
        6.1.1 Creating a KISS device.
           6.1.1.1 Configuring for Dual Port TNC's
        6.1.2 Creating a Baycom device.
        6.1.3 Configuring the AX.25 channel access parameters.
           6.1.3.1 Configuring the Kernel AX.25 to use the BayCom device
        6.1.4 Creating a SoundModem device.
           6.1.4.1 Configuring the sound card.
           6.1.4.2 Configuring the SoundModem driver.
           6.1.4.3 Configuring the AX.25 channel access parameters.
           6.1.4.4 Setting the audio levels and tuning the driver.
           6.1.4.5 Configuring the Kernel AX.25 to use the SoundModem
        6.1.5 Creating a PI card device.
        6.1.6 Creating a PacketTwin device.
        6.1.7 Creating a generic SCC device.
           6.1.7.1 Obtaining and building the configuration tool package.
           6.1.7.2 Configuring the driver for your card.
              6.1.7.2.1 Configuration of the hardware parameters.
           6.1.7.3 Channel Configuration
           6.1.7.4 Using the driver.
           6.1.7.5 The
        6.1.8 Creating a BPQ ethernet device.
        6.1.9 Configuring the BPQ Node to talk to the Linux AX.25 support.
     6.2 Creating the
     6.3 Configuring AX.25 routing.

  7. Configuring an AX.25 interface for TCP/IP.

  8. Configuring a NetRom port.

     8.1 Configuring
     8.2 Configuring
     8.3 Creating the NetRom Network device
     8.4 Starting the NetRom daemon
     8.5 Configuring NetRom routing.

  9. Configuring a NetRom interface for TCP/IP.

  10. Configuring a Rose port.

     10.1 Configuring
     10.2 Creating the Rose Network device.
     10.3 Configuring Rose Routing

  11. Making AX.25/NetRom/Rose calls.

  12. Configuring Linux to accept Packet connections.

     12.1 Creating the
     12.2 A simple example
     12.3 Starting

  13. Configuring the

     13.1 Creating the
     13.2 Creating the
     13.3 Configuring
     13.4 Configuring

  14. Configuring

     14.1 Creating the

  15. Configuring the

     15.1 Create the
     15.2 Create the
     15.3 Associate AX.25 callsigns with system users.
     15.4 Add the PMS to the
     15.5 Test the PMS.

  16. Configuring the

  17. Configuring the Rose Uplink and Downlink commands

     17.1 Configuring a Rose downlink
     17.2 Configuring a Rose uplink

  18. Associating AX.25 callsigns with Linux users.

  19. The

  20. AX.25, NetRom, Rose network programming.

     20.1 The address families.
     20.2 The header files.
     20.3 Callsign mangling and examples.

  21. Some sample configurations.

     21.1 Small Ethernet LAN with Linux as a router to Radio LAN
     21.2 IPIP encapsulated gateway configuration.
     21.3 AXIP encapsulated gateway configuration
        21.3.1 AXIP configuration options.
        21.3.2 A typical
        21.3.3 Running
        21.3.4 Some notes about the routes and route flags
     21.4 Linking NOS and Linux using a pipe device
  22. Where do I find more information about .... ?

     22.1 Packet Radio
     22.2 Protocol Documentation
     22.3 Hardware Documentation

  23. Discussion relating to Amateur Radio and Linux.

  24. Acknowledgements.

  25. Copyright.



  ______________________________________________________________________

  1.  Introduction.

  This document was originally an appendix to the HAM-HOWTO, but grew
  too large to be reasonably managed in that fashion. This document
  describes how to install and configure the native AX.25, NetRom and
  Rose support for Linux. A few typical configurations are described
  that could be used as models to work from.

  The Linux implementation of the amateur radio protocols is very
  flexible.  To people relatively unfamiliar with the Linux operating
  system the configuration process may look daunting and complicated. It
  will take you a little time to come to understand how the whole thing
  fits together. You will find configuration very difficult if you have
  not properly prepared yourself by learning about Linux in general. You
  cannot expect to switch from some other environment to Linux without
  learning about Linux itself.


  1.1.  Changes from the previous version


  Additions:
          Joerg Reuters Web Page
          "More Information" section
          ax25ipd configuration.

  Corrections/Updates:
          Changed pty's to a safer range to prevent possible conflicts
          Updated module and ax25-utils versions.

  ToDo:
          Fix up the SCC section, this is probably wrong.
          Expand on the programming section.



  1.2.  Where to obtain new versions of this document.

  The best place to obtain the latest version of this document is from a
  Linux Documentation Project archive. The Linux Documentation Project
  runs a Web Server and this document appears there as the AX25-HOWTO
  <http://sunsite.unc.edu/LDP/HOWTO/AX25-HOWTO.html>. This document is
  also available in various formats from the sunsite.unc.edu ftp archive
  <ftp://sunsite.unc.edu/pub/Linux/docs/howto/>.

  You can always contact me, but I pass new versions of the document
  directly to the LDP HOWTO coordinator, so if it isn't there then
  chances are I haven't finished it.
  1.3.  Other related documentation.

  There is a lot of related documentation. There are many documents that
  relate to Linux networking in more general ways and I strongly
  recommend you also read these as they will assist you in your efforts
  and provide you with stronger insight into other possible
  configurations.

  They are:

  The HAM-HOWTO <http://sunsite.unc.edu/LDP/HOWTO/HAM-HOWTO.html>,

  the NET-3-HOWTO <http://sunsite.unc.edu/LDP/HOWTO/NET-3-HOWTO.html>,

  the Ethernet-HOWTO <http://sunsite.unc.edu/LDP/HOWTO/Ethernet-
  HOWTO.html>,

  and:

  the Firewall-HOWTO <http://sunsite.unc.edu/LDP/HOWTO/Firewall-
  HOWTO.html>

  More general Linux information may be found by reference to other
  Linux HOWTO <http://sunsite.unc.edu/LDP/HOWTO/> documents.


  2.  The Packet Radio Protocols and Linux.

  The AX.25 protocol offers both connected and connectionless modes of
  operation, and is used either by itself for point-point links, or to
  carry other protocols such as TCP/IP and NetRom.

  It is similar to X.25 level 2 in structure, with some extensions to
  make it more useful in the amateur radio environment.

  The NetRom protocol is an attempt at a full network protocol and uses
  AX.25 at its lowest layer as a datalink protocol. It provides a
  network layer that is an adapted form of AX.25. The NetRom protocol
  features dynamic routing and node aliases.

  The Rose protocol was conceived and first implemented by Tom Moulton
  W2VY and is an implementation of the X.25 packet layer protocol and is
  designed to operate with AX.25 as its datalink layer protocol. It too
  provides a network layer. Rose addresses take the form of 10 digit
  numbers. The first four digits are called the Data Network
  Identification Code (DNIC) and are taken from Appendix B of the CCITT
  X.121 recommendation. More information on the Rose protocol may be
  ontained from the RATS Web server <http://www.rats.org/>.

  Alan Cox developed some early kernel based AX.25 software support for
  Linux.  Jonathon Naylor <g4klx@g4klx.demon.co.uk> has taken up ongoing
  development of the code, has added NetRom and Rose support and is now
  the developer of the AX.25 related kernel code. DAMA support was
  developed by Joerg, DL1BKE, jreuter@poboxes.com. Baycom and SoundModem
  support were added by Thomas Sailer, <sailer@ife.ee.ethz.ch>. The
  AX.25 utility software is now maintained by me.

  The Linux code supports KISS based TNC's (Terminal Node Controllers),
  the Ottawa PI card, the Gracilis PacketTwin card and other Z8530 SCC
  based cards with the generic SCC driver and both the Parallel and
  Serial port Baycom modems. Thomas's new soundmodem driver supports
  Soundblaster and soundcards based on the Crystal chipset.

  The User programs contain a simple PMS (Personal Message System), a
  beacon facility, a line mode connect program, `listen' an example of
  how to capture all AX.25 frames at raw interface level and programs to
  configure the NetRom protocol. Included also are an AX.25 server style
  program to handle and despatch incoming AX.25 connections and a NetRom
  daemon which does most of the hard work for NetRom support.


  2.1.  How it all fits together.

  The Linux AX.25 implementation is a brand new implementation. While in
  many ways it may looks similar to NOS, or BPQ or other AX.25
  implementations, it is none of these and is not identical to any of
  them. The Linux AX.25 implementation is capable of being configured to
  behave almost identically to other implementations, but the
  configuration process is very different.

  To assist you in understanding how you need to think when configuring
  this section describes some of the structural features of the AX.25
  implementation and how it fits into the context of the overall Linux
  structure.

  Simplified Protocol Layering Diagram


       -----------------------------------------------
       | AF_AX25 | AF_NETROM |  AF_INET    | AF_ROSE |
       |=========|===========|=============|=========|
       |         |           |             |         |
       |         |           |    TCP/IP   |         |
       |         |           ----------    |         |
       |         |   NetRom           |    | Rose    |
       |         -------------------------------------
       |            AX.25                            |
       -----------------------------------------------



  This diagram simply illustrates that NetRom, Rose and TCP/IP all run
  directly on top of AX.25, but that each of these protocols is treated
  as a seperate protocol at the programming interface. The `AF_' names
  are simply the names given to the `Address Family' of each of these
  protocols when writing programs to use them. The important thing to
  note here is the implicit dependence on the configuration of your
  AX.25 devices before you can configure your NetRom, Rose or TCP/IP
  devices.


  Software module diagram of Linux Network Implementation

  ----------------------------------------------------------------------------
   User    | Programs  |   call        node    ||  Daemons | ax25d  mheardd
           |           |   pms         mheard  ||          | inetd  netromd
  ----------------------------------------------------------------------------
           | Sockets   | open(), close(), listen(), read(), write(), connect()
           |           |------------------------------------------------------
           |           |    AF_AX25   |  AF_NETROM  |   AF_ROSE   |  AF_INET
           |------------------------------------------------------------------
  Kernel   | Protocols |    AX.25     |   NetRom    |     Rose    | IP/TCP/UDP
           |------------------------------------------------------------------
           | Devices   |    ax0,ax1   |  nr0,nr1    | rose0,rose1 | eth0,ppp0
           |------------------------------------------------------------------
           | Drivers   |  Kiss   PI2   PacketTwin   SCC   BPQ     | slip ppp
           |           |      Soundmodem      Baycom              | ethernet
  ----------------------------------------------------------------------------
  Hardware | PI2 Card, PacketTwin Card, SCC card, Serial port, Ethernet Card
  ----------------------------------------------------------------------------

  This diagram is a little more general than the first. This diagram
  attempts to show the relationship between user applications, the ker-
  nel and the hardware.  It also shows the relationship between the
  Socket application programming interface, the actual protocol modules,
  the kernel networking devices and the device drivers. Anything in this
  diagram is dependent on anything underneath it, and in general you
  must configure from the bottom of the diagram upwards.  So for exam-
  ple, if you want to run the call program you must also configure the
  Hardware, then ensure that the kernel has the appropriate device
  driver, that you create the appropriate network device, that the ker-
  nel includes the desired protocol that presents a programming inter-
  face that the call program can use. I have attempted to lay out this
  document in roughly that order.


  3.  The AX.25/NetRom/Rose software components.

  The AX.25 software is comprised of three components, the kernel
  source, the network configuration tools and the utility programs.

  The version 2.0.xx Linux kernels include the AX.25, NetRom, Z8530 SCC,
  PI card and PacketTwin drivers by default. These have been
  significantly enhanced in the 2.1.* kernels. Unfortunately, the rest
  of the 2.1.*  kernels makes them fairly unstable at the moment and not
  a good choice for a production system. To solve this problem Jonathon
  Naylor has prepared a patch kit which will bring the amateur radio
  protocol support in a 2.0.28 kernel up to the standard of the 2.1.*
  kernels. This is very simple to apply, and provides a range of
  facilities not present in the standard kernel such as Rose support.


  3.1.  Finding the kernel, tools and utility packages.



  3.1.1.  The kernel source:

  The kernel source can be found in its usual place at: ftp.kernel.org


       /pub/linux/kernel/v2.0/linux-2.0.31.tar.gz



  The current version of the AX25 upgrade patch is available at:
  ftp.pspt.fi


       /pub/linux/ham/ax25/ax25-module-14e.tar.gz



  3.1.2.  The network tools:

  The latest alpha release of the standard Linux network tools support
  AX.25 and NetRom and can be found at: ftp.inka.de


       /pub/comp/Linux/networking/net-tools/net-tools-1.33.tar.gz



  The latest ipfwadm package can be found at: ftp.xos.nl


       /pub/linux/ipfwadm/



  3.1.3.  The AX25 utilities:

  There are two different families of AX25-utilities. One is for the
  2.0.* kernels and the other will work with either the 2.1.*  kernels
  or the 2.0.*+moduleXX kernels. The ax25-utils version number indicates
  the oldest version of kernel that they will work with. Please choose a
  version of the ax25-utils appropriate to your kernel. The following
  are working combinations. You must use one of the following
  combinations, any other combination will not work, or will not work
  well.



       Linux Kernel             AX25 Utility set
       ----------------------   -------------------------
       linux-2.0.29             ax25-utils-2.0.12c.tar.gz **
       linux-2.0.28+module12    ax25-utils-2.1.22b.tar.gz **
       linux-2.0.30+module14c   ax25-utils-2.1.42a.tar.gz
       linux-2.0.31+module14d   ax25-utils-2.1.42a.tar.gz
       linux-2.1.22 ++          ax25-utils-2.1.22b.tar.gz
       linux-2.1.42 ++          ax25-utils-2.1.42a.tar.gz



  Note: the ax25-utils-2.0.* series (marked above with the '**' symbol)
  is now obsolete and is no longer supported. This document covers
  configuration using the versions of software recommended above the
  table. While there are differences between the releases, most of the
  information will be relevant to earlier releases of code.

  The AX.25 utility programs can be found at: ftp.pspt.fi
  <ftp://ftp.pspt.fi/pub/linux/ham/ax25/>

  or at: sunsite.unc.edu <ftp://sunsite.unc.edu/pub/Linux/apps/ham/>


  4.  Installing the AX.25/NetRom/Rose software.

  To successfully install AX.25 support on your linux system you must
  configure and install an appropriate kernel and then install the AX.25
  utilities.


  4.1.  Compiling the kernel.

  If you are already familiar with the process of compiling the Linux
  Kernel then you can skip this section, just be sure to select the
  appropriate options when compiling the kernel. If you are not, then
  read on.

  The normal place for the kernel source to be unpacked to is the
  /usr/src directory into a subdirectory called linux.  To do this you
  should be logged in as root and execute a series of commands similar
  to the following:


       # mv linux linux.old
       # cd /usr/src
       # tar xvfz linux-2.0.31.tar.gz
       # tar xvfz /pub/net/ax25/ax25-module-14e.tar.gz
       # patch -p0 </usr/src/ax25-module-14/ax25-2.0.31-2.1.47-2.diff
       # cd linux



  After you have unpacked the kernel source and applied the upgrade, you
  need to run the configuration script and choose the options that suit
  your hardware configuration and the options that you wish built into
  your kernel.  You do this by using the command:



       # make menuconfig



  You might also try:



       # make config



  I'm going to describe the full screen method (menuconfig) because it
  is easier to move around, but you use whichever you are most
  comfortable with.

  In either case you will be offered a range of options at which you
  must answer `Y' or `N'. (Note you may also answer `M' if you are using
  modules.  For the sake of simplicity I will assume you are not, please
  make appropriate modifications if you are).

  The options most relevant to an AX.25 configuration are:



  Code maturity level options  --->
      ...
      [*] Prompt for development and/or incomplete code/drivers
      ...
  General setup  --->
      ...
      [*] Networking support
      ...
  Networking options  --->
      ...
      [*] TCP/IP networking
      [?] IP: forwarding/gatewaying
      ...
      [?] IP: tunneling
      ...
      [?] IP: Allow large windows (not recommended if <16Mb of memory)
      ...
      [*] Amateur Radio AX.25 Level 2
      [?] Amateur Radio NET/ROM
      [?] Amateur Radio X.25 PLP (Rose)
      ...
  Network device support  --->
      [*] Network device support
      ...
      [*] Radio network interfaces
      [?] BAYCOM ser12 and par96 driver for AX.25
      [?] Soundcard modem driver for AX.25
      [?] Soundmodem support for Soundblaster and compatible cards
      [?] Soundmodem support for WSS and Crystal cards
      [?] Soundmodem support for 1200 baud AFSK modulation
      [?] Soundmodem support for 4800 baud HAPN-1 modulation
      [?] Soundmodem support for 9600 baud FSK G3RUH modulation
      [?] Serial port KISS driver for AX.25
      [?] BPQ Ethernet driver for AX.25
      [?] Gracilis PackeTwin support for AX.25
      [?] Ottawa PI and PI/2 support for AX.25
      [?] Z8530 SCC KISS emulation driver for AX.25
      ...



  The options I have flagged with a `*' are those that you must must
  answer `Y' to. The rest are dependent on what hardware you have and
  what other options you want to include. Some of these options are
  described in more detail later on, so if you don't know what you want
  yet, then read ahead and come back to this step later.

  After you have completed the kernel configuration you should be able
  to cleanly compile your new kernel:



       # make dep
       # make clean
       # make zImage



  maake sure you move your arch/i386/boot/zImage file wherever you want
  it and then edit your /etc/lilo.conf file and rerun lilo to ensure
  that you actually boot from it.



  4.1.1.  A word about Kernel modules

  I recommend that you don't compile any of the drivers as modules. In
  nearly all installations you gain nothing but additional complexity.
  Many people have problems trying to get the modularised components
  working, not because the software is faulty but because modules are
  more complicated to install and configure.

  If you've chosen to compile any of the components as modules, then
  you'll also need to use:



       # make modules
       # make modules_install



  to install your modules in the appropriate location.

  You will also need to add some entries into your /etc/conf.modules
  file that will ensure that the kerneld program knows how to handle the
  kernel modules. You should add/modify the following:



       alias net-pf-3     ax25
       alias net-pf-6     netrom
       alias net-pf-11    rose
       alias tty-ldisc-1  slip
       alias tty-ldisc-3  ppp
       alias tty-ldisc-5  mkiss
       alias bc0          baycom
       alias nr0          netrom
       alias pi0a         pi2
       alias pt0a         pt
       alias scc0         optoscc    (or one of the other scc drivers)
       alias sm0          soundmodem
       alias tunl0        newtunnel
       alias char-major-4 serial
       alias char-major-5 serial
       alias char-major-6 lp



  4.1.2.  What's new in 2.0.*+ModuleXX or 2.1.* Kernels ?

  The 2.1.* kernels have enhanced versions of nearly all of the
  protocols and drivers. The most significant of the enhancements are:

     modularised
        the protocols and drivers have all been modularised so that you
        can insmod and rmmod them whenever you wish. This reduces the
        kernel memory requirements for infrequently used modules and
        makes development and bug hunting much simpler. That being said,
        it also makes configuration slightly more difficult.

     All drivers are now network drivers
        all of the network devices such as Baycom, SCC, PI, Packettwin
        etc now present a normal network interface, that is they now
        look like the ethernet driver does, they no longer look like
        KISS TNC's. A new utility called net2kiss allows you to build a
        kiss interface to these devices if you wish.
     bug fixed
        there have been many bug fixes and new features added to the
        drivers and protocols. The Rose protocol is one important
        addition.


  4.2.  The network configuration tools.

  Now that you have compiled the kernel you should compile the new
  network configuration tools. These tools allow you to modify the
  configuration of network devices and to add routes to the routing
  table.

  The new alpha release of the standard net-tools package includes
  support for AX.25 and NetRom support. I've tested this and it seems to
  work well for me.


  4.2.1.  A patch kit that adds Rose support and fixes some bugs.

  The standard net-tools-1.33.tar.gz package has some small bugs that
  affect the AX.25 and NetRom support. I've made a small patch kit that
  corrects these and adds Rose support to the tools as well.

  You can get the patch from: zone.pspt.fi
  <ftp://zone.pspt.fi/pub/linux/ham/ax25/net-
  tools-1.33.rose.tjd.diff.gz>.



  4.2.2.  Building the standard net-tools release.

  Don't forget to read the Release file and follow any instructions
  there. The steps I used to compile the tools were:



       # cd /usr/src
       # tar xvfz net-tools-1.33.tar.gz
       # zcat net-tools-1.33.rose.tjd.diff.gz | patch -p0
       # cd net-tools-1.33
       # make config



  At this stage you will be presented with a series of configuration
  questions, similar to the kernel configuration questions. Be sure to
  include support for all of the protocols and network devices types
  that you intend to use. If you do not know how to answer a particular
  question then answer `Y'.

  When the compilation is complete, you should use the:



       # make install



  command to install the programs in their proper place.


  If you wish to use the IP firewall facilities then you will need the
  latest firewall administration tool ipfwadm. This tool replaces the
  older ipfw tool which will not work with new kernels.

  I compiled the ipfwadm utility with the following commands:


       # cd /usr/src
       # tar xvfz ipfwadm-2.0beta2.tar.gz
       # cd ipfwadm-2.0beta2
       # make install
       # cp ipfwadm.8 /usr/man/man8
       # cp ipfw.4 /usr/man/man4



  4.3.  The AX.25 user and utility programs.

  After you have successfully compiled and booted your new kernel, you
  need to compile the user programs. To compile and install the user
  programs you should use a series of commands similar to the following:



       # cd /usr/src
       # tax xvfz ax25-utils-2.1.42a.tar.gz
       # cd ax25-utils-2.1.42a
       # make config
       # make
       # make install



  The files will be installed under the /usr directory by default in
  subdirectories: bin, sbin, etc and man.

  If this is a first time installation, that is you've never installed
  any ax25 utilities on your machine before you should also use the:


       # make installconf



  command to install some sample configuration files into the /etc/ax25/
  directory from which to work.


  If you get messages something like:

  gcc -Wall -Wstrict-prototypes -O2 -I../lib -c call.c
  call.c: In function `statline':
  call.c:268: warning: implicit declaration of function `attron'
  call.c:268: `A_REVERSE' undeclared (first use this function)
  call.c:268: (Each undeclared identifier is reported only once
  call.c:268: for each function it appears in.)



  then you should double check that you have the ncurses package
  properly installed on your system. The configuration script attempts
  to locate your ncurses packages in the common locations, but some
  installations have ncurses badly installed and it is unable to locate
  them.
  5.  A note on callsigns, addresses and things before we start.

  Each AX.25 and NetRom port on your system must have a callsign/ssid
  allocated to it. These are configured in the configuration files that
  will be described in detail later on.

  Some AX.25 implementations such as NOS and BPQ will allow you to
  configure the same callsign/ssid on each AX.25 and NetRom port. For
  somewhat complicated technical reasons Linux does not allow this. This
  isn't as big a problem in practise as it might seem.

  This means that there are things you should be aware of and take into
  consideration when doing your configurations.


  1. Each AX.25 and NetRom port must be configured with a unique
     callsign/ssid.

  2. TCP/IP will use the callsign/ssid of the AX.25 port it is being
     transmitted or received by, ie the one you configured for the AX.25
     interface in point 1.

  3. NetRom will use the callsign/ssid specified for it in its
     configuration file, but this callsign is only used when your NetRom
     is speaking to another NetRom, this is not the callsign/ssid that
     AX.25 users who wish to use your NetRom `node' will use. More on
     this later.

  4. Rose will, by default, use the callsign/ssid of the AX.25 port,
     unless the Rose callsign has been specifically set using the
     `rsparms' command.  If you set a callsign/ssid using the `rsparms'
     command then Rose will use this callsign/ssid on all ports.

  5. Other programs, such as the `ax25d' program can listen using any
     callsign/ssid that they wish and these may be duplicated across
     different ports.

  6. If you are careful with routing you can configure the same IP
     address on all ports if you wish.


  5.1.  What are all those T1, T2, N2 and things ?

  Not every AX.25 implementation is a TNC2. Linux uses nomenclature that
  differs in some respects from that you will be used to if your sole
  experience with packet is a TNC. The following table should help you
  interpret what each of the configurable items are, so that when you
  come across them later in this text you'll understand what they mean.



  -------------------------------------------------------------------
  Linux  | TAPR TNC | Description
  -------------------------------------------------------------------
  T1     | FRACK    | How long to wait before retransmitting an
         |          | unacknowledged frame.
  -------------------------------------------------------------------
  T2     | RESPTIME | The minimum amount of time to wait for another
         |          | frame to be received before transmitting
         |          | an acknowledgement.
  -------------------------------------------------------------------
  T3     | CHECK    | The period of time we wait between sending
         |          | a check that the link is still active.
  -------------------------------------------------------------------
  N2     | RETRY    | How many times to retransmit a frame before
         |          | assuming the connection has failed.
  -------------------------------------------------------------------
  Idle   |          | The period of time a connection can be idle
         |          | before we close it down.
  -------------------------------------------------------------------
  Window | MAXFRAME | The maximum number of unacknowledged
         |          | transmitted frames.
  -------------------------------------------------------------------



  5.2.  Run time configurable parameters

  The 2.1.* and 2.0.* +moduleXX kernels have a new feature that allows
  you to change many previously unchangable parameters at run time. If
  you take a careful look at the /proc/sys/net/ directory structure you
  will see many files with useful names that describe various parameters
  for the network configuration. The files in the /proc/sys/net/ax25/
  directory each represents one configured AX.25 port. The name of the
  file relates to the name of the port.

  The structure of the files in /proc/sys/net/ax25/<portname>/ is as
  follows:

  FileName              Meaning              Values                  Default
  ip_default_mode       IP Default Mode      0=DG 1=VC               0
  ax25_default_mode     AX.25 Default Mode   0=Normal 1=Extended     0
  backoff_type          Backoff              0=Linear 1=Exponential  1
  connect_mode          Connected Mode       0=No 1=Yes              1
  standard_window_size  Standard Window      1  <= N <= 7            2
  extended_window_size  Extended Window      1  <= N <= 63           32
  t1_timeout            T1 Timeout           1s <= N <= 30s          10s
  t2_timeout            T2 Timeout           1s <= N <= 20s          3s
  t3_timeout            T3 Timeout           0s <= N <= 3600s        300s
  idle_timeout          Idle Timeout         0m <= N                 20m
  maximum_retry_count   N2                   1  <= N <= 31           10
  maximum_packet_length AX.25 Frame Length   1  <= N <= 512          256


  In the table T1, T2 and T3 are given in seconds, and the Idle Timeout
  is given in minutes. But please note that the values used in the
  sysctl interface are given in internal units where the time in seconds
  is multiplied by 10, this allows resolution down to 1/10 of a second.
  With timers that are allowed to be zero, eg T3 and Idle, a zero value
  indicates that the timer is disabled.


  The structure of the files in /proc/sys/net/netrom/ is as follows:


  FileName                       Values                  Default
  default_path_quality                                   10
  link_fails_count                                       2
  network_ttl_initialiser                                16
  obsolescence_count_initialiser                         6
  routing_control                                        1
  transport_acknowledge_delay                            50
  transport_busy_delay                                   1800
  transport_maximum_tries                                3
  transport_requested_window_size                        4
  transport_timeout                                      1200



  The structure of the files in /proc/sys/net/rose/ is as follows:

  FileName                       Values                  Default
  acknowledge_hold_back_timeout                          50
  call_request_timeout                                   2000
  clear_request_timeout                                  1800
  link_fail_timeout                                      1200
  maximum_virtual_circuits                               50
  reset_request_timeout                                  1800
  restart_request_timeout                                1800
  routing_control                                        1
  window_size                                            3



  To set a parameter all you need to do is write the desired value to
  the file itself, for example to check and set the Rose window size
  you'd use something like:


       # cat /proc/sys/net/rose/window_size
       3
       # echo 4 >/proc/sys/net/rose/window_size
       # cat /proc/sys/net/rose/window_size
       4



  6.  Configuring an AX.25 port.

  Each of the AX.25 applications read a particular configuration file to
  obtain the parameters for the various AX.25 ports configured on your
  Linux machine.  For AX.25 ports the file that is read is the
  /etc/ax25/axport file.  You must have an entry in this file for each
  AX.25 port you want on your system.


  6.1.  Creating the AX.25 network device.

  The network device is what is listed when you use the `ifconfig'
  command. This is the object that the Linux kernel sends and receives
  network data from. Nearly always the network device has a physical
  port associated with it, but there are occasions where this isn't
  necessary. The network device does relate directly to a device driver.

  In the Linux AX.25 code there are a number of device drivers. The most
  common is probably the KISS driver, but others are the SCC driver(s),
  the Baycom driver and the SoundModem driver.
  Each of these device drivers will create a network device when it is
  started.


  6.1.1.  Creating a KISS device.

  Kernel Compile Options:


       General setup  --->
           [*] Networking support
       Network device support  --->
           [*] Network device support
           ...
           [*] Radio network interfaces
           [*] Serial port KISS driver for AX.25



  Probably the most common configuration will be for a KISS TNC on a
  serial port.  You will need to have the TNC preconfigured and
  connected to your serial port.  You can use a communications program
  like minicom or seyon to configure the TNC into kiss mode.

  To create a KISS device you use the kissattach program. In it simplest
  form you can use the kissattach program as follows:



       # /usr/sbin/kissattach /dev/ttyS0 radio
       # kissparms -p radio -t 100 -s 100 -r 25



  The kissattach command will create a KISS network device. These
  devices are called `ax[0-9]'. The first time you use the kissattach
  command it creates `ax0', the second time it creates `ax1' etc. Each
  KISS device has an associated serial port.

  The kissparms command allows you to set various KISS parameters on a
  KISS device.

  Specifically the example presented would create a KISS network device
  using the serial device `/dev/ttyS0' and the entry from the
  /etc/ax25/axports with a port name of `radio'. It then configures it
  with a txdelay and slottime of 100 milliseconds and a ppersist value
  of 25.

  Please refer to the man pages for more information.


  6.1.1.1.  Configuring for Dual Port TNC's

  The mkiss utility included in the ax25-utils distribution allows you
  to make use of both modems on a dual port TNC. Configuration is fairly
  simple. It works by taking a single serial device connected to a
  single multiport TNC and making it look like a number of devices each
  connected to a single port TNC. You do this before you do any of the
  AX.25 configuration. The devices that you then do the AX.25
  configuration on are pseudo-TTY interfaces, (/dev/ttyq*), and not the
  actual serial device. Pseudo-TTY devices create a kind of pipe through
  which programs designed to talk to tty devices can talk to other
  programs designed to talk to tty devices. Each pipe has a master and a
  slave end. The master end is generally called `/dev/ptyq*' and the
  slave ends are called `/dev/ttyq*'. There is a one to one relationship
  between masters and slaves, so /dev/ptyq0 is the master end of a pipe
  with /dev/ttyq0 as its slave. You must open the master end of a pipe
  before opening the slave end. mkiss exploits this mechanism to split a
  single serial device into seperate devices.


  Example: if you have a dual port tnc and it is connected to your
  /dev/ttyS0 serial device at 9600 bps, the command:



       # /usr/sbin/mkiss -s 9600 /dev/ttyS0 /dev/ptyq0 /dev/ptyq1
       # /usr/sbin/kissattach /dev/ttyq0 port1
       # /usr/sbin/kissattach /dev/ttyq1 port2



  would create two pseudo-tty devices that each look like a normal
  single port TNC. You would then treat /dev/ttyq0 and /dev/ttyq1 just
  as you would a conventional serial device with TNC connected. This
  means you'd then use the kissattach command as described above, on
  each of those, in the example for AX.25 ports called port1 and port2.
  You shouldn't use kissattach on the actual serial device as the mkiss
  program uses it.

  The mkiss command has a number of optional arguments that you may wish
  to use. They are summarised as follows:

     -c enables the addition of a one byte checksum to each KISS frame.
        This is not supported by most KISS implementation, it is
        supported by the G8BPG KISS rom.

     -s <speed>
        sets the speed of the serial port.

     -h enables hardware handshaking on the serial port, it is off by
        default. Most KISS implementation do not support this, but some
        do.

     -l enables logging of information to the syslog logfile.


  6.1.2.  Creating a Baycom device.

  Kernel Compile Options:


       Code maturity level options  --->
           [*] Prompt for development and/or incomplete code/drivers
       General setup  --->
           [*] Networking support
       Network device support  --->
           [*] Network device support
           ...
           [*] Radio network interfaces
           [*] BAYCOM ser12 and par96 driver for AX.25



  Thomas Sailer, <sailer@ife.ee.ethz.ch>, despite the popularly held
  belief that it would not work very well, has developed Linux support
  for Baycom modems. His driver supports the Ser12 serial port, Par96
  and the enhanced PicPar parallel port modems.  Further information
  about the modems themselves may be obtained from the Baycom Web site
  <http://www.baycom.de/>.

  Your first step should be to determine the i/o and addresses of the
  serial or parallel port(s) you have Baycom modem(s) connected to.
  When you have these you must configure the Baycom driver with them.

  The BayCom driver creates network devices called: bc0, bc1, bc2 etc.
  when it is configured.

  The sethdlc utility allows you to configure the driver with these
  parameters, or, if you have only one Baycom modem installed you may
  specify the parameters on the insmod commmand line when you load the
  Baycom module.

  For example, a simple configuration.  Disable the serial driver for
  COM1: then configure the Baycom driver for a Ser12 serial port modem
  on COM1: with the software DCD option enabled:


       # setserial /dev/ttyS0 uart none
       # insmod hdlcdrv
       # insmod baycom mode="ser12*" iobase=0x3f8 irq=4



  Par96 parallel port type modem on LPT1: using hardware DCD detection:


       # insmod hdlcdrv
       # insmod baycom mode="par96" iobase=0x378 irq=7 options=0



  This is not really the preferred way to do it. The sethdlc utility
  works just as easily with one device as with many.

  The sethdlc man page has the full details, but a couple of examples
  will illustrate the most important aspects of this configuration. The
  following examples assume you have already loaded the Baycom module
  using:


       # insmod hdlcdrv
       # insmod baycom



  or that you compiled the kernel with the driver inbuilt.

  Configure the bc0 device driver as a Parallel port Baycom modem on
  LPT1: with software DCD:


       # sethdlc -p -i bc0 mode par96 io 0x378 irq 7



  Configure the bc1 device driver as a Serial port Baycom modem on COM1:



  # sethdlc -p -i bc1 mode "ser12*" io 0x3f8 irq 4



  6.1.3.  Configuring the AX.25 channel access parameters.

  The AX.25 channel access parameters are the equivalent of the KISS
  ppersist, txdelay and slottime type parameters. Again you use the
  sethdlc utility for this.

  Again the sethdlc man page is the source of the most complete
  information but another example of two won't hurt:

  Configure the bc0 device with TxDelay of 200 mS, SlotTime of 100 mS,
  PPersist of 40 and half duplex:


       # sethdlc -i bc0 -a txd 200 slot 100 ppersist 40 half



  Note that the timing values are in milliseconds.


  6.1.3.1.  Configuring the Kernel AX.25 to use the BayCom device

  The BayCom driver creates standard network devices that the AX.25
  Kernel code can use. Configuration is much the same as that for a PI
  or PacketTwin card.

  The first step is to configure the device with an AX.25 callsign. The
  ifconfig utility may be used to perform this.


       # /sbin/ifconfig bc0 hw ax25 VK2KTJ-15 up



  will assign the BayCom device bc0 the AX.25 callsign VK2KTJ-15.
  Alternatively you can use the axparms command, you'll still need to
  use the ifconfig command to bring the device up though:


       # ifconfig bc0 up
       # axparms -setcall bc0 vk2ktj-15



  The next step is to create an entry in the /etc/ax25/axports file as
  you would for any other device. The entry in the axports file is
  associated with the network device you've configured by the callsign
  you configure. The entry in the axports file that has the callsign
  that you configured the BayCom device with is the one that will be
  used to refer to it.

  You may then treat the new AX.25 device as you would any other. You
  can configure it for TCP/IP, add it to ax25d and run NetRom or Rose
  over it as you please.



  6.1.4.  Creating a SoundModem device.

  Kernel Compile Options:


       Code maturity level options  --->
           [*] Prompt for development and/or incomplete code/drivers
       General setup  --->
           [*] Networking support
       Network device support  --->
           [*] Network device support
           ...
           [*] Radio network interfaces
           [*] Soundcard modem driver for AX.25
           [?] Soundmodem support for Soundblaster and compatible cards
           [?] Soundmodem support for WSS and Crystal cards
           [?] Soundmodem support for 1200 baud AFSK modulation
           [?] Soundmodem support for 4800 baud HAPN-1 modulation
           [?] Soundmodem support for 9600 baud FSK G3RUH modulation



  Thomas Sailer has built a new driver for the kernel that allows you to
  use your soundcard as a modem. Connect your radio directly to your
  soundcard to play packet! Thomas recommends at least a 486DX2/66 if
  you want to use this software as all of the digital signal processing
  is done by the main CPU.

  The driver currently emulates 1200 bps AFSK, 4800 HAPN and 9600 FSK
  (G3RUH compatible) modem types. The only sound cards currently
  supported are SoundBlaster and WindowsSoundSystem Compatible models.
  The sound cards require some circuitry to help them drive the Push-To-
  Talk circuitry, and information on this is available from Thomas's
  SoundModem PTT circuit web page
  <http://www.ife.ee.ethz.ch/~sailer/pcf/ptt_circ/ptt.html>. There are
  quite a few possible options, they are: detect the sound output from
  the soundcard, or use output from a parallel port, serial port or midi
  port. Circuit examples for each of these are on Thomas's site.

  The SoundModem driver creates network devices called: sm0, sm1, sm2
  etc when it is configured.

  Note: the SoundModem driver competes for the same resources as the
  Linux sound driver. So if you wish to use the SoundModem driver you
  must ensure that the Linux sound driver is not installed. You can of
  course compile them both as modules and insert and remove them as you
  wish.


  6.1.4.1.  Configuring the sound card.

  The SoundModem driver does not initialise the sound card. The
  ax25-utils package includes a utility to do this called `setcrystal'
  that may be used for SoundCards based on the Crystal chipset. If you
  have some other card then you will have to use some other software to
  initialise it.  Its syntax is fairly straightforward:


       setcrystal [-w wssio] [-s sbio] [-f synthio] [-i irq] [-d dma] [-c dma2]



  So, for example, if you wished to configure a soundblaster card at i/o
  base address 0x388, irq 10 and DMA 1 you would use:
       # setcrystal -s 0x388 -i 10 -d 1



  To configure a WindowSoundSystem card at i/o base address 0x534, irq
  5, DMA 3 you would use:


       # setcrystal -w 0x534 -i 5 -d 3



  The [-f synthio] parameter is the set the synthesiser address, and the
  [-c dma2] parameter is to set the second DMA channel to allow full
  duplex operation.


  6.1.4.2.  Configuring the SoundModem driver.

  When you have configured the soundcard you need to configure the
  driver telling it where the sound card is located and what sort of
  modem you wish it to emulate.

  The sethdlc utility allows you to configure the driver with these
  parameters, or, if you have only one soundcard installed you may
  specify the parameters on the insmod commmand line when you load the
  SoundModem module.

  For example, a simple configuration, with one SoundBlaster soundcard
  configured as described above emulating a 1200 bps modem:


       # insmod hdlcdrv
       # insmod soundmodem mode="sbc:afsk1200" iobase=0x220 irq=5 dma=1



  This is not really the preferred way to do it. The sethdlc utility
  works just as easily with one device as with many.

  The sethdlc man page has the full details, but a couple of examples
  will illustrate the most important aspects of this configuration. The
  following examples assume you have already loaded the SoundModem
  modules using:


       # insmod hdlcdrv
       # insmod soundmodem



  or that you compiled the kernel with the driver inbuilt.

  Configure the driver to support the WindowsSoundSystem card we
  configured above to emulate a G3RUH 9600 compatible modem as device
  sm0 using a parallel port at 0x378 to key the Push-To-Talk:


       # sethdlc -p -i sm0 mode wss:fsk9600 io 0x534 irq 5 dma 3 pario 0x378



  Configure the driver to support the SoundBlaster card we configured
  above to emulate a 4800 bps HAPN modem as device sm1 using the serial
  port located at 0x2f8 to key the Push-To-Talk:


       # sethdlc -p -i sm1 mode sbc:hapn4800 io 0x388 irq 10 dma 1 serio 0x2f8



  Configure the driver to support the SoundBlaster card we configured
  above to emulate a 1200 bps AFSK modem as device sm1 using the serial
  port located at 0x2f8 to key the Push-To-Talk:


       # sethdlc -p -i sm1 mode sbc:afsk1200 io 0x388 irq 10 dma 1 serio 0x2f8



  6.1.4.3.  Configuring the AX.25 channel access parameters.

  The AX.25 channel access parameters are the equivalent of the KISS
  ppersist, txdelay and slottime type parameters. You use the sethdlc
  utility for this as well.

  Again the sethdlc man page is the source of the most complete
  information but another example of two won't hurt:

  Configure the sm0 device with TxDelay of 100 mS, SlotTime of 50mS,
  PPersist of 128 and full duplex:


       # sethdlc -i sm0 -a txd 100 slot 50 ppersist 128 full



  Note that the timing values are in milliseconds.


  6.1.4.4.  Setting the audio levels and tuning the driver.

  It is very important that the audio levels be set correctly for any
  radio based modem to work. This is equally true of the SoundModem.
  Thomas has developed some utility programs that make this task easier.
  They are called smdiag and smmixer.


     smdiag
        provides two types of display, either an oscilloscope type
        display or an eye pattern type display.

     smmixer
        allows you to actually adjust the transmit and receive audio
        levels.

  To start the smdiag utility in 'eye' mode for the SoundModem device
  sm0 you would use:


       # smdiag -i sm0 -e



  To start the smmixer utility for the SoundModem device sm0 you would
  use:


       # smmixer -i sm0



  6.1.4.5.  Configuring the Kernel AX.25 to use the SoundModem

  The SoundModem driver creates standard network devices that the AX.25
  Kernel code can use. Configuration is much the same as that for a PI
  or PacketTwin card.

  The first step is to configure the device with an AX.25 callsign.  The
  ifconfig utility may be used to perform this.


       # /sbin/ifconfig sm0 hw ax25 VK2KTJ-15 up



  will assign the SoundModem device sm0 the AX.25 callsign VK2KTJ-15.
  Alternatively you can use the axparms command, but you still need the
  ifconfig utility to bring the device up:


       # ifconfig sm0 up
       # axparms -setcall sm0 vk2ktj-15



  The next step is to create an entry in the /etc/ax25/axports file as
  you would for any other device. The entry in the axports file is
  associated with the network device you've configured by the callsign
  you configure. The entry in the axports file that has the callsign
  that you configured the SoundModem device with is the one that will be
  used to refer to it.

  You may then treat the new AX.25 device as you would any other. You
  can configure it for TCP/IP, add it to ax25d and run NetRom or Rose
  over it as you please.


  6.1.5.  Creating a PI card device.

  Kernel Compile Options:


       General setup  --->
           [*] Networking support
       Network device support  --->
           [*] Network device support
           ...
           [*] Radio network interfaces
           [*] Ottawa PI and PI/2 support for AX.25



  The PI card device driver creates devices named `pi[0-9][ab]'. The
  first PI card detected will be allocated `pi0', the second `pi1' etc.
  The `a' and `b' refer to the first and second physical interface on
  the PI card. If you have built your kernel to include the PI card
  driver, and the card has been properly detected then you can use the
  following command to configure the network device:



       # /sbin/ifconfig pi0a hw ax25 VK2KTJ-15 up



  This command would configure the first port on the first PI card
  detected with the callsign VK2KTJ-15 and make it active. To use the
  device all you now need to do is to configure an entry into your
  /etc/ax25/axports file with a matching callsign/ssid and you will be
  ready to continue on.


  The PI card driver was written by David Perry, <dp@hydra.carleton.edu>


  6.1.6.  Creating a PacketTwin device.

  Kernel Compile Options:


       General setup  --->
           [*] Networking support
       Network device support  --->
           [*] Network device support
           ...
           [*] Radio network interfaces
           [*] Gracilis PackeTwin support for AX.25



  The PacketTwin card device driver creates devices named `pt[0-9][ab]'.
  The first PacketTwin card detected will be allocated `pt0', the second
  `pt1' etc. The `a' and `b' refer to the first and second physical
  interface on the PacketTwin card. If you have built your kernel to
  include the PacketTwin card driver, and the card has been properly
  detected then you can use the following command to configure the
  network device:



       # /sbin/ifconfig pt0a hw ax25 VK2KTJ-15 up



  This command would configure the first port on the first PacketTwin
  card detected with the callsign VK2KTJ-15 and make it active. To use
  the device all you now need to do is to configure an entry into your
  /etc/ax25/axports file with a matching callsign/ssid and you will be
  ready to continue on.

  The PacketTwin card driver was written by Craig Small VK2XLZ,
  <csmall@triode.apana.org.au>.



  6.1.7.  Creating a generic SCC device.

  Kernel Compile Options:


       General setup  --->
           [*] Networking support
       Network device support  --->
           [*] Network device support
           ...
           [*] Radio network interfaces
           [*] Z8530 SCC KISS emulation driver for AX.25



  Joerg Reuter, DL1BKE, jreuter@poboxes.com has developed generic
  support for Z8530 SCC based cards. His driver is configurable to
  support a range of different types of cards and present an interface
  that looks like a KISS TNC so you can treat it as though it were a
  KISS TNC.


  6.1.7.1.  Obtaining and building the configuration tool package.

  While the kernel driver is included in the standard kernel
  distribution, Joerg distributes more recent versions of his driver
  with the suite of configuration tools that you will need to obtain as
  well.

  You can obtain the configuration tools package from:

  Joerg's web page <http://www.rat.de/jr/>

  or:

  db0bm.automation.fh-aachen.de


       /incoming/dl1bke/



  or:

  insl1.etec.uni-karlsruhe.de


       /pub/hamradio/linux/z8530/



  or:

  ftp.ucsd.edu


       /hamradio/packet/tcpip/linux
       /hamradio/packet/tcpip/incoming/



  You will find multiple versions, choose the one that best suits the
  kernel you intend to use:


  z8530drv-2.4a.dl1bke.tar.gz   2.0.*
  z8530drv-utils-3.0.tar.gz    2.1.6 or greater



  The following commands were what I used to compile and install the
  package for kernel version 2.0.30:


       # cd /usr/src
       # gzip -dc z8530drv-2.4a.dl1bke.tar.gz | tar xvpofz -
       # cd z8530drv
       # make clean
       # make dep
       # make module         # If you want to build the driver as a module
       # make for_kernel     # If you want the driver to built into your kernel
       # make install



  After the above is complete you should have three new programs
  installed in your /sbin directory: gencfg, sccinit and sccstat. It is
  these programs that you will use to configure the driver for your
  card.

  You will also have a group of new special device files created in your
  /dev called scc0-scc7. These will be used later and will be the `KISS'
  devices you will end up using.

  If you chose to 'make for_kernel' then you will need to recompile your
  kernel. To ensure that you include support for the z8530 driver you
  must be sure to answer `Y' to: `Z8530 SCC kiss emulation driver for
  AX.25' when asked during a kernel `make config'.

  If you chose to 'make module' then the new scc.o will have been
  installed in the appropriate /lib/modules directory and you do not
  need to recompile your kernel. Remember to use the insmod command to
  load the module before your try and configure it.


  6.1.7.2.  Configuring the driver for your card.

  The z8530 SCC driver has been designed to be as flexible as possible
  so as to support as many different types of cards as possible. With
  this flexibility has come some cost in configuration.

  There is more comprehensive documentation in the package and you
  should read this if you have any problems. You should particularly
  look at doc/scc_eng.doc or doc/scc_ger.doc for more detailed
  information. I've paraphrased the important details, but as a result
  there is a lot of lower level detail that I have not included.

  The main configuration file is read by the sccinit program and is
  called /etc/z8530drv.conf. This file is broken into two main stages:
  Configuration of the hardware parameters and channel configuration.
  After you have configured this file you need only add:



  # sccinit



  into the rc file that configures your network and the driver will be
  initialised according to the contents of the configuration file. You
  must do this before you attempt to use the driver.



  6.1.7.2.1.  Configuration of the hardware parameters.

  The first section is broken into stanzas, each stanza representing an
  8530 chip. Each stanza is a list of keywords with arguments. You may
  specify up to four SCC chips in this file by default. The #define
  MAXSCC 4 in scc.c can be increased if you require support for more.

  The allowable keywords and arguments are:


     chip
        the chip keyword is used to separate stanzas. It will take
        anything as an argument. The arguments are not used.

     data_a
        this keyword is used to specify the address of the data port for
        the z8530 channel `A'. The argument is a hexadecimal number e.g.
        0x300

     ctrl_a
        this keyword is used to specify the address of the control port
        for the z8530 channel `A'. The arguments is a hexadecimal number
        e.g. 0x304

     data_b
        this keyword is used to specify the address of the data port for
        the z8530 channel `B'. The argument is a hexadecimal number e.g.
        0x301

     ctrl_b
        this keyword is used to specify the address of the control port
        for the z8530 channel `B'. The arguments is a hexadecimal number
        e.g. 0x305

     irq
        this keyword is used to specify the IRQ used by the 8530 SCC
        described in this stanza. The argument is an integer e.g. 5

     pclock
        this keyword is used to specify the frequency of the clock at
        the PCLK pin of the 8530. The argument is an integer frequency
        in Hz which defaults to 4915200 if the keyword is not supplied.

     board
        the type of board supporting this 8530 SCC. The argument is a
        character string. The allowed values are:

        PA0HZP
           the PA0HZP SCC Card

        EAGLE
           the Eagle card

        PC100
           the DRSI PC100 SCC card
        PRIMUS
           the PRIMUS-PC (DG9BL) card

        BAYCOM
           BayCom (U)SCC card

     escc
        this keyword is optional and is used to enable support for the
        Extended SCC chips (ESCC) such as the 8580, 85180, or the 85280.
        The argument is a character string with allowed values of `yes'
        or `no'. The default is `no'.

     vector
        this keyword is optional and specifies the address of the vector
        latch (also known as "intack port") for PA0HZP cards. There can
        be only one vector latch for all chips. The default is 0.

     special
        this keyword is optional and specifies the address of the
        special function register on several cards. The default is 0.

     option
        this keyword is optional and defaults to 0.

  Some example configurations for the more popular cards are as follows:


     BayCom USCC


          chip    1
          data_a  0x300
          ctrl_a  0x304
          data_b  0x301
          ctrl_b  0x305
          irq     5
          board   BAYCOM
          #
          # SCC chip 2
          #
          chip    2
          data_a  0x302
          ctrl_a  0x306
          data_b  0x303
          ctrl_b  0x307
          board   BAYCOM



     PA0HZP SCC card



     chip 1
     data_a 0x153
     data_b 0x151
     ctrl_a 0x152
     ctrl_b 0x150
     irq 9
     pclock 4915200
     board PA0HZP
     vector 0x168
     escc no
     #
     #
     #
     chip 2
     data_a 0x157
     data_b 0x155
     ctrl_a 0x156
     ctrl_b 0x154
     irq 9
     pclock 4915200
     board PA0HZP
     vector 0x168
     escc no



     DRSI SCC card


          chip 1
          data_a 0x303
          data_b 0x301
          ctrl_a 0x302
          ctrl_b 0x300
          irq 7
          pclock 4915200
          board DRSI
          escc no



  If you already have a working configuration for your card under NOS,
  then you can use the gencfg command to convert the PE1CHL NOS driver
  commands into a form suitable for use in the z8530 driver
  configuration file.

  To use gencfg you simply invoke it with the same parameters as you
  used for the PE1CHL driver in NET/NOS. For example:


       # gencfg 2 0x150 4 2 0 1 0x168 9 4915200



  will generate a skeleton configuration for the OptoSCC card.


  6.1.7.3.  Channel Configuration

  The Channel Configuration section is where you specify all of the
  other parameters associated with the port you are configuring. Again
  this section is broken into stanzas. One stanza represents one logical
  port, and therefore there would be two of these for each one of the
  hardware parameters stanzas as each 8530 SCC supports two ports.

  These keywords and arguments are also written to the
  /etc/z8530drv.conf file and must appear after the hardware parameters
  section.

  Sequence is very important in this section, but if you stick with the
  suggested sequence it should work ok. The keywords and arguments are:

     device
        this keyword must be the first line of a port definition and
        specifies the name of the special device file that the rest of
        the configuration applies to. e.g. /dev/scc0

     speed
        this keyword specifies the speed in bits per second of the
        interface. The argument is an integer: e.g. 1200

     clock
        this keyword specifies where the clock for the data will be
        sourced. Allowable values are:

        dpll
           normal halfduplex operation

        external
           MODEM supplies its own Rx/Tx clock

        divider
           use fullduplex divider if installed.

     mode
        this keyword specifies the data coding to be used. Allowable
        arguments are: nrzi or nrz

     rxbuffers
        this keyword specifies the number of receive buffers to allocate
        memory for. The argument is an integer, e.g. 8.

     txbuffers
        this keyword specifies the number of transmit buffers to
        allocate memory for. The argument is an integer, e.g. 8.

     bufsize
        this keyword specifies the size of the receive and transmit
        buffers. The arguments is in bytes and represents the total
        length of the frame, so it must also take into account the AX.25
        headers and not just the length of the data field. This keyword
        is optional and default to 384

     txdelay
        the KISS transmit delay value, the argument is an integer in mS.

     persist
        the KISS persist value, the argument is an integer.

     slot
        the KISS slot time value, the argument is an integer in mS.

     tail
        the KISS transmit tail value, the argument is an integer in mS.

     fulldup
        the KISS full duplex flag, the argument is an integer.  1==Full
        Duplex, 0==Half Duplex.

     wait
        the KISS wait value, the argument is an integer in mS.

     min
        the KISS min value, the argument is an integer in S.

     maxkey
        the KISS maximum keyup time, the argument is an integer in S.

     idle
        the KISS idle timer value, the argument is an integer in S.

     maxdef
        the KISS maxdef value, the argument is an integer.

     group
        the KISS group value, the argument is an integer.

     txoff
        the KISS txoff value, the argument is an integer in mS.

     softdcd
        the KISS softdcd value, the argument is an integer.

     slip
        the KISS slip flag, the argument is an integer.


  6.1.7.4.  Using the driver.

  To use the driver you simply treat the /dev/scc* devices just as you
  would a serial tty device with a KISS TNC connected to it. For
  example, to configure Linux Kernel networking to use your SCC card you
  could use something like:


       # kissattach -s 4800 /dev/scc0 VK2KTJ



  You can also use NOS to attach to it in precisely the same way. From
  JNOS for example you would use something like:



       attach asy scc0 0 ax25 scc0 256 256 4800



  6.1.7.5.  The sccstat  and sccparam  tools.

  To assist in the diagnosis of problems you can use the sccstat program
  to display the current configuration of an SCC device. To use it try:



       # sccstat /dev/scc0



  you will displayed a very large amount of information relating to the
  configuration and health of the /dev/scc0 SCC port.
  The sccparam command allows you to change or modify a configuration
  after you have booted. Its syntax is very similar to the NOS param
  command, so to set the txtail setting of a device to 100mS you would
  use:



       # sccparam /dev/scc0 txtail 0x8



  6.1.8.  Creating a BPQ ethernet device.

  Kernel Compile Options:



       General setup  --->
           [*] Networking support
       Network device support  --->
           [*] Network device support
           ...
           [*] Radio network interfaces
           [*] BPQ Ethernet driver for AX.25



  Linux supports BPQ Ethernet compatibility. This enables you to run the
  AX.25 protocol over your Ethernet LAN and to interwork your linux
  machine with other BPQ machines on the LAN.

  The BPQ network devices are named `bpq[0-9]'. The `bpq0' device is
  associated with the `eth0' device, the `bpq1' device with the `eth1'
  device etc.

  Configuration is quite straightforward. You firstly must have
  configured a standard Ethernet device. This means you will have
  compiled your kernel to support your Ethernet card and tested that
  this works. Refer to the Ethernet-HOWTO <Ethernet-HOWTO.html> for more
  information on how to do this.

  To configure the BPQ support you need to configure the Ethernet device
  with an AX.25 callsign. The following command will do this for you:



       # /sbin/ifconfig bpq0 hw ax25 vk2ktj-14 up



  Again, remember that the callsign you specify should match the entry
  in the /etc/ax25/axports file that you wish to use for this port.


  6.1.9.  Configuring the BPQ Node to talk to the Linux AX.25 support.

  BPQ Ethernet normally uses a multicast address. The Linux
  implementation does not, and instead it uses the normal Ethernet
  broadcast address. The NET.CFG file for the BPQ ODI driver should
  therefore be modifified to look similar to this:

       LINK SUPPORT

               MAX STACKS 1
               MAX BOARDS 1

       LINK DRIVER E2000                    ; or other MLID to suit your card

               INT 10                       ;
               PORT 300                     ; to suit your card

               FRAME ETHERNET_II

               PROTOCOL BPQ 8FF ETHERNET_II ; required for BPQ - can change PID

       BPQPARAMS                            ; optional - only needed if you want
                                            ; to override the default target addr

               ETH_ADDR  FF:FF:FF:FF:FF:FF  ; Target address



  6.2.  Creating the /etc/ax25/axports  file.

  The /etc/ax25/axports is a simple text file that you create with a
  text editor. The format of the /etc/ax25/axports file is as follows:



       portname  callsign  baudrate  paclen  window  description



  where:


     portname
        is a text name that you will refer to the port by.

     callsign
        is the AX.25 callsign you want to assign to the port.

     baudrate
        is the speed at which you wish the port to communicate with your
        TNC.

     paclen
        is the maximum packet length you want to configure the port to
        use for AX.25 connected mode connections.

     window
        is the AX.25 window (K) parameter. This is the same as the
        MAXFRAME setting of many tnc's.

     description
        is a textual description of the port.

  In my case, mine looks like:



       radio    VK2KTJ-15       4800        256     2       4800bps 144.800 MHz
       ether    VK2KTJ-14       10000000    256     2       BPQ/ethernet device

  Remember, you must assign unique callsign/ssid to each AX.25 port you
  create.  Create one entry for each AX.25 device you want to use, this
  includes KISS, Baycom, SCC, PI, PT and SoundModem ports. Each entry
  here will describe exactly one AX.25 network device. The entries in
  this file are associated with the network devices by the
  callsign/ssid. This is at least one good reason for requiring unique
  callsign/ssid.


  6.3.  Configuring AX.25 routing.

  You may wish to configure default digipeaters paths for specific
  hosts.  This is useful for both normal AX.25 connections and also IP
  based connections.  The axparms command enables you to do this. Again,
  the man page offers a complete description, but a simple example might
  be:


       # /usr/sbin/axparms -route add radio VK2XLZ VK2SUT



  This command would set a digipeater entry for VK2XLZ via VK2SUT on the
  AX.25 port named radio.


  7.  Configuring an AX.25 interface for TCP/IP.

  It is very simple to configure an AX.25 port to carry TCP/IP.  If you
  have KISS interfaces then there are two methods for configuring an IP
  address. The kissattach command has an option that allows you to do
  specify an IP address. The more conventional method using the ifconfig
  command will work on all interface types.

  So, modifying the previous KISS example:


       # /usr/sbin/kissattach -i 44.136.8.5 -m 512 /dev/ttyS0 radio
       # /sbin/route add -net 44.136.8.0 netmask 255.255.255.0 ax0
       # /sbin/route add default ax0



  to create the AX.25 interface with an IP address of 44.136.8.5 and an
  MTU of 512 bytes. You should still use the ifconfig to configure the
  other parameters if necessary.

  If you have any other interface type then you use the ifconfig program
  to configure the ip address and netmask details for the port and add a
  route via the port, just as you would for any other TCP/IP interface.
  The following example is for a PI card device, but would work equally
  well for any other AX.25 network device:



       # /sbin/ifconfig pi0a 44.136.8.5 netmask 255.255.255.0 up
       # /sbin/ifconfig pi0a broadcast 44.136.8.255 mtu 512
       # /sbin/route add -net 44.136.8.0 netmask 255.255.255.0 pi0a
       # /sbin/route add default pi0a



  The commands listed above are typical of the sort of configuration
  many of you would be familiar with if you have used NOS or any of its
  derivatives or any other TCP/IP software. Note that the default route
  might not be required in your configuration if you have some other
  network device configured.

  To test it out, try a ping or a telnet to a local host.



       # ping -i 5 44.136.8.58



  Note the use of the `-i 5' arguments to ping to tell it to send pings
  every 5 seconds instead of its default of 1 second.


  8.  Configuring a NetRom port.

  The NetRom protocol relies on, and uses the AX.25 ports you have
  created.  The NetRom protocol rides on top of the AX.25 protocol. To
  configure NetRom on an AX.25 interface you must configure two files.
  One file describes the Netrom interfaces, and the other file describes
  which of the AX.25 ports will carry NetRom. You can configure multiple
  NetRom ports, each with its own callsign and alias, the same procedure
  applies for each.


  8.1.  Configuring /etc/ax25/nrports

  The first is the /etc/ax25/nrports file. This file describes the
  NetRom ports in much the same way as the /etc/ax25/axports file
  describes the AX.25 ports. Each NetRom device you wish to create must
  have an entry in the /etc/ax25/nrports file. Normally a Linux machine
  would have only one NetRom device configured that would use a number
  of the AX.25 ports defined. In some situations you might wish a
  special service such as a BBS to have a seperate NetRom alias and so
  you would create more than one.

  This file is formatted as follows:



       name callsign  alias  paclen   description



  Where:

     name
        is the text name that you wish to refer to the port by.

     callsign
        is the callsign that the NetRom traffic from this port will use.
        Note, this is not that address that users should connect to to
        get access to a node style interface. (The node program is
        covered later). This callsign/ssid should be unique and should
        not appear elsewhere in either of the /etc/ax25/axports or the
        /etc/ax25/nrports files.

     alias
        is the NetRom alias this port will have assigned to it.

     paclen
        is the maximum size of NetRom frames transmitted by this port.

     description
        is a free text description of the port.

  An example would look something like the following:



       netrom  VK2KTJ-9        LINUX   236     Linux Switch Port



  This example creates a NetRom port known to the rest of the NetRom
  network as `LINUX:VK2KTJ-9'.

  This file is used by programs such as the call program.


  8.2.  Configuring /etc/ax25/nrbroadcast

  The second file is the /etc/ax25/nrbroadcast file. This file may
  contain a number of entries. There would normally be one entry for
  each AX.25 port that you wish to allow NetRom traffic on.

  This file is formatted as follows:



       axport min_obs def_qual worst_qual verbose



  Where:

     axport
        is the port name obtained from the /etc/ax25/axports file. If
        you do not have an entry in /etc/ax25/nrbroadcasts for a port
        then this means that no NetRom routing will occur and any
        received NetRom broadcasts will be ignored for that port.

     min_obs
        is the minimum obselesence value for the port.

     def_qual
        is the default quality for the port.

     worst_qual
        is the worst quality value for the port, any routes under this
        quality will be ignored.

     verbose
        is a flag determining whether full NetRom routing broadcasts
        will occur from this port or only a routing broadcast
        advertising the node itself.

  An example would look something like the following:



       radio    1       200      100         1


  8.3.  Creating the NetRom Network device

  When you have the two configuration files completed you must create
  the NetRom device in much the same way as you did for the AX.25
  devices.  This time you use the nrattach command. The nrattach works
  in just the same way as the axattach command except that it creates
  NetRom network devices called `nr[0-9]'. Again, the first time you use
  the nrattach command it creates the `nr0' device, the second time it
  creates the `nr1' network devices etc. To create the network device
  for the NetRom port we've defined we would use:



       # nrattach netrom



  This command would start the NetRom device (nr0) named netrom
  configured with the details specified in the /etc/ax25/nrports file.


  8.4.  Starting the NetRom daemon

  The Linux kernel does all of the NetRom protocol and switching, but
  does not manage some functions. The NetRom daemon manages the NetRom
  routing tables and generates the NetRom routing broadcasts. You start
  NetRom daemon with the command:



       # /usr/sbin/netromd -i



  You should soon see the /proc/net/nr_neigh file filling up with
  information about your NetRom neighbours.

  Remember to put the /usr/sbin/netromd command in your rc files so that
  it is started automatically each time you reboot.


  8.5.  Configuring NetRom routing.


  You may wish to configure static NetRom routes for specific hosts.
  The nrparms command enables you to do this. Again, the man page offers
  a complete description, but a simple example might be:


       # /usr/sbin/nrparms -nodes VK2XLZ-10 + #MINTO 120 5 radio VK2SUT-9



  This command would set a NetRom route to #MINTO:VK2XLZ-10 via a
  neighbour VK2SUT-9 on my AX.25 port called `radio'.


  You can manually create entries for new neighbours using the nrparms
  command as well. For example:



  # /usr/sbin/nrparms -routes radio VK2SUT-9 + 120



  This command would create VK2SUT-9 as a NetRom neighbour with a
  quality of 120 and this will be locked and will not be deleted
  automatically.


  9.  Configuring a NetRom interface for TCP/IP.

  Configuring a NetRom interface for TCP/IP is almost identical to
  configuring an AX.25 interface for TCP/IP.

  Again you can either specify the ip address and mtu on the nrattach
  command line, or use the ifconfig and route commands, but you need to
  manually add arp entries for hosts you wish to route to because there
  is no mechanism available for your machine to learn what NetRom
  address it should use to reach a particular IP host.

  So, to create an nr0 device with an IP address of 44.136.8.5, an mtu
  of 512 and configured with the details from the /etc/ax25/nrports file
  for a NetRom port named netrom you would use:



       # /usr/sbin/nrattach -i 44.136.8.5 -m 512 netrom
       # route add 44.136.8.5 nr0



  or you could use something like the following commands manually:



       # /usr/sbin/nrattach netrom
       # ifconfig nr0 44.136.8.5 netmask 255.255.255.0 hw netrom VK2KTJ-9
       # route add 44.136.8.5 nr0



  Then for each IP host you wish to reach via NetRom you need to set
  route and arp entries. To reach a destination host with an IP address
  of 44.136.80.4 at NetRom address BBS:VK3BBS via a NetRom neighbour
  with callsign VK2SUT-0 you would use commands as follows:



       # route add 44.136.80.4 nr0
       # arp -t netrom -s 44.136.80.4 vk2sut-0
       # nrparms -nodes vk3bbs + BBS 120 6 sl0 vk2sut-0



  The `120' and `6' arguments to the nrparms command are the NetRom
  quality and obsolescence count values for the route.


  10.  Configuring a Rose port.

  The Rose packet layer protocol is similar to layer three of the X.25
  specification. The kernel based Rose support is a modified version of
  the FPAC Rose implementation
  <http://fpac.lmi.ecp.fr/f1oat/f1oat.html>.

  The Rose packet layer protocol protocol relies on, and uses the AX.25
  ports you have created. The Rose protocol rides on top of the AX.25
  protocol.  To configure Rose you must create a configuration file that
  describes the Rose ports you want. You can create multiple Rose ports
  if you wish, the same procedure applies for each.


  10.1.  Configuring /etc/ax25/rsports


  The file where you configure your Rose interfaces is the
  /etc/ax25/rsports file. This file describes the Rose port in much the
  same way as the /etc/ax25/axports file describes the AX.25 ports.

  This file is formatted as follows:



       name  addresss  description



  Where:

     name
        is the text name that you wish to refer to the port by.

     address
        is the 10 digit Rose address you wish to assign to this port.

     description
        is a free text description of the port.

  An example would look something like the following:



       rose  5050294760  Rose Port



  Note that Rose will use the default callsign/ssid configured on each
  AX.25 port unless you specify otherwise.

  To configure a seperate callsign/ssid for Rose to use on each port you
  use the rsparms command as follows:



       # /usr/sbin/rsprams -call VK2KTJ-10



  This example would make Linux listen for and use the callsign/ssid
  VK2KTJ-10 on all of the configured AX.25 ports for Rose calls.



  10.2.  Creating the Rose Network device.

  When you have created the /etc/ax25/rsports file you may create the
  Rose device in much the same way as you did for the AX.25 devices.
  This time you use the rsattach command. The rsattach command creates
  network devices named `rose[0-5]'. The first time you use the rsattach
  command it create the `rose0' device, the second time it creates the
  `rose1' device etc. For example:



       # rsattach rose



  This command would start the Rose device (rose0) configured with the
  details specified in the /etc/ax25/rsports file for the entry named
  `rose'.


  10.3.  Configuring Rose Routing

  The Rose protocol currently supports only static routing. The rsparms
  utility allows you to configure your Rose routing table under Linux.

  For example:


       # rsparms -nodes add 5050295502 radio vk2xlz



  would add a route to Rose node 5050295502 via an AX.25 port named
  `radio' in your /etc/ax25/axports file to a neighbour with the call-
  sign VK2XLZ.

  You may specify a route with a mask to capture a number of Rose
  destinations into a single routing entry. The syntax looks like:


       # rsparms -nodes add 5050295502/4 radio vk2xlz



  which would be identical to the previous example except that it would
  match any destination address that matched the first four digits sup-
  plied, in this case any address commencing with the digits 5050. An
  alternate form for this command is:


       # rsparms -nodes add 5050/4 radio vk2xlz



  which is probably the less ambiguous form.


  11.  Making AX.25/NetRom/Rose calls.

  Now that you have all of your AX.25, NetRom and Rose interfaces
  configured and active, you should be able to make test calls.

  The AX25 Utilities package includes a program called `call' which is a
  splitscreen terminal program for AX.25, NetRom and Rose.

  A simple AX.25 call would look like:


       /usr/bin/call radio VK2DAY via VK2SUT



  A simple NetRom call to a node with an alias of SUNBBS would look
  like:


       /usr/bin/call netrom SUNBBS



  A simple Rose call to HEARD at node 5050882960 would look like:


       /usr/bin/call rose HEARD 5050882960



  Note: you must tell call which port you wish to make the call on, as
  the same destination node might be reachable on any of the ports you
  have configured.

  The call program is a linemode terminal program for making AX.25
  calls. It recognises lines that start with `~' as command lines.  The
  `~.' command will close the connection.

  Please refer to the man page in /usr/man for more information.


  12.  Configuring Linux to accept Packet connections.

  Linux is a powerful operating system and offers a great deal of
  flexibility in how it is configured. With this flexibility comes a
  cost in configuring it to do what you want. When configuring your
  Linux machine to accept incoming AX.25, NetRom or Rose connections
  there are a number of questions you need to ask yourself. The most
  important of which is: "What do I want users to see when they
  connect?". People are developing neat little applications that may be
  used to provide services to callers, a simple example is the pms
  program included in the AX25 utilities, a more complex example is the
  node program also included in the AX25 utilities. Alternatively you
  might want to give users a login prompt so that they can make use of a
  shell account, or you might even have written your own program, such
  as a customised database or a game, that you want people to connect
  to. Whatever you choose, you must tell the AX.25 software about this
  so that it knows what software to run when it accepts an incoming
  AX.25 connection.

  The ax25d program is similar to the inetd program commonly used to
  accept incoming TCP/IP connections on unix machines. It sits and
  listens for incoming connections, when it detects one it goes away and
  checks a configuration file to determine what program to run and
  connect to that connection. Since this the standard tool for accepting
  incoming AX.25, NetRom and Rose connections I'll describe how to
  configure it.

  12.1.  Creating the /etc/ax25/ax25d.conf  file.

  This file is the configuration file for the ax25d AX.25 daemon which
  handles incoming AX.25, NetRom and Rose connections.

  The file is a little cryptic looking at first, but you'll soon
  discover it is very simple in practice, with a small trap for you to
  be wary of.

  The general format of the ax25d.conf file is as follows:



       # This is a comment and is ignored by the ax25d program.
       [port_name] || <port_name> || {port_name}
       <peer1>    window T1 T2 T3 idle N2 <mode> <uid> <cmd> <cmd-name> <arguments>
       <peer2>    window T1 T2 T3 idle N2 <mode> <uid> <cmd> <cmd-name> <arguments>
       parameters window T1 T2 T3 idle N2 <mode>
       <peer3>    window T1 T2 T3 idle N2 <mode> <uid> <cmd> <cmd-name> <arguments>
          ...
       default    window T1 T2 T3 idle N2 <mode> <uid> <cmd> <cmd-name> <arguments>



  Where:

     #  at the start of a line marks a comment and is completely ignored
        by the ax25d program.

     <port_name>
        is the name of the AX.25, NetRom or Rose port as specified in
        the /etc/ax25/axports, /etc/ax25/nrports and /etc/ax25/rsports
        files. The name of the port is surrounded by the `[]' brackets
        if it is an AX.25 port, the `<>' brackets if it is a NetRom
        port, or the `{}' brackets if it is a Rose port.  There is an
        alternate form for this field, and that is use prefix the port
        name with `callsign/ssid via' to indicate that you wish accept
        calls to the callsign/ssid via this interface. The example
        should more clearly illustrate this.

     <peer>
        is the callsign of the peer node that this particular
        configuration applies to. If you don't specify an SSID here then
        any SSID will match.

     window
        is the AX.25 Window parameter (K) or MAXFRAME parameter for this
        configuration.

     T1 is the Frame retransmission (T1) timer in half second units.

     T2 is the amount of time the AX.25 software will wait for another
        incoming frame before preparing a response in 1 second units.

     T3 is the amount of time of inactivity before the AX.25 software
        will disconnect the session in 1 second units.

     idle
        is the idle timer value in seconds.

     N2 is the number of consecutive retransmissions that will occur
        before the connection is closed.

     <mode>
        provides a mechanism for determining certain types of general
        permissions. The modes are enabled or disabled by supplying a
        combination of characters, each representing a permission. The
        characters may be in either upper or lower case and must be in a
        single block with no spaces.

        u/U
           UTMP                   - currently unsupported.

        v/V
           Validate call          - currently unsupported.

        q/Q
           Quiet                  - Don't log connection

        n/N
           check NetRom Neighbour - currently unsupported.

        d/D
           Disallow Digipeaters   - Connections must be direct, not
           digipeated.

        l/L
           Lockout                - Don't allow connection.

        */0
           marker                 - place marker, no mode set.

     <uid>
        is the userid that the program to be run to support the
        connection should be run as.

     <cmd>
        is the full pathname of the command to be run, with no arguments
        specified.

     <cmd-name>
        is the text that should appear in a ps as the command name
        running (normally the same as <cmd> except without the directory
        path information.

     <arguments>
        are the command line argument to be passed to the <:cmd> when it
        is run. You pass useful information into these arguments by use
        of the following tokens:

        %d Name of the port the connection was received on.

        %U AX.25 callsign of the connected party without the SSID, in
           uppercase.

        %u AX.25 callsign of the connected party without the SSID, in
           lowercase.

        %S AX.25 callsign of the connected party with the SSID, in
           uppercase.

        %s AX.25 callsign of the connected party with the SSID, in
           lowercase.

        %P AX.25 callsign of the remote node that the connection came in
           from without the SSID, in uppercase.

        %p AX.25 callsign of the remote node that the connection came in
           from without the SSID, in lowercase.


        %R AX.25 callsign of the remote node that the connection came in
           from with the SSID, in uppercase.

        %r AX.25 callsign of the remote node that the connection came in
           from with the SSID, in lowercase.

  You need one section in the above format for each AX.25, NetRom or
  Rose interface you want to accept incoming AX.25, NetRom or Rose
  connections on.

  There are two special lines in the paragraph, one starts with the
  string `parameters' and the other starts with the string `default'
  (yes there is a difference). These lines serve special functions.

  The `default' lines purpose should be obvious, this line acts as a
  catch-all, so that any incoming connection on the <interface_call>
  interface that doesn't have a specific rule will match the `default'
  rule. If you don't have a `default' rule, then any connections not
  matching any specific rule will be disconnected immediately without
  notice.

  The `parameters' line is a little more subtle, and here is the trap I
  mentioned earlier. In any of the fields for any definition for a peer
  you can use the `*' character to say `use the default value'. The
  `parameters' line is what sets those default values. The kernel
  software itself has some defaults which will be used if you don't
  specify any using the `parameters' entry. The trap is that the these
  defaults apply only to those rules below the `parameters' line, not to
  those above. You may have more than one `parameters' rule per
  interface definition, and in this way you may create groups of default
  configurations. It is important to note that the `parameters' rule
  does not allow you to set the `uid' or `command' fields.


  12.2.  A simple example ax25d.conf  file.

  Ok, an illustrative example:



  # ax25d.conf for VK2KTJ - 02/03/97
  # This configuration uses the AX.25 port defined earlier.

  # <peer> Win T1  T2  T3  idl N2 <mode> <uid> <exec> <argv[0]>[<args....>]

  [VK2KTJ-0 via radio]
  parameters 1    10  *  *  *   *   *
  VK2XLZ     *     *  *  *  *   *   *    root  /usr/sbin/axspawn axspawn %u +
  VK2DAY     *     *  *  *  *   *   *    root  /usr/sbin/axspawn axspawn %u +
  NOCALL     *     *  *  *  *   *   L
  default    1    10  5 100 180 5   *    root  /usr/sbin/pms pms -a -o vk2ktj

  [VK2KTJ-1 via radio]
  default    *     *    *   *   *   0    root /usr/sbin/node node

  <netrom>
  parameters 1    10  *  *  *   *   *
  NOCALL     *     *  *  *  *   *   L
  default    *     *  *  *  *   *   0        root /usr/sbin/node node

  {VK2KTJ-0 via rose}
  parameters 1    10  *  *  *   *   *
  VK2XLZ     *     *  *  *  *   *   *    root  /usr/sbin/axspawn axspawn %u +
  VK2DAY     *     *  *  *  *   *   *    root  /usr/sbin/axspawn axspawn %u +
  NOCALL     *     *  *  *  *   *   L
  default    1    10  5 100 180 5   *    root  /usr/sbin/pms pms -a -o vk2ktj

  {VK2KTJ-1 via rose}
  default    *     *    *   *   *   0    root /usr/sbin/node node radio



  This example says that anybody attempting to connect to the callsign
  `VK2KTJ-0' heard on the AX.25 port called `radio' will have the
  following rules applied:

  Anyone whose callsign is set to `NOCALL' should be locked out, note
  the use of mode `L'.

  The parameters line changes two parameters from the kernel defaults
  (Window and T1) and will run the /usr/sbin/axspawn program for them.
  Any copies of /usr/sbin/axspawn run this way will appear as axspawn in
  a ps listing for convenience. The next two lines provide definitions
  for two stations who will receive those permissions.

  The last line in the paragraph is the `catch all' definition that
  everybody else will get (including VK2XLZ and VK2DAY using any other
  SSID other than -1).  This definition sets all of the parameters
  implicitly and will cause the pms program to be run with a command
  line argument indicating that it is being run for an AX.25 connection,
  and that the owner callsign is VK2KTJ. (See the `Configuring the PMS'
  section below for more details).

  The next configuration accepts calls to VK2KTJ-1 via the radio port.
  It runs the node program for everybody that connects to it.

  The next configuration is a NetRom configuration, note the use of the
  greater-then and less-than braces instead of the square brackets.
  These denote a NetRom configuration. This configuration is simpler, it
  simply says that anyone connecting to our NetRom port called `netrom'
  will have the node program run for them, unless they have a callsign
  of `NOCALL' in which case they will be locked out.

  The last two configurations are for incoming Rose connections. The
  first for people who have placed calls to `vk2ktj-0' and the second
  for `VK2KTJ-1 at the our Rose node address. These work precisely the
  same way. Not the use of the curly braces to distinguish the port as a
  Rose port.

  This example is a contrived one but I think it illustrates clearly the
  important features of the syntax of the configuration file. The
  configuration file is explained fully in the ax25d.conf man page. A
  more detailed example is included in the ax25-utils package that might
  be useful to you too.


  12.3.  Starting ax25d

  When you have the two configuration files completed you start ax25d
  with the command:



       # /usr/sbin/ax25d



  When this is run people should be able to make AX.25 connections to
  your Linux machine. Remember to put the ax25d command in your rc files
  so that it is started automatically when you reboot each time.


  13.  Configuring the node  software.

  The node software was developed by Tomi Manninen
  <tomi.manninen@hut.fi> and was based on the original PMS program.  It
  provides a fairly complete and flexible node capability that is easily
  configured. It allows users once they are connected to make Telnet,
  NetRom, Rose, and AX.25 connections out and to obtain various sorts of
  information such as Finger, Nodes and Heard lists etc. You can
  configure the node to execute any Linux command you wish fairly
  simply.

  The node would normally be invoked from the ax25d program although it
  is also capable of being invoked from the TCP/IP inetd program to
  allow users to telnet to your machine and obtain access to it, or by
  running it from the command line.


  13.1.  Creating the /etc/ax25/node.conf  file.

  The node.conf file is where the main configuration of the node takes
  place. It is a simple text file and its format is as follows:



  # /etc/ax25/node.conf
  # configuration file for the node(8) program.
  #
  # Lines beginning with '#' are comments and are ignored.

  # Hostname
  # Specifies the hostname of the node machine
  hostname        radio.gw.vk2ktj.ampr.org

  # Local Network
  # allows you to specify what is consider 'local' for the
  # purposes of permission checking using nodes.perms.
  localnet        44.136.8.96/29

  # Hide Ports
  # If specified allows you to make ports invisible to users. The
  # listed ports will not be listed by the (P)orts command.
  hiddenports     rose netrom

  # Node Identification.
  # this will appear in the node prompt
  NodeId          LINUX:VK2KTJ-9

  # NetRom port
  # This is the name of the netrom port that will be used for
  # outgoing NetRom connections from the node.
  NrPort          netrom

  # Node Idle Timeout
  # Specifies the idle time for connections to this node in seconds.
  idletimout      1800

  # Connection Idle Timeout
  # Specifies the idle timer for connections made via this node in
  # seconds.
  conntimeout     1800

  # Reconnect
  # Specifies whether users should be reconnected to the node
  # when their remote connections disconnect, or whether they
  # should be disconnected complete.
  reconnect       on

  # Command Aliases
  # Provide a way of making complex node commands simple.
  alias           CONV    "telnet vk1xwt.ampr.org 3600"
  alias           BBS     "connect radio vk2xsb"

  # Externam Command Aliases
  # Provide a means of executing external commands under the node.
  # extcmd <cmdname> <flag> <userid> <command>
  # Flag == 1 is the only implemented function.
  # <command> is formatted as per ax25d.conf
  extcmd          PMS     1       root    /usr/sbin/pms pms -u %U -o VK2KTJ

  # Logging
  # Set logging to the system log. 3 is the noisiest, 0 is disabled.
  loglevel        3

  # The escape character
  # 20 = (Control-T)
  EscapeChar      20



  13.2.  Creating the /etc/ax25/node.perms  file.

  The node allows you to assign permissions to users. These permissions
  allow you to determine which users should be allowed to make use of
  options such as the (T)elnet, and (C)onnect commands, for example, and
  which shouldn't. The node.perms file is where this information is
  stored and contains five key fields. For all fields an asterisk `*'
  character matches anything. This is useful for building default rules.


     user
        The first field is the callsign or user to which the permissions
        should apply.  Any SSID value is ignored, so you should just
        place the base callsign here.

     method
        Each protocol or access method is also given permissions. For
        example you might allow users who have connected via AX.25 or
        NetRom to use the (C)onnect option, but prevent others, such as
        those who are telnet connected from a non-local node from having
        access to it. The second field therefore allows you to select
        which access method this permissions rule should apply to.  The
        access methods allowed are:


          method  description
          ------  -----------------------------------------------------------
          ampr    User is telnet connected from an amprnet address (44.0.0.0)
          ax25    User connected by AX.25
          host    User started node from command line
          inet    user is telnet connected from a non-loca, non-ampr address.
          local   User is telnet connected from a 'local' host
          netrom  User connected by NetRom
          rose    User connected by Rose
          *       User connected by any means.



     port
        For AX.25 users you can control permissions on a port by port
        basis too if you choose. This allows you to determine what AX.25
        are allowed to do based on which of your ports they have
        connected to. The third field contains the port name if you are
        using this facility. This is useful only for AX.25 connections.

     password
        You may optionally configure the node so that it prompts users
        to enter a password when they connect. This might be useful to
        help protect specially configured users who have high authority
        levels. If the fourth field is set then its value will be the
        password that will be accepted.

     permissions
        The permissions field is the final field in each entry in the
        file.  The permissions field is coded as a bit field, with each
        facility having a bit value which if set allows the option to be
        used and if not set prevents the facility being used. The list
        of controllable facilities and their corresponding bit values
        are:



     value   description
     -----   -------------------------------------------------
      1      Login allowed.
      2      AX25 (C)onnects allowed.
      4      NetRom (C)onnects allowed.
      8      (T)elnet to local hosts allowed.
      16     (T)elnet to amprnet (44.0.0.0) hosts allowed.
      32     (T)elnet to non-local, non-amprnet hosts allowed.
      64     Hidden ports allowed for AX.25 (C)onnects.
      128    Rose (C)onnects allowed.



     To code the permissions value for a rule, simply take each of the
     permissions you want that user to have and add their values
     together. The resulting number is what you place in field five.

  A sample nodes.perms might look like:



       # /etc/ax25/node.perms
       #
       # The node operator is VK2KTJ, has a password of 'secret' and
       # is allowed all permissions by all connection methods
       vk2ktj  *       *       secret  255

       # The following users are banned from connecting
       NOCALL  *       *       *       0
       PK232   *       *       *       0
       PMS     *       *       *       0

       # INET users are banned from connecting.
       *       inet    *       *       0

       # AX.25, NetRom, Local, Host and AMPR users may (C)onnect and (T)elnet
       # to local and ampr hosts but not to other IP addresses.
       *       ax25    *       *       159
       *       netrom  *       *       159
       *       local   *       *       159
       *       host    *       *       159
       *       ampr    *       *       159



  13.3.  Configuring node  to run from ax25d

  The node program would normally be run by the ax25d program.  To do
  this you need to add appropriate rules to the /etc/ax25/ax25d.conf
  file. In my configuration I wanted users to have a choice of either
  connecting to the node or connecting to other services. ax25d allows
  you to do this by cleverly creating creating port aliases. For
  example, given the ax25d configuration presented above, I want to
  configure node so that all users who connect to VK2KTJ-1 are given the
  node. To do this I add the following to my /etc/ax25/ax25d.conf file:


       [vk2ktj-1 via radio]
       default    *     *    *   *   *   0    root /usr/sbin/node node



  This says that the Linux kernel code will answer any connection
  requests for the callsign `VK2KTJ-1' heard on the AX.25 port named
  `radio', and will cause the node program to be run.


  13.4.  Configuring node  to run from inetd

  If you want users to be able to telnet a port on your machine and
  obtain access to the node you can go this fairly easily. The first
  thing to decide is what port users should connect to. In this example
  I've arbitrarily chosen port 4000, though Tomi gives details on how
  you could replace the normal telnet daemon with the node in his
  documentation.

  You need to modify two files.

  To /etc/services you should add:


       node    3694/tcp        #OH2BNS's node software



  and to /etc/inetd.conf you should add:


       node    stream  tcp     nowait  root    /usr/sbin/node node



  When this is done, and you have restarted the inetd program any user
  who telnet connects to port 3694 of your machine will be prompted to
  login and if configured, their password and then they will be con-
  nected to the node.


  14.  Configuring axspawn .

  The axspawn program is a simple program that allows AX.25 stations who
  connect to be logged in to your machine. It may be invoked from the
  ax25d program as described above in a manner similar to the node
  program. To allow a user to log in to your machine you should add a
  line similar to the following into your /etc/ax25/ax25d.conf file:


       default * * * * * 1 root /usr/sbin/axspawn axspawn %u



  If the line ends in the + character then the connecting user must hit
  return before they will be allowed to login. The default is to not
  wait.  Any individual host configurations that follow this line will
  have the axspawn program run when they connect. When axspawn is run it
  first checks that the command line argument it is supplied is a legal
  callsign, strips the SSID, then it checks that /etc/passwd file to see
  if that user has an account configured. If there is an account, and
  the password is either "" (null) or + then the user is logged in, if
  there is anything in the password field the user is prompted to enter
  a password. If there is not an existing account in the /etc/passwd
  file then axspawn may be configured to automatically create one.



  14.1.  Creating the /etc/ax25/axspawn.conf  file.

  You can alter the behaviour of axspawn in various ways by use of the
  /etc/ax25/axspawn.conf file. This file is formatted as follows:


       # /etc/ax25/axspawn.conf
       #
       # allow automatic creation of user accounts
       create    yes
       #
       # guest user if above is 'no' or everything else fails. Disable with "no"
       guest     no
       #
       # group id or name for autoaccount
       group     ax25
       #
       # first user id to use
       first_uid 2001
       #
       # maximum user id
       max_uid   3000
       #
       # where to add the home directory for the new users
       home      /home/ax25
       #
       # user shell
       shell     /bin/bash
       #
       # bind user id to callsign for outgoing connects.
       associate yes



  The eight configurable characteristics of axspawn are as follows:


     #  indicates a comment.

     create
        if this field is set to yes then axspawn will attempt to
        automatically create a user account for any user who connects
        and does not already have an entry in the /etc/passwd file.

     guest
        this field names the login name of the account that will be used
        for people who connect who do not already have accounts if
        create is set to no. This is usually ax25 or guest.

     group
        this field names the group name that will be used for any users
        who connect and do not already have an entry in the /etc/passwd
        file.

     first_uid
        this is the number of the first userid that will be
        automatically created for new users.

     max_uid
        this is the maximum number that will be used for the userid of
        new users.

     home
        this is the home (login) directory of new users.

     shell
        this is the login shell of any new users.

     associate
        this flag indicates whether outgoing AX.25 connections made by
        this user after they login will use their own callsign, or your
        stations callsign.


  15.  Configuring the pms

  The pms program is an implementation of a simple personal message
  system. It was originally written by Alan Cox. Dave Brown, N2RJT,
  <dcb@vectorbd.com> has taken on further development of it.  At present
  it is still very simple, supporting only the ability to send mail to
  the owner of the system and to obtain some limited system information
  but Dave is working to expand its capability to make it more useful.

  After that is done there are a couple of simple files that you should
  create that give users some information about the system and then you
  need to add appropriate entries into the ax25d.conf file so that
  connected users are presented with the PMS.



  15.1.  Create the /etc/ax25/pms.motd  file.

  The /etc/ax25/pms.motd file contains the `message of the day' that
  users will be presented with after they connect and receive the usual
  BBS id header. The file is a simple text file, any text you include in
  this file will be sent to users.


  15.2.  Create the /etc/ax25/pms.info  file.

  The /etc/ax25/pms.info file is also a simple text file in which you
  would put more detailed information about your station or
  configuration.  This file is presented to users in response to their
  issuing of the Info command from the PMS> prompt.


  15.3.  Associate AX.25 callsigns with system users.

  When a connected user sends mail to an AX.25 callsign, the pms expects
  that callsign to be mapped, or associated with a real system user on
  your machine. This is described in a section of its own.


  15.4.  Add the PMS to the /etc/ax25/ax25d.conf  file.

  Adding the pms to your ax25d.conf file is very simple.  There is one
  small thing you need to think about though. Dave has added command
  line arguments to the PMS to allow it to handle a number of different
  text end-of-line conventions. AX.25 and NetRom by convention expect
  the end-of-line to be carriage return, linefeed while the standard
  unix end-of-line is just newline. So, for example, if you wanted to
  add an entry that meant that the default action for a connection
  received on an AX.25 port is to start the PMS then you would add a
  line that looked something like:



       default  1  10 5 100 5   0    root  /usr/sbin/pms pms -a -o vk2ktj



  This simply runs the pms program, telling it that it is an AX.25
  connection it is connected to and that the PMS owner is vk2ktj.  Check
  the man page for what you should specify for other connection methods.


  15.5.  Test the PMS.

  To test the PMS, you can try the following command from the command
  line:

  # /usr/sbin/pms -u vk2ktj -o vk2ktj


  Substitute your own callsign for mine and this will run the pms,
  telling it that it is to use the unix end-of-line convention, and that
  user logging in is vk2ktj. You can do all the things connected users
  can.

  Additionally you might try getting some other node to connect to you
  to confirm that your ax25d.conf configuration works.


  16.  Configuring the user_call  programs.

  The `user_call' programs are really called: ax25_call and netrom_call.
  They are very simple programs designed to be called from ax25d to
  automate network connections to remote hosts. They may of course be
  called from a number of other places such as shell scripts or other
  daemons such as the node program.

  They are like a very simple call program. They don't do any meddling
  with the data at all, so the end of line handling you'll have to worry
  about yourself.

  Let's start with an example of how you might use them. Imagine you
  have a small network at home and that you have one linux machine
  acting as your Linux radio gateway and another machine, lets say a BPQ
  node connected to it via an ethernet connection.

  Normally if you wanted radio users to be able to connect to the BPQ
  node they would either have to digipeat through your linux node, or
  connect to the node program on your linux node and then connect from
  it.  The ax25_call program can simplify this if it is called from the
  ax25d program.

  Imagine the BPQ node has the callsign VK2KTJ-9 and that the linux
  machine has the AX.25/ethernet port named `bpq'. Let us also imagine
  the Linux gateway machine has a radio port called `radio'.

  An entry in the /etc/ax25/ax25d.conf that looked like:


       [VK2KTJ-1 via radio]
       default    * * * *   *   *  *
                       root /usr/sbin/ax25_call ax25_call bpq %u vk2ktj-9



  would enable users to connect direct to `VK2KTJ-1' which would actu-
  ally be the Linux ax25d daemon and then be automatically switched to
  an AX.25 connection to `VK2KTJ-9' via the `bpq' interface.

  There are all sorts of other possible configurations that you might
  try.  The `netrom_call' and `rose_call' utilities work in similar
  ways. One amateur has used this utility to make connections to a
  remote BBS easier. Normally the users would have to manually enter a
  long connection string to make the call so he created an entry that
  made the BBS appear as though it were on the local network by having
  his ax25d proxy the connection to the remote machine.


  17.  Configuring the Rose Uplink and Downlink commands

  If you are familiar with the ROM based Rose implementation you will be
  familiar with the method by which AX.25 users make calls across a Rose
  network. If a users local Rose node has the callsign VK2KTJ-5 and the
  AX.25 user wants to connect to VK5XXX at remote Rose node 5050882960
  then they would issue the command:



       c vk5xxx v vk2ktj-5 5050 882960



  At the remote node, VK5XXX would see an incoming connection with the
  local AX.25 users callsign and being digipeated via the remote Rose
  nodes callsign.

  The Linux Rose implementation does not support this capability in the
  kernel, but there are two application programs called rsuplnk and
  rsdwnlnk which perform this function.


  17.1.  Configuring a Rose downlink

  To configure your Linux machine to accept a Rose connection and
  establish an AX.25 connection to any destination callsign that is not
  being listened for on your machine you need to add an entry to your
  /etc/ax25/ax25d.conf file. Normally you would configure this entry to
  be the default behaviour for incoming Rose connections. For example
  you might have Rose listeners operating for destinations like NODE-0
  or HEARD-0 that you wish to handle locally, but for all other
  destination calls you may want to pass them to the rsdwnlink command
  and assume they are AX.25 users.

  A typical configuration would look like:



       #
       {* via rose}
       NOCALL   * * * * * *  L
       default  * * * * * *  - root  /usr/sbin/rsdwnlnk rsdwnlnk 4800 vk2ktj-5
       #



  With this configuration any user who established a Rose connection to
  your Linux nodes address with a destination call of something that you
  were not specifically listening for would be converted into an AX.25
  connection on the AX.25 port named 4800 with a digipeater path of
  VK2KTJ-5.


  17.2.  Configuring a Rose uplink

  To configure your Linux machine to accept AX.25 connections in the
  same way that a ROM Rose node would you must add an entry into your
  /etc/ax25/ax25d.conf file that looks similar to the following:



       #
       [VK2KTJ-5* via 4800]
       NOCALL   * * * * * *  L
       default  * * * * * *  - root  /usr/sbin/rsuplnk rsuplnk rose
       #



  Note the special syntax for the local callsign. The `*' character
  indicates that the application should be invoked if the callsign is
  heard in the digipeater path of a connection.

  This configuration would allow an AX.25 user to establish Rose calls
  using the example connect sequence presented in the introduction.
  Anybody attempting to digipeat via VK2KTJ-5 on the AX.25 port named
  4800 would be handled by the rsuplnk command.


  18.  Associating AX.25 callsigns with Linux users.

  There are a number of situations where it is highly desirable to
  associate a callsign with a linux user account. One example might be
  where a number of amateur radio operators share the same linux machine
  and wish to use their own callsign when making calls. Another is the
  case of PMS users wanting to talk to a particular user on your
  machine.

  The AX.25 software provides a means of managing this association of
  linux user account names with callsigns. We've mentioned it once
  already in the PMS section, but I'm spelling it out here to be sure
  you don't miss it.

  You make the association with the axparms command. An example looks
  like:


       # axparms -assoc vk2ktj terry



  This command associates that AX.25 callsign vk2ktj with the user terry
  on the machine. So, for example, any mail for vk2ktj on the pms will
  be sent to Linux account terry.

  Remember to put these associations into your rc file so that they are
  available each time your reboot.

  Note you should never associate a callsign with the root account as
  this can cause configuration problems in other programs.


  19.  The /proc/  file system entries.

  The /proc filesystem contains a number of files specifically related
  to the AX25 and NetRom kernel software. These files are normally used
  by the AX52 utilities, but they are plainly formatted so you may be
  interested in reading them. The format is fairly easily understood so
  I don't think much explanation will be necessary.


     /proc/net/arp
        contains the list of Address Resolution Protocol mappings of IP
        addresses to MAC layer protocol addresses. These can can AX.25,
        ethernet or some other MAC layer protocol.

     /proc/net/ax25
        contains a list of AX.25 sockets opened. These might be
        listening for a connection, or active sessions.

     /proc/net/ax25_bpqether
        contains the AX25 over ethernet BPQ style callsign mappings.

     /proc/net/ax25_calls
        contains the linux userid to callsign mappings set my the
        axparms -assoc command.

     /proc/net/ax25_route
        contains AX.25 digipeater path information.

     /proc/net/nr
        contains a list of NetRom sockets opened. These might be
        listening for a connection, or active sessions.

     /proc/net/nr_neigh
        contains information about the NetRom neighbours known to the
        NetRom software.

     /proc/net/nr_nodes
        contains information about the NetRom nodes known to the NetRom
        software.

     /proc/net/rose
        contains a list of Rose sockets opened. These might be listening
        for a connection, or active sessions.

     /proc/net/rose_nodes
        contains a mapping of Rose destinations to Rose neighbours.

     /proc/net/rose_neigh
        contains a list of known Rose neighbours.

     /proc/net/rose_routes
        contains a list of all established Rose connections.


  20.  AX.25, NetRom, Rose network programming.

  Probably the biggest advantage of using the kernel based
  implementations of the amateur packet radio protocols is the ease with
  which you can develop applications and programs to use them.

  While the subject of Unix Network Programming is outside the scope of
  this document I will describe the elementary details of how you can
  make use of the AX.25, NetRom and Rose protocols within your software.


  20.1.  The address families.

  Network programming for AX.25, NetRom and Rose is quite similar to
  programming for TCP/IP under Linux. The major differences being the
  address families used, and the address structures that need to be
  mangled into place.

  The address family names for AX.25, NetRom and Rose are AF_AX25,
  AF_NETROM and AF_ROSE respectively.

  20.2.  The header files.

  You must always include the `ax25.h' header file, and also the
  `netrom.h' or `rose.h' header files if you are dealing with those
  protocols. Simple top level skeletons would look something like the
  following:

  For AX.25:


       #include <ax25.h>
       int s, addrlen = sizeof(struct full_sockaddr_ax25);
       struct full_sockaddr_ax25 sockaddr;
       sockaddr.fsa_ax25.sax25_family = AF_AX25



  For NetRom:


       #include <ax25.h>
       #include <netrom.h>
       int s, addrlen = sizeof(struct full_sockaddr_ax25);
       struct full_sockaddr_ax25 sockaddr;
       sockaddr.fsa_ax25.sax25_family = AF_NETROM;



  For Rose:


       #include <ax25.h>
       #include <rose.h>
       int s, addrlen = sizeof(struct sockaddr_rose);
       struct sockaddr_rose sockaddr;
       sockaddr.srose_family = AF_ROSE;



  20.3.  Callsign mangling and examples.

  There are routines within the lib/ax25.a library built in the AX25
  utilities package that manage the callsign conversions for you. You
  can write your own of course if you wish.

  The user_call utilities are excellent examples from which to work. The
  source code for them is included in the AX25 utilities package.  If
  you spend a little time working with those you will soon see that
  ninety percent of the work is involved in just getting ready to open
  the socket. Actually making the connection is easy, the preparation
  takes time.

  The example are simple enough to not be very confusing. If you have
  any questions, you should feel to direct them to the linux-hams
  mailing list and someone there will be sure to help you.


  21.  Some sample configurations.

  Following are examples of the most common types of configurations.
  These are guides only as there are as many ways of configuring your
  network as there are networks to configure, but they may give you a
  start.


  21.1.  Small Ethernet LAN with Linux as a router to Radio LAN

  Many of you may have small local area networks at home and want to
  connect the machines on that network to your local radio LAN. This is
  the type of configuration I use at home. I arranged to have a suitable
  block of addresses allocated to me that I could capture in a single
  route for convenience and I use these on my Ethernet LAN. Your local
  IP coordinator will assist you in doing this if you want to try it as
  well. The addresses for the Ethernet LAN form a subset of the radio
  LAN addresses. The following configuration is the actual one for my
  linux router on my network at home:



                                                 .      .   .    .    . .
         ---                                .
          | Network       /---------\     .    Network
          | 44.136.8.96/29|         |    .     44.136.8/24        \ | /
          |               | Linux   |   .                          \|/
          |               |         |  .                            |
          |          eth0 | Router  |  .  /-----\    /----------\   |
          |---------------|         |-----| TNC |----| Radio    |---/
          |   44.136.8.97 |  and    |  .  \-----/    \----------/
          |               |         | sl0
          |               | Server  | 44.136.8.5
          |               |         |    .
          |               |         |     .
          |               \_________/       .
         ---                                     .      .   .    .    . .



  #!/bin/sh
  # /etc/rc.net
  # This configuration provides one KISS based AX.25 port and one
  # Ethernet device.

  echo "/etc/rc.net"
  echo "  Configuring:"

  echo -n "    loopback:"
  /sbin/ifconfig lo 127.0.0.1
  /sbin/route add 127.0.0.1
  echo " done."

  echo -n "    ethernet:"
  /sbin/ifconfig eth0 44.136.8.97 netmask 255.255.255.248 \
                  broadcast 44.136.8.103 up
  /sbin/route add 44.136.8.97 eth0
  /sbin/route add -net 44.136.8.96 netmask 255.255.255.248 eth0
  echo " done."

  echo -n "    AX.25: "
  kissattach -i 44.136.8.5 -m 512 /dev/ttyS1 4800
  ifconfig sl0 netmask 255.255.255.0 broadcast 44.136.8.255
  route add -host 44.136.8.5 sl0
  route add -net 44.136.8.0 window 1024 sl0

  echo -n "    Netrom: "
  nrattach -i 44.136.8.5 netrom

  echo "  Routing:"
  /sbin/route add default gw 44.136.8.68 window 1024 sl0
  echo "    default route."
  echo done.

  # end



  /etc/ax25/axports


       # name  callsign        speed   paclen  window  description
       4800    VK2KTJ-0        4800    256     2       144.800 MHz



  /etc/ax25/nrports


       # name  callsign        alias   paclen  description
       netrom  VK2KTJ-9        LINUX   235     Linux Switch Port



  /etc/ax25/nrbroadcast


       # ax25_name     min_obs def_qual        worst_qual      verbose
       4800            1       120             10              1



  o  You must have IP_FORWARDING enabled in your kernel.

  o  The AX.25 configuration files are pretty much those used as
     examples in the earlier sections, refer to those where necessary.

  o  I've chosen to use an IP address for my radio port that is not
     within my home network block. I needn't have done so, I could have
     easily used 44.136.8.97 for that port too.

  o  44.136.8.68 is my local IPIP encapsulated gateway and hence is
     where I point my default route.

  o  Each of the machines on my Ethernet network have a route:


       route add -net 44.0.0.0 netmask 255.0.0.0 \
               gw 44.136.8.97 window 512 mss 512 eth0



  The use of the mss and window parameters means that I can get optimum
  performance from both local Ethernet and radio based connections.

  o  I also run my smail, http, ftp and other daemons on the router
     machine so that it needs to be the only machine to provide others
     with facilities.

  o  The router machine is a lowly 386DX20 with a 20Mb harddrive and a
     very minimal linux configuration.


  21.2.  IPIP encapsulated gateway configuration.

  Linux is now very commonly used for TCP/IP encapsulated gateways
  around the world. The new tunnel driver supports multiple encapsulated
  routes and makes the older ipip daemon obsolete.

  A typical configuration would look similar to the following.



                                                 .      .   .    .    . .
         ---                                .
          | Network       /---------\     .    Network
          | 154.27.3/24   |         |    .     44.136.16/24       \ | /
          |               | Linux   |   .                          \|/
          |               |         |  .                            |
          |          eth0 | IPIP    |  .  /-----\    /----------\   |
       ---|---------------|         |-----| TNC |----| Radio    |---/
          |   154.27.3.20 | Gateway |  .  \-----/    \----------/
          |               |         | sl0
          |               |         | 44.136.16.1
          |               |         |    .
          |               |         |     .
          |               \_________/       .
         ---                                     .      .   .    .    . .



  The configuration files of interest are:



  # /etc/rc.net
  # This file is a simple configuration that provides one KISS AX.25
  # radio port, one Ethernet device, and utilises the kernel tunnel driver
  # to perform the IPIP encapsulation/decapsulation
  #
  echo "/etc/rc.net"
  echo "  Configuring:"
  #
  echo -n "    loopback:"
  /sbin/ifconfig lo 127.0.0.1
  /sbin/route add 127.0.0.1
  echo " done."
  #
  echo -n "    ethernet:"
  /sbin/ifconfig eth0 154.27.3.20 netmask 255.255.255.0 \
                  broadcast 154.27.3.255 up
  /sbin/route add 154.27.3.20 eth0
  /sbin/route add -net 154.27.3.0 netmask 255.255.255.0 eth0
  echo " done."
  #
  echo -n "    AX.25: "
  kissattach -i 44.136.16.1 -m 512 /dev/ttyS1 4800
  /sbin/ifconfig sl0 netmask 255.255.255.0 broadcast 44.136.16.255
  /sbin/route add -host 44.136.16.1 sl0
  /sbin/route add -net 44.136.16.0 netmask 255.255.255.0 window 1024 sl0
  #
  echo -n "    tunnel:"
  /sbin/ifconfig tunl0 44.136.16.1 mtu 512 up
  #
  echo done.
  #
  echo -n "Routing ... "
  source /etc/ipip.routes
  echo done.
  #
  # end.



  and:



       # /etc/ipip.routes
       # This file is generated using the munge script
       #
       /sbin/route add -net 44.134.8.0 netmask 255.255.255.0 tunl0 gw 134.43.26.1
       /sbin/route add -net 44.34.9.0 netmask 255.255.255.0 tunl0 gw 174.84.6.17
       /sbin/route add -net 44.13.28.0 netmask 255.255.255.0 tunl0 gw 212.37.126.3
          ...
          ...
          ...



  /etc/ax25/axports


       # name  callsign        speed   paclen  window  description
       4800    VK2KTJ-0        4800    256     2       144.800 MHz



  Some points to note here are:


  o  The new tunnel driver uses the gw field in the routing table in
     place of the pointopoint parameter to specify the address of the
     remote IPIP gateway. This is why it now supports multiple routes
     per interface.

  o  You can configure two network devices with the same address.  In
     this example both the sl0 and the tunl0 devices have been
     configured with the IP address of the radio port. This is done so
     that the remote gateway sees the correct address from your gateway
     in encapsulated datagrams sent to it.

  o  The route commands used to specify the encapsulated routes can be
     automatically generated by a modified version of the munge script.
     This is included below. The route commands would then be written to
     a separate file and read in using the bash source /etc/ipip.routes
     command (assuming you called the file with the routing commands
     /etc/ipip.routes) as illustrated. The source file must be in the
     NOS route command format.

  o  Note the use of the window argument on the route command. Setting
     this parameter to an appropriate value improves the performance of
     your radio link.


  The new tunnel-munge script:



  #!/bin/sh
  #
  # From: Ron Atkinson <n8fow@hamgate.cc.wayne.edu>
  #
  #  This script is basically the 'munge' script written by Bdale N3EUA
  #  for the IPIP daemon and is modified by Ron Atkinson N8FOW. It's
  #  purpose is to convert a KA9Q NOS format gateways route file
  #  (usually called 'encap.txt') into a Linux routing table format
  #  for the IP tunnel driver.
  #
  #        Usage: Gateway file on stdin, Linux route format file on stdout.
  #               eg.  tunnel-munge < encap.txt > ampr-routes
  #
  # NOTE: Before you use this script be sure to check or change the
  #       following items:
  #
  #     1) Change the 'Local routes' and 'Misc user routes' sections
  #        to routes that apply to your own area (remove mine please!)
  #     2) On the fgrep line be sure to change the IP address to YOUR
  #        gateway Internet address. Failure to do so will cause serious
  #        routing loops.
  #     3) The default interface name is 'tunl0'. Make sure this is
  #        correct for your system.

  echo "#"
  echo "# IP tunnel route table built by $LOGNAME on `date`"
  echo "# by tunnel-munge script v960307."
  echo "#"
  echo "# Local routes"
  echo "route add -net 44.xxx.xxx.xxx netmask 255.mmm.mmm.mmm dev sl0"
  echo "#"
  echo "# Misc user routes"
  echo "#"
  echo "# remote routes"

  fgrep encap | grep "^route" | grep -v " XXX.XXX.XXX.XXX" | \
  awk '{
          split($3, s, "/")
          split(s[1], n,".")
          if      (n[1] == "")        n[1]="0"
          if      (n[2] == "")        n[2]="0"
          if      (n[3] == "")        n[3]="0"
          if      (n[4] == "")        n[4]="0"
          if      (s[2] == "1")       mask="128.0.0.0"
          else if (s[2] == "2")       mask="192.0.0.0"
          else if (s[2] == "3")       mask="224.0.0.0"
          else if (s[2] == "4")       mask="240.0.0.0"
          else if (s[2] == "5")       mask="248.0.0.0"
          else if (s[2] == "6")       mask="252.0.0.0"
          else if (s[2] == "7")       mask="254.0.0.0"
          else if (s[2] == "8")       mask="255.0.0.0"
          else if (s[2] == "9")       mask="255.128.0.0"
          else if (s[2] == "10")      mask="255.192.0.0"
          else if (s[2] == "11")      mask="255.224.0.0"
          else if (s[2] == "12")      mask="255.240.0.0"
          else if (s[2] == "13")      mask="255.248.0.0"
          else if (s[2] == "14")      mask="255.252.0.0"
          else if (s[2] == "15")      mask="255.254.0.0"
          else if (s[2] == "16")      mask="255.255.0.0"
          else if (s[2] == "17")      mask="255.255.128.0"
          else if (s[2] == "18")      mask="255.255.192.0"
          else if (s[2] == "19")      mask="255.255.224.0"
          else if (s[2] == "20")      mask="255.255.240.0"
          else if (s[2] == "21")      mask="255.255.248.0"
          else if (s[2] == "22")      mask="255.255.252.0"
          else if (s[2] == "23")      mask="255.255.254.0"
          else if (s[2] == "24")      mask="255.255.255.0"
          else if (s[2] == "25")      mask="255.255.255.128"
          else if (s[2] == "26")      mask="255.255.255.192"
          else if (s[2] == "27")      mask="255.255.255.224"
          else if (s[2] == "28")      mask="255.255.255.240"
          else if (s[2] == "29")      mask="255.255.255.248"
          else if (s[2] == "30")      mask="255.255.255.252"
          else if (s[2] == "31")      mask="255.255.255.254"
          else                    mask="255.255.255.255"

  if (mask == "255.255.255.255")
          printf "route add -host %s.%s.%s.%s gw %s dev tunl0\n"\
                  ,n[1],n[2],n[3],n[4],$5
  else
          printf "route add -net %s.%s.%s.%s gw %s netmask %s dev tunl0\n"\
                  ,n[1],n[2],n[3],n[4],$5,mask
   }'

  echo "#"
  echo "# default the rest of amprnet via mirrorshades.ucsd.edu"
  echo "route add -net 44.0.0.0 gw 128.54.16.18 netmask 255.0.0.0 dev tunl0"
  echo "#"
  echo "# the end"



  21.3.  AXIP encapsulated gateway configuration

  Many Amateur Radio Internet gateways encapsulate AX.25, NetRom and
  Rose in addition to tcp/ip. Encapsulation of AX.25 frames within IP
  datagrams is described in RFC-1226 by Brian Kantor. Mike Westerhof
  wrote an implementation of an AX.25 encapsulation daemon for unix in
  1991. The ax25-utils package includes a marginally enhanced version of
  it for Linux.

  An AXIP encapsulation program accepts AX.25 frames at one end, looks
  at the destination AX.25 address to determine what IP address to send
  them to, encapsulates them in a tcp/ip datagram and then transmits
  them to the appropriate remote destination. It also accepts tcp/ip
  datagrams that contain AX.25 frames, unwraps them and processes them
  as if it had received them directly from an AX.25 port. To distinguish
  IP datagrams containing AX.25 frames from other IP datagrams which
  don't, AXIP datagrams are coded with a protocol id of 4 (or 94 which
  is now deprecated). This process is described in RFC-1226.

  The ax25ipd program included in the ax25-utils package presents itself
  as a program supporting a KISS interface across which you pass AX.25
  frames, and an interface into the tcp/ip protocols. It is configured
  with a single configuration file called /etc/ax25/ax25ipd.conf.


  21.3.1.  AXIP configuration options.

  The ax25ipd program has two major modes of operation. "digipeater"
  mode and "tnc" mode. In "tnc" mode the daemon is treated as though it
  were a kiss TNC, you pass KISS encapsulated frames to it and it will
  transmit them, this is the usual configuration. In "digipeater" mode,
  you treat the daemon as though it were an AX.25 digipeater. There are
  subtle differences between these modes.

  In the configuration file you configure "routes" or mappings between
  destination AX.25 callsigns and the IP addresses of the hosts that you
  want to send the AX.25 packets too. Each route has options which will
  be explained later.
  Other options that are configured here are

  the tty that the ax25ipd daemon will open and its speed (usually one
  end of a pipe)

  what callsign you want to use in "digipeater" mode

  beacon interval and text

  whether you want to encapsulate the AX.25 frames in IP datagrams or in
  UDP/IP datagrams. Nearly all AXIP gateways use IP encapsulation, but
  some gateways are behind firewalls that will not allow IP with the
  AXIP protocol id to pass and are forced to use UDP/IP. Whatever you
  choose must match what the tcp/ip host at the other end of the link is
  using.


  21.3.2.  A typical /etc/ax25/ax25ipd.conf  file.



  #
  # ax25ipd configuration file for station floyd.vk5xxx.ampr.org
  #
  # Select axip transport. 'ip' is what you want for compatibility
  # with most other gateways.
  #
  socket ip
  #
  # Set ax25ipd mode of operation. (digi or tnc)
  #
  mode tnc
  #
  # If you selected digi, you must define a callsign.  If you selected
  # tnc mode, the callsign is currently optional, but this may change
  # in the future! (2 calls if using dual port kiss)
  #
  #mycall vk5xxx-4
  #mycall2 vk5xxx-5
  #
  # In digi mode, you may use an alias. (2 for dual port)
  #
  #myalias svwdns
  #myalias2 svwdn2
  #
  # Send an ident every 540 seconds ...
  #
  #beacon after 540
  #btext ax25ip -- tncmode rob/vk5xxx -- Experimental AXIP gateway
  #
  # Serial port, or pipe connected to a kissattach in my case
  #
  device /dev/ttyq0
  #
  # Set the device speed
  #
  speed 9600
  #
  # loglevel 0 - no output
  # loglevel 1 - config info only
  # loglevel 2 - major events and errors
  # loglevel 3 - major events, errors, and AX25 frame trace
  # loglevel 4 - all events
  # log 0 for the moment, syslog not working yet ...
  #
  loglevel 2
  #
  # If we are in digi mode, we might have a real tnc here, so use param to
  # set the tnc parameters ...
  #
  #param 1 20
  #
  # Broadcast Address definition. Any of the addresses listed will be forwarded
  # to any of the routes flagged as broadcast capable routes.
  #
  broadcast QST-0 NODES-0
  #
  # ax.25 route definition, define as many as you need.
  # format is route (call/wildcard) (ip host at destination)
  # ssid of 0 routes all ssid's
  #
  # route <destcall> <destaddr> [flags]
  #
  # Valid flags are:
  #         b  - allow broadcasts to be transmitted via this route
  #         d  - this route is the default route
  #
  route vk2sut-0 44.136.8.68 b
  route vk5xxx 44.136.188.221 b
  route vk2abc 44.1.1.1
  #
  #



  21.3.3.  Running ax25ipd


     Create your /etc/ax25/axports entry:


          # /etc/ax25/axports
          #
          axip    VK2KTJ-13       9600    256     AXIP port
          #



     Run the kissattach command to create that port:


          /usr/sbin/kissattach /dev/ptyq0 axip



     Run the ax25ipd program:


          /usr/sbin/ax25ipd &



     Test the AXIP link:


          call axip vk5xxx



  21.3.4.  Some notes about the routes and route flags

  The "route" command is where you specify where you want your AX.25
  packets encapsulated and sent to. When the ax25ipd daemon receives a
  packet from its interface, it compares the destination callsign with
  each of the callsigns in its routing table. If if finds a match then
  the ax.25 packet is encapsulated in an IP datagram and then
  transmitted to the host at the specified IP address.

  There are two flags you can add to any of the route commands in the
  ax25ipd.conf file. The two flags are:

     b  traffic with a destination address matching any of those on the
        list defined by the "broadcast" keyword should be transmitted
        via this route.

     d  any packets not matching any route should be transmitted via
        this route.

  The broadcast flag is very useful, as it enables informations that is
  normally destined for all stations to a number of AXIP destinations.
  Normally axip routes are point-to-point and unable to handle
  'broadcast' packets.


  21.4.  Linking NOS and Linux using a pipe device

  Many people like to run some version of NOS under Linux because it has
  all of the features and facilities they are used to. Most of those
  people would also like to have the NOS running on their machine
  capable of talking to the Linux kernel so that they can offer some of
  the linux capabilities to radio users via NOS.

  Brandon S. Allbery, KF8NH, contributed the following information to
  explain how to interconnect the NOS running on a Linux machine with
  the kernel code using the Linux pipe device.

  Since both Linux and NOS support the slip protocol it is possible to
  link the two together by creating a slip link. You could do this by
  using two serial ports with a loopback cable between them, but this
  would be slow and costly. Linux provides a feature that many other
  Unix-like operating systems provide called `pipes'. These are special
  pseudo devices that look like a standard tty device to software but in
  fact loopback to another pipe device. To use these pipes the first
  program must open the master end of the pipe, and the open then the
  second program can open the slave end of the pipe. When both ends are
  open the programs can communicate with each other simply by writing
  characters to the pipes in the way they would if they were terminal
  devices.

  To use this feature to connect the Linux Kernel and a copy of NOS, or
  some other program you first must choose a pipe device to use. You can
  find one by looking in your /dev directory. The master end of the
  pipes are named: ptyq[1-f] and the slave end of the pipes are known
  as: ttyq[1-f]. Remember they come in pairs, so if you select
  /dev/ptyqf as your master end then you must use /dev/ttyqf as the
  slave end.

  Once you have chosen a pipe device pair to use you should allocate the
  master end to you linux kernel and the slave end to the NOS program,
  as the Linux kernel starts first and the master end of the pipe must
  be opened first.  You must also remember that your Linux kernel must
  have a different IP address to your NOS, so you will need to allocate
  a unique address for it if you haven't already.

  You configure the pipe just as if it were a serial device, so to
  create the slip link from your linux kernel you can use commands
  similar to the following:



       # /sbin/slattach -s 38400 -p slip /dev/ptyqf &
       # /sbin/ifconfig sl0 broadcast 44.255.255.255 pointopoint 44.70.248.67 /
               mtu 1536 44.70.4.88
       # /sbin/route add 44.70.248.67 sl0
       # /sbin/route add -net 44.0.0.0 netmask 255.0.0.0 gw 44.70.248.67



  In this example the Linux kernel has been given IP address 44.70.4.88
  and the NOS program is using IP address 44.70.248.67. The route
  command in the last line simply tells your linux kernel to route all
  datagrams for the amprnet via the slip link created by the slattach
  command. Normally you would put these commands into your
  /etc/rc.d/rc.inet2 file after all your other network configuration is
  complete so that the slip link is created automatically when you
  reboot.  Note: there is no advantage in using cslip instead of slip as
  it actually reduces performance because the link is only a virtual one
  and occurs fast enough that having to compress the headers first takes
  longer than transmitting the uncompressed datagram.

  To configure the NOS end of the link you could try the following:



       # you can call the interface anything you want; I use "linux" for convenience.
       attach asy ttyqf - slip linux 1024 1024 38400
       route addprivate 44.70.4.88 linux



  These commands will create a slip port named `linux' via the slave end
  of the pipe device pair to your linux kernel, and a route to it to
  make it work. When you have started NOS you should be able to ping and
  telnet to your NOS from your Linux machine and vice versa. If not,
  double check that you have made no mistakes especially that you have
  the addresses configured properly and have the pipe devices around the
  right way.



  22.  Where do I find more information about .... ?

  Since this document assumes you already have some experience with
  packet radio and that this might not be the case I've collected a set
  of references to other information that you might find useful.



  22.1.  Packet Radio

  You can get general information about Packet Radio from these sites:

  American Radio Relay League <http://www.arrl.org/>,

  Radio Amateur Teleprinter Society <http://www.rats.org/>

  Tucson Amateur Packet Radio Group <http://www.tapr.org/>



  22.2.  Protocol Documentation


  AX.25, NetRom - Jonathon Naylor has collated a variety of documents
  that relate to the packet radio protocols themselves. This
  documentation has been packaged up into ax25-doc-1.0.tar.gz
  <ftp://ftp.pspt.fi/pub/ham/linux/ax25/ax25-doc-1.0.tar.gz>



  22.3.  Hardware Documentation


  Information on the PI2 Card is provided by the Ottawa Packet Radio
  Group <http://hydra.carleton.ca/>.

  Information on Baycom hardware is available at the Baycom Web Page
  <http://www.baycom.de/>.


  23.  Discussion relating to Amateur Radio and Linux.

  There are various places that discussion relating to Amateur Radio and
  Linux take place. They take place in the comp.os.linux.* newsgroups,
  they also take place on the HAMS list on vger.rutgers.edu. Other
  places where they are held include the tcp-group mailing list at
  ucsd.edu (the home of amateur radio TCP/IP discussions), and you might
  also try the #linpeople channel on the linuxnet irc network.

  To join the Linux linux-hams channel on the mail list server, send
  mail to:


       Majordomo@vger.rutgers.edu



  with the line:


       subscribe linux-hams



  in the message body. The subject line is ignored.

  The linux-hams mailing list is archived at:

  zone.pspt.fi <http://zone.pspt.fi/archive/linux-hams/> and
  zone.oh7rba.ampr.org <http://zone.oh7rba.ampr.org/archive/linux-
  hams/>.  Please use the archives when you are first starting, because
  many common questions are answered there.


  To join the tcp-group send mail to:


       listserver@ucsd.edu



  with the line:


       subscribe tcp-group



  in the body of the text.

  Note: Please remember that the tcp-group is primarily for discussion
  of the use of advanced protocols, of which TCP/IP is one, in Amateur
  Radio. Linux specific questions should not ordinarily go there.


  24.  Acknowledgements.

  The following people have contributed to this document in one way or
  another, knowingly or unknowingly. In no particular order (as I find
  them): Jonathon Naylor, Thomas Sailer, Joerg Reuter, Ron Atkinson,
  Alan Cox, Craig Small, John Tanner, Brandon Allbery, Hans Alblas,
  Klaus Kudielka, Carl Makin.


  25.  Copyright.

  The AX25-HOWTO, information on how to install and configure some of
  the more important packages providing AX25 support for Linux.
  Copyright (c) 1996 Terry Dawson.

  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation; either version 2 of the License, or (at
  your option) any later version.

  This program is distributed in the hope that it will be useful, but
  WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
  General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with this program; if not, write to the:

  Free Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139,
  USA.



  Linux Access HOWTO
  Michael De La Rue,  <access-howto@ed.ac.uk>
  v2.11, 28 March 1997

  The Linux Access HOWTO covers the use of adaptive technology with
  Linux, In particular, using adaptive technology to make Linux accessi-
  ble to those who could not use it otherwise.  It also covers areas
  where Linux can be used within more general adaptive technology solu-
  tions.
  ______________________________________________________________________

  Table of Contents



  1. Introduction

     1.1 Distribution Policy

  2. Comparing Linux with other Operating Systems

     2.1 General Comparison
     2.2 Availability of Adaptive Technology
     2.3 Inherent Usability

  3. Visually Impaired

     3.1 Seeing the Screen with Low Vision
        3.1.1 SVGATextMode
        3.1.2 X Window System
           3.1.2.1 Different Screen Resolutions
           3.1.2.2 Screen Magnification
           3.1.2.3 Change Screen Font
           3.1.2.4 Cross Hair Cursors etc..
        3.1.3 Audio
        3.1.4 Producing Large Print
           3.1.4.1 LaTeX / TeX
        3.1.5 Outputting Large Text
     3.2 Aids for Those Who Can't Use Visual Output
        3.2.1 Braille Terminals
        3.2.2 Speech Synthesis
        3.2.3 Handling Console Output
        3.2.4 Optical Character Recognition
     3.3 Beginning to Learn Linux
     3.4 Braille Embossing

  4. Hearing Problems

     4.1 Visual Bells

  5. Physical Problems

     5.1 Unable to Use a Mouse/Pointer
        5.1.1 Unable to Use a Keyboard
           5.1.1.1 Other Input Hardware (X Windows System only)
        5.1.2 Controlling Physical Hardware From Linux
     5.2 Speech Recognition
     5.3 Making the Keyboard Behave
        5.3.1 X Window System.
        5.3.2 Getting Rid of Auto Repeat
        5.3.3 Macros / Much input, few key presses
        5.3.4 Sticky Keys

  6. General Programming Issues

     6.1 Try to Make it Easy to Provide Multiple Interfaces
     6.2 Make software configurable.
     6.3 Test the Software on Users.
     6.4 Make Output Distinct
     6.5 Licenses

  7. Other Information

     7.1 Linux Documentation
        7.1.1 The Linux Info Sheet
        7.1.2 The Linux Meta FAQ
        7.1.3 The Linux Software Map
        7.1.4 The Linux HOWTO documents
        7.1.5 The Linux FAQ
     7.2 Mailing Lists
        7.2.1 The Linux Access List
        7.2.2 The Linux Blind List
     7.3 WWW References
     7.4 Suppliers
     7.5 Manufacturers
        7.5.1 Alphavision
           7.5.1.1 Linux Supported Alphavision AT Products
        7.5.2 Blazie Engineering
           7.5.2.1 Blazie AT Products
        7.5.3 Digital Equipment Corporation
           7.5.3.1 Linux Supported DEC AT Products
        7.5.4 Kommunikations-Technik Stolper GmbH
           7.5.4.1 Linux Supported KTG AT Products

  8. Software Packages

     8.1 Emacspeak
     8.2 BRLTTY
     8.3 Screen
     8.4 Rsynth
     8.5 xocr
     8.6 xzoom
     8.7 NFBtrans
        8.7.1 Compiling NFBtrans on Linux
     8.8 UnWindows
        8.8.1 dynamag
        8.8.2 coloreyes
        8.8.3 border
        8.8.4 un-twm

  9. Hardware

     9.1 Braille terminals driven from Screen Memory
        9.1.1 Braillex
        9.1.2 Brailloterm
        9.1.3 Patching the Kernel for Braillex and Brailloterm
     9.2 Software Driven Braille Terminals
        9.2.1 Tieman B.V.
           9.2.1.1 CombiBraille
        9.2.2 Alva B.V.
        9.2.3 Telesensory Systems Inc. displays
           9.2.3.1 Powerbraille
           9.2.3.2 Navigator
        9.2.4 Braille Lite
     9.3 Speech Synthesisers
        9.3.1 DECTalk Express
        9.3.2 Accent SA
        9.3.3 SPO256-AL2 Speak and Spell chip.

  10. Acknowledgements



  ______________________________________________________________________

  1.  Introduction

  The aim of this document is to serve as an introduction to the
  technologies which are available to make Linux usable by people who,
  through some disability would otherwise have problems with it.  In
  other words the target groups of the technologies are, the blind, the
  partially sighted, deaf and the physically disabled.  As any other
  technologies or pieces of information are discovered they will be
  added.



  The information here not just for these people (although that is
  probably the main aim) but also to allow developers of Linux to become
  aware of the difficulties involved here.  Possibly the biggest problem
  is that, right now, very few of the developers of Linux are aware of
  the issues and various simple ways to make life simpler for
  implementors of these systems.  This has, however, changed noticeably
  since the introduction of this document, and at least to a small
  extent because of this document, but also to a large extent due to the
  work of some dedicated developers, many of whom are mentioned in the
  document's Acknowledgements.


  Please send any comments or extra information or offers of assistance
  to <access-howto@ed.ac.uk> This address might become a mailing list in
  future, or be automatically handed over to a future maintainer of the
  HOWTO, so please don't use it for personal email.

  I don't have time to follow developments in all areas.  I probably
  won't even read a mail until I have time to update this document.
  It's still gratefully received.  If a mail is sent to the blind-list
  or the access-list, I will eventually read it and put any useful
  information into the document.  Otherwise, please send a copy of
  anything interesting to the above email address.


  Normal mail can be sent to



       Linux Access HOWTO
       23 Kingsborough Gardens
       Glasgow G12 9NH
       Scotland
       U.K.



  And will gradually make its way round the world to me.  Email will be
  faster by weeks.



  I can be personally contacted using <miked@ed.ac.uk>.  Since I use
  mail filtering on all mail I receive, please use the other address
  except for personal email.  This is most likely to lead to an
  appropriate response.


  1.1.  Distribution Policy


       The ACCESS-HOWTO is copyrighted (c) 1996 Michael De La Rue



       The ACCESS-HOWTO may be distributed, at your choice, under either the
       terms of the GNU Public License version 2 or later or the standard
       Linux Documentation project terms.  These licenses should be available
       from where you got this document.  Please note that since the LDP
       terms don't allow modification (other than translation), modified ver-
       sions can be assumed to be distributed under the GPL.



  2.  Comparing Linux with other Operating Systems

  2.1.  General Comparison

  The best place to find out about this is in such documents as the
  `Linux Info Sheet', `Linux Meta FAQ' and `Linux FAQ' (see ``Linux
  Documentation'').  Major reasons for a visually impaired person to use
  Linux would include it's inbuilt networking which gives full access to
  the Internet.  More generally, users are attracted by the full
  development environment included.  Also, unlike most other modern GUI
  environments, the graphical front end to Linux (X Windows) is clearly
  separated from the underlying environment and there is a complete set
  of modern programs such as World Wide Web browsers and fax software
  which work directly in the non graphical environment.  This opens up
  the way to provide alternative access paths to the systems
  functionality; Emacspeak is a good example.

  For other users, the comparison is probably less favourable and less
  clear.  People with very specific and complex needs will find that the
  full development system included allows properly customised solutions.
  However, much of the software which exists on other systems is only
  just beginning to become available.  More development is being done
  however in almost all directions.


  2.2.  Availability of Adaptive Technology

  There is almost nothing commercial available specifically for Linux.
  There is a noticeable amount of free software which would be helpful
  in adaptation, for example, a free speech synthesiser and some free
  voice control software.  There are also a number of free packages
  which provide good support for certain Braille terminals, for example.


  2.3.  Inherent Usability

  Linux has the vast advantage over Windows that most of it's software
  is command line oriented.  This is now changing and almost everything
  is now available with a graphical front end.  However, because it is
  in origin a programmers operating system, line oriented programs are
  still being written covering almost all new areas of interest.  For
  the physically disabled, this means that it is easy to build custom
  programs to suit their needs.  For the visually impaired, this should
  make use with a speech synthesiser or Braille terminal easy and useful
  for the foreseeable future.


  Linux's multiple virtual consoles system make it practical to use as a
  multi-tasking operating system by a visually impaired person working
  directly through Braille.


  The windowing system used by Linux (X11) comes with many programming
  tools, and should be adaptable.  However, in practice, the adaptive
  programs available up till now have been more primitive than those on
  the Macintosh or Windows.  They are, however, completely free (as
  opposed to hundreds of pounds) and the quality is definitely
  improving.


  In principle it should be possible to put together a complete, usable
  Linux system for a visually impaired person for about $500 (cheap &
  nasty PC + sound card).  This compares with many thousands of dollars
  for other operating systems (screen reader software/ speech
  synthesiser hardware).  I have yet to see this.  I doubt it would work
  in practice because the software speech synthesisers available for
  Linux aren't yet sufficiently good.  For a physically disabled person,
  the limitation will still be the expense of input hardware.


  3.  Visually Impaired

  I'll use two general categories here.  People who are partially
  sighted and need help seeing / deciphering / following the text and
  those who are unable to use any visual interface whatsoever.


  3.1.  Seeing the Screen with Low Vision

  There are many different problems here.  Often magnification can be
  helpful, but that's not the full story.  Sometimes people can't track
  motion, sometimes people can't find the cursor unless it moves.  This
  calls for a range of techniques, the majority of which are only just
  being added to X.


  3.1.1.  SVGATextMode

  This program is useful for improving the visibility of the normal text
  screen that Linux provides.  The normal screen that Linux provides
  shows 80 characters across by 25 vertically.  This can be changed (and
  the quality of those characters improved) using SVGATextMode.  The
  program allows full access to the possible modes of an SVGA graphics
  card.  For example, the text can be made larger so that only 50 by 15
  characters appear on the screen.  There isn't any easy way to zoom in
  on sections of a screen, but you can resize when needed.


  3.1.2.  X Window System

  For people who can see the screen there are a large number of ways of
  improving X.  They don't add up to a coherent set of features yet, but
  if set up correctly could solve many problems.


  3.1.2.1.  Different Screen Resolutions

  The X server can be set up with many different resolutions.  A single
  key press can then change between them allowing difficult to read text
  to be seen.

  In the file /etc/XF86Config, you have an entry in the Screen section
  with a line beginning with modes.  If, for example, you set this to


       Modes       "1280x1024" "1024x768" "800x600" "640x480" "320x240"



  with each mode set up correctly (which requires a reasonably good mon-
  itor for the highest resolution mode), you will be able to have four
  times screen magnification, switching between the different levels
  using

  Ctrl+Alt+Keypad-Plus and Ctrl+Alt+Keypad-Minus


  Moving the mouse around the screen will scroll you to different parts
  of the screen.  For more details on how to set this up you should see
  the documentation which comes with the XFree86 X server.

  3.1.2.2.  Screen Magnification

  There are several known screen magnification programs, xmag which will
  magnify a portion of the screen as much as needed but is very
  primitive.  Another one is xzoom.  Previously I said that there had to
  be something better than xmag, well this is it.  See section
  ``xzoom''.

  Another program which is available is puff.  This is specifically
  oriented towards visually impaired users.  It provides such features
  as a box around the pointer which makes it easier to locate.  Other
  interesting features of puff are that, if correctly set up, it is able
  to select and magnify portions of the screen as they are updated.
  However, there seem to be interacations between xpuff and the window
  manager which could make it difficult to use.  When used with my fvwm
  setup, it didn't respond at all to key presses.  However using twm
  improved the situation.

  The final program which I have seen working is dynamag.  This again
  has some specific advantages such as the ability to select a specific
  area of the screen and monitor it, refreshing the magnified display at
  regular intervals between a few tenths of a second at twenty seconds.
  dynamag is part of the UnWindows distribution.  See ``UnWindows'' for
  more details.



  3.1.2.3.  Change Screen Font

  The screen fonts all properly written X software should be changeable.
  You can simply make it big enough for you to read.  This is generally
  accomplished by putting a line the file .Xdefaults which should be in
  your home directory.  By putting the correct lines in this you can
  change the fonts of your programs, for example


  Emacs.font: -sony-fixed-medium-r-normal--16-150-75-75-c-80-iso8859-*



  To see what fonts are available, use the program xfontsel under X.

  There should be some way of changing things at a more fundamental
  level so that everything comes out with a magnified font.  This could
  be done by renaming fonts, and by telling telling font generating
  programs to use a different level of scaling.  If someone gets this to
  work properly, please send me the details of how you did it.


  3.1.2.4.  Cross Hair Cursors etc..

  For people that have problems following cursors there are many things
  which can help;


  o  cross-hair cursors (horizontal and vertical lines from the edge of
     the screen)

  o  flashing cursors (flashes when you press a key)


  No software I know of specifically provides a cross hair cursor.
  puff, mentioned in the previous section does however provide a
  flashing box around the cursor which can make it considerably easier
  to locate.

  For now the best that can be done is to change the cursor bitmap.
  Make a bitmap file as you want it, and another one which is the same
  size, but completely black.  Convert them to the XBM format and run


          xsetroot -cursor cursorfile.xbm black-file.xbm



  actually, if you understand masks, then the black-file doesn't have to
  be completely black, but start with it like that.  The .Xdefaults file
  controls cursors used by actual applications.  For much more
  information, please see the X Big Cursor mini-HOWTO, by Joerg
  Schneider <schneid@ira.uka.de>.


  3.1.3.  Audio

  Provided that the user can hear, audio input can be very useful for
  making a more friendly and communicative computing environment.  For a
  person with low vision, audio clues can be used to help locate the
  pointer (see ``UnWindows'').  For a console mode user using Emacspeak
  (see ``Emacspeak''), the audio icons available will provide very many
  useful facilities.

  Setting up Linux audio is covered in the Linux Sound HOWTO (see
  ``Linux Documentation'').  Once sound is set up, sounds can be played
  with the play command which is included with most versions of Linux.
  This is the way to use my version of UnWindows.


  3.1.4.  Producing Large Print

  Using large print with Linux is quite easy.  There are several
  techniques.


  3.1.4.1.  LaTeX / TeX

  LaTeX is an extremely powerful document preparation system.  It may be
  used to produce large print documents of almost any nature.  Though
  somewhat complicated to learn, many documents are produced using LaTeX
  or the underlying typesetting program, TeX.



  this will produce some reasonably large text



       \font\magnifiedtenrm=cmr10 at 20pt  % setup a big font
       \magnifiedtenrm
       this is some large text
       \bye



  For more details, see the LaTeX book which is available in any
  computer book shop.  There are also a large number of introductions
  available on the internet.



  3.1.5.  Outputting Large Text

  Almost all Linux printing uses postscript, and Linux can drive almost
  any printer using it.  I output large text teaching materials using a
  standard Epson dot matrix printer.

  For users of X, there are various tools available which can produce
  large Text.  These include LyX, and many commercial word processors.


  3.2.  Aids for Those Who Can't Use Visual Output

  For someone who is completely unable to use a normal screen there are
  two alternatives Braille and Speech.  Obviously for people who also
  have hearing loss, speech isn't always useful, so Braille will always
  be important.

  If you can choose, which should you choose?  This is a matter of
  `vigorous' debate.  Speech is rapid to use, reasonably cheap and
  especially good for textual applications (e.g. reading a long document
  like this one).  Problems include needing a quiet environment,
  possibly needing headphones to work without disturbing others and
  avoid being listened in on by them (not available for all speech
  synthesisers).

  Braille is better for applications where precise layout is important
  (e.g. spreadsheets).  Also can be somewhat more convenient if you want
  to check the beginning of a sentence when you get to the end.  Braille
  is, however, much more expensive and slower for reading text.
  Obviously, the more you use Braille, the faster you get.  Grade II
  Braille is difficult to learn, but is almost certainly worth it since
  it is much faster.  This means that if you don't use Braille for a
  fair while you can never discover its full potential and decide for
  yourself. Anyway, enough said on this somewhat controversial topic.

  based on original by James Bowden <jrbowden@bcs.org.uk>


  3.2.1.  Braille Terminals

  Braille terminals are normally a line or two of Braille.  Since these
  are at most 80 characters wide and normally 40 wide, they are somewhat
  limited.  I know of two kinds


  o  Hardware driven Braille terminals.

  o  Software driven Braille terminals.


  The first kind works only when the computer is in text mode and reads
  the screen memory directly. See section ``hardware driven Braille
  terminals''.


  The second kind of Braille terminal is similar, in many ways, to a
  normal terminal screen of the kind Linux supports automatically.
  Unfortunately, they need special software to make them usable.


  There are two packages which help with these.  The first, BRLTTY,
  works with several Braille display types and the authors are keen to
  support more as information becomes available.  Currently BRLTTY
  supports Tieman B.V.'s CombiBraille series, Alva B.V.'s ABT3 series
  and Telesensory Systems Inc.'s PowerBraille and Navigator series
  displays.  The use of Blazie Engineering's Braille Lite as a Braille
  display is discouraged, but support may be renewed on demand.  See
  section ``Software Braille Terminals''.


  The other package I am aware of is Braille Enhanced Screen.  This is
  designed to work on other UNIX systems as well as Linux.  This should
  allow user access to a Braille terminal with many useful features such
  as the ability to run different programs in different `virtual
  terminals' at the same time.


  3.2.2.  Speech Synthesis

  Speech Synthesisers take (normally) ASCII text and convert it into
  actual spoken output.  It is possible to have these implemented as
  either hardware or software.  Unfortunately, the free Linux speech
  synthesisers are, reportedly, not good enough to use as a sole means
  of output.


  Hardware speech synthesisers are the alternative.  The main one that I
  know of that works is DECtalk from Digital, driven by emacspeak.
  However, at this time (March 1997) a driver for the Doubletalk
  synthesiser has been announced.  Using emacspeak full access to all of
  the facilities of Linux is fairly easy.  This includes the normal use
  of the shell, a world wide web browser and many other similar
  features, such as email.  Although, it only acts as a plain text
  reader (similar to IBM's one for the PC) when controling programs it
  doesn't understand, with those that it does, it can provide much more
  sophisticated control.  See section ``Emacspeak'' for more information
  about emacspeak.


  3.2.3.  Handling Console Output

  When it starts up, Linux at present puts all of its messages straight
  to the normal (visual) screen.  This could be changed if anyone with a
  basic level of kernel programming ability wants to do it.  This means
  that it is impossible for most Braille devices to get information
  about what Linux is doing before the operating system is completely
  working.


  It is only at that stage that you can start the program that you need
  for access.  If the BRLTTY program is used and run very early in the
  boot process, then from this stage on the messages on the screen can
  be read.  Most hardware and software will still have to wait until the
  system is completely ready.  This makes administering a Linux system
  difficult, but not impossible for a visually impaired person.  Once
  the system is ready however, you can scroll back by pressing (on the
  default keyboard layout) Shift-PageUP.


  There is one Braille system that can use the console directly, called
  the Braillex.  This is designed to read directly from the screen
  memory.  Unfortunately the normal scrolling of the terminal gets in
  the way of this.  If you are using a Kernel newer than 1.3.75, just
  type linux no-scroll at the LILO prompt or configure LILO to do this
  automatically. If you have an earlier version of Linux, see section
  ``Screen Memory Braille Terminals''


  The other known useful thing to do is to use sounds to say when each
  stage of the boot process has been reached. (T.V. Raman suggestion)


  3.2.4.  Optical Character Recognition

  There is a free Optical Character Recognition (OCR) program for Linux
  called xocr.  In principle, if it is good enough, this program would
  allow visually impaired people to read normal books to some extent
  (accuracy of OCR is never high enough..).  However, according to the
  documentation, this program needs training to recognise the particular
  font that it is going to use and I have no idea how good it is since I
  don't have the hardware to test it.


  3.3.  Beginning to Learn Linux

  Beginning to learn Linux can seem difficult and daunting for someone
  who is either coming from no computing background or from a pure DOS
  background.  Doing the following things may help:


  o  Learn to use Linux (or UNIX) on someone else's system before
     setting up your own.

  o  Initially control Linux from your own known speaking/Braille
     terminal.  If you plan to use speech, you may want to learn emacs
     now.  You can learn it as you go along though.  See below

  o  If you come from an MS-DOS background, read the DOS2Linux Mini
     HOWTO for help with converting (see ``The Linux HOWTO Documents'').

  The Emacspeak HOWTO written by Jim Van Zandt (<jrv@vanzandt.mv.com>)
  covers this in much more detail (see ``The Linux HOWTO Documents'').


  If you are planning to use Emacspeak, you should know that Emacspeak
  does not attempt to teach Emacs, so in this sense, prior knowledge of
  Emacs would always be useful.  This said, you certainly do not need to
  know much about Emacs to start using Emacspeak.  In fact, once
  Emacspeak is installed and running, it provides a fluent interface to
  the rich set of online documentation including the info pages, and
  makes learning what you need a lot easier.

  "In summary: starting to use Emacspeak takes little learning.  Getting
  the full mileage out of Emacs and Emacspeak, especially if you intend
  using it as a replacement for X Windows as I do does involve
  eventually becoming familiar with a lot of the Emacs extensions; but
  this is an incremental process and does not need to be done in a day."
  - T.V.Raman

  One other option which may be interesting are the RNIB training tapes
  which include one covering UNIX.  These can be got from



       RNIB
       Customer Services
       PO Box 173
       Peterborough
       Cambridgeshire PE2 6WS
       Tel: 01345 023153 (probably only works in UK)



  3.4.  Braille Embossing

  Linux should be the perfect platform to drive a Braille embosser from.
  There are many formatting tools which are aimed specifically at the
  fixed width device.  A Braille embosser can just be connected to the
  serial port using the standard Linux printing mechanisms.  For more
  info see the Linux Printing HOWTO.


  There is a free software package which acts as a multi-lingual grade
  two translator available for Linux from the American ``National
  Federation for the Blind''.  This is called NFBtrans.  See section
  ``NFB translator'' for more details.


  4.  Hearing Problems

  For the most part there is little problem using a computer for people
  with hearing problems.  Almost all of the output is visual.  There are
  some situations where sound output is used though.  For these, the
  problem can sometimes be worked round by using visual output instead.


  4.1.  Visual Bells

  By tradition, computers go `beep' when some program sends them a
  special code.  This is generally used to get attention to the program
  and for little else.  Most of the time, it's possible to replace this
  by making the entire screen (or terminal emulator) flash.  How to do
  this is very variable though.



     xterm (under X)
        for xterm, you can either change the setting by pressing the
        middle mouse button while holding down the control key, or by
        putting a line with just `XTerm*visualBell: true' (not the
        quotes of course) in the file .Xdefaults in your home directory.


     the console (otherwise)
        The console is slightly more complex.  Please see Alessandro
        Rubini's Visual Bell mini HOWTO for details on this.  Available
        along with all the other Linux documentation (see section
        ``other Linux documents'').  Mostly the configuration has to be
        done on a per application basis, or by changing the Linux Kernel
        its self.



  5.  Physical Problems

  Many of these problems have to be handled individually.  The needs of
  the individual, the ways that they can generate input and other
  factors vary so much that all that this HOWTO can provide is a general
  set of pointers to useful software and expertise.


  5.1.  Unable to Use a Mouse/Pointer

  Limited mobility can make it difficult to use a mouse.  For some
  people a tracker ball can be a very good solution, but for others the
  only possible input device is a keyboard (or even something which
  simulates a keyboard).  For normal use of Linux this shouldn't be a
  problem (but see the section ``Making the keyboard behave''), but for
  users of X, this may cause major problems under some circumstances.
  Fortunately, the fvwm window manager has been designed for use without
  a pointer and most things can be done using this.  I actually do this
  myself when I lose my mouse (don't ask) or want to just keep typing.
  fvwm is included with all distributions of Linux that I know of.
  Actually using other programs will depend on their ability to accept
  key presses.  Many X programs do this for all functions.  Many don't.
  I sticky mouse keys, which are supposedly present in the current
  release of X should make this easier.


  5.1.1.  Unable to Use a Keyboard

  People who are unable to use a keyboard normally can sometimes use one
  through a headstick or a mouthstick.  This calls for special setup of
  the keyboard.  Please see also the section ``Making the keyboard
  behave''.


  5.1.1.1.  Other Input Hardware (X Windows System only)

  For others, the keyboard cannot be used at all and only pointing
  devices are available.  In this case, no solution is available under
  the standard Linux Console and X will have to be used.  If the X-Input
  extension can be taught to use the device and the correct software for
  converting pointer input to characters can be found (I haven't seen it
  yet) then any pointing should be usable without a keyboard.

  There are a number of devices worth considering for such input such as
  touch screens and eye pointers.  Many of these will need a `device
  driver' written for them.  This is not terribly difficult if the
  documentation is available, but requires someone with good C
  programming skills.  Please see the Linux Kernel Hackers guide and
  other kernel reference materials for more information.  Once this is
  set up, it should be possible to use these devices like a normal
  mouse.


  5.1.2.  Controlling Physical Hardware From Linux

  The main group of interest here are the Linux Lab Project.  Generally,
  much GPIB (a standard interface to scientific equipment, also known as
  the IEEE bus) hardware can be controlled.  This potentially gives much
  potential for very ambitious accessibility projects.  As far as I know
  none have yet been attempted.



  5.2.  Speech Recognition

  Speech recognition is a very powerful tool for enabling computer use.
  There are two recognition systems that I know of for Linux, the first
  is ears which is described as ``recognition is not optimal.  But it is
  fine for playing and will be improved'', the second is AbbotDemo ``A
  speaker independent continuous speech recognition system'' which may
  well be more interesting, though isn't available for commercial use
  without prior arrangement.  See the Linux software map for details
  (see section ``other Linux documents'').


  5.3.  Making the Keyboard Behave

  5.3.1.  X Window System.

  The latest X server which comes with Linux can include many features
  which assist in input.  This includes such features as StickKeys,
  MouseKeys, RepeatKeys, BounceKeys, SlowKeys, and TimeOut.  These allow
  customisation of the keyboard to the needs of the user.  These are
  provided as part of the XKB> extension in versions of X after version
  6.1.  To find out your version and see whether you have the extension
  installed, you can try.


  xdpyinfo -queryExtensions



  5.3.2.  Getting Rid of Auto Repeat

  To turn off key repeat on the Linux console run this command (I think
  it has to be run once per console; a good place to run it would be in
  your login files, .profile or .login in your home directory).


  setterm -repeat off



  To get rid of auto repeat on any X server, you can use the command


  xset -r



  which you could put into the file which get runs when you start using
  X (often .xsession or .xinit under some setups)


  Both of these commands are worth looking at for more ways of changing
  behaviour of the console.


  5.3.3.  Macros / Much input, few key presses

  Often in situations such as this, the biggest problem is speed of
  input.  Here the most important thing to aim for is the most number of
  commands with the fewest key presses.  For users of the shell (bash /
  tcsh) you should look at the manual page, in particular command and
  filename completion (press the tab key and bash tries to guess what
  should come next).  For information on macros which provide sequences
  of commands for just one key press, have a look at the Keystroke
  HOWTO.


  5.3.4.  Sticky Keys

  Sticky keys are a feature that allow someone who can only reliably
  press one button at a time to use a keyboard with all of the various
  modifier keys such as shift and control.  These keys, instead of
  having to be held on at the same time as the other key instead become
  like the caps lock key and stay on while the other key is pressed.
  They may then either switch off or stay on for the next key depending
  on what is needed.  For information about how to set this up please
  see the Linux Keyboard HOWTO, especially section `I can use only one
  finger to type with' (section 15 in the version I have) for more
  information on this.  - Information from Toby Reed.



  6.  General Programming Issues

  Many of the issues worth taking into account are the same when writing
  software which is designed to be helpful for access as when trying to
  follow good design.


  6.1.  Try to Make it Easy to Provide Multiple Interfaces

  If your software is only usable through a graphical interface then it
  can be very hard to make it usable for someone who can't see.  If it's
  only usable through a line oriented interface, then someone who can't
  type will have difficulties.

  Provide keyboard shortcuts as well as the use of the normal X pointer
  (generally the mouse).  You can almost certainly rely on the user
  being able to generate key presses to your application.


  6.2.  Make software configurable.

  If it's easy to change fonts then people will be able to change to one
  they can read.  If the colour scheme can be changed then people who
  are colour blind will be more likely to be able to use it.  If fonts
  can be changed easily then the visually impaired will find your
  software more useful.


  6.3.  Test the Software on Users.

  If you have a number of people use your software, each with different
  access problems then they will be more likely to point up specific
  problems.  Obviously, this won't be practical for everybody, but you
  can always ask for feedback.


  6.4.  Make Output Distinct

  Where possible, make it clear what different parts of your program are
  what.  Format error messages in a specific way to identify them.
  Under X, make sure each pane of your window has a name so that any
  screen reader software can identify it.


  6.5.  Licenses

  Some software for Linux (though none of the key programs) has license
  like `not for commercial use'.  This could be quite bad for a person
  who starts using the software for their personal work and then
  possibly begins to be able to do work they otherwise couldn't with it.
  This could be something which frees them from financial and other
  dependence on others people.  Even if the author of the software is
  willing to make exceptions, it makes the user vulnerable both to
  changes of commercial conditions (some company buys up the rights) and
  to refusal from people they could work for (many companies are overly
  paranoid about licenses).  It is much better to avoid this kind of
  licensing where possible.  Protection from commercial abuse of
  software can be obtained through more specific licenses like the GNU
  Public License or Artistic License where needed.


  7.  Other Information



  7.1.  Linux Documentation

  The Linux documentation is critical to the use of Linux and most of
  the documents mentioned here should be included in recent versions of
  Linux, from any source I know of.

  If you want to get the documentation on the Internet, here are some
  example sites.  These should be mirrored at most of the major FTP
  sites in the world.


  o  ftp.funet.fi (128.214.6.100) : /pub/OS/Linux/doc/

  o  tsx-11.mit.edu (18.172.1.2) : /pub/linux/docs/

  o  sunsite.unc.edu (152.2.22.81) : /pub/Linux/docs/


  7.1.1.  The Linux Info Sheet

  A simple and effective explanation of what Linux is.  This is one of
  the things that you should hand over when you want to explain why you
  want Linux and what it is good for.

  The Linux Info Sheet is available on the World Wide Web from
  <http://sunsite.unc.edu/mdw/HOWTO/INFO-SHEET.html> and other mirrors.



  7.1.2.  The Linux Meta FAQ

  A list of other information resources, much more complete than this
  one.  The meta FAQ is available on the World Wide Web from
  <http://sunsite.unc.edu/mdw/HOWTO/META-FAQ.html> and other mirrors


  7.1.3.  The Linux Software Map

  The list of software available for Linux on the Internet.  Many of the
  packages listed here were found through this.  The LSM is available in
  a searchable form from <http://www.boutell.com/lsm/>.  It is also
  available in a single text file in all of the FTP sites mentioned in
  section ``Linux Documentation''.


  7.1.4.  The Linux HOWTO documents

  The HOWTO documents are the main documentation of Linux.  This Access
  HOWTO is an example of one.

  The home site for the Linux Documentation Project which produces this
  information is  <http://sunsite.unc.edu/mdw/linux.html>.  There are
  also many companies producing these in book form.  Contact a local
  Linux supplier for more details.

  The Linux HOWTO documents will be in the directory HOWTO in all of the
  FTP sites mentioned in section ``Linux Documentation''.


  7.1.5.  The Linux FAQ

  A list of `Frequently Asked Questions' with answers which should solve
  many common questions.  The FAQ list is available from
  <http://www.cl.cam.ac.uk/users/iwj10/linux-faq/> as well as all of the
  FTP sites mentioned in section ``Linux Documentation''.

  7.2.  Mailing Lists

  There are two lists that I know of covering these issues specifically
  for Linux.  There are also others which it is worth researching which
  cover computer use more generally.  Incidentally, if a mail is sent to
  these lists I will read it eventually and include any important
  information in the Access-HOWTO, so you don't need to send me a
  separate copy unless it's urgent in some way.


  7.2.1.  The Linux Access List

  This is a general list covering Linux access issues.  It is designed
  `to service the needs of users and developers of the Linux OS and
  software who are either disabled or want to help make Linux more
  accessible'.  To subscribe send email to
  <majordomo@ssv1.union.utah.edu> and in the BODY (not the subject) of
  the email message put:



       subscribe linux-access <your-email-address>



  7.2.2.  The Linux Blind List

  This is a mailing list covering Linux use for blind users.  There is
  also a list of important and useful software being gathered in the
  list's archive.  To subscribe send mail to <blinux-list-
  request@redhat.com> with the subject: help.  This list is now
  moderated.


  7.3.  WWW References

  The World Wide Web is, by it's nature, very rapidly changing.  If you
  are reading this document in an old version then some of these are
  likely to be out of date.  The original version that I maintain on the
  WWW shouldn't go more than a month or two out of date, so refer to
  that please.

  Linux Documentation is available from
  <http://sunsite.unc.edu/mdw/linux.html>

  Linux Access On the Web <http://www.tardis.ed.ac.uk/~mikedlr/access/>
  with all of the versions of the HOWTO in
  <http://www.tardis.ed.ac.uk/~mikedlr/access/HOWTO/>.  Preferably,
  however, download from one of the main Linux FTP sites.  If I get a
  vast amount of traffic I'll have to close down these pages and move
  them elsewhere.

  The BLINUX Documentation and Development Project
  <http://leb.net/blinux/>.  "The purpose of The BLINUX Documentation
  and Development Project is to serve as a catalyst which will both spur
  and speed the development of software and documentation which will
  enable the blind user to run his or her own Linux workstation."

  Emacspeak WWW page
  <http://cs.cornell.edu/home/raman/emacspeak/emacspeak.html>

  BRLTTY unofficial WWW page
  <http://www.sf.co.kr/t.linux/new/brltty.html>

  Yahoo (one of the most major Internet catalogues)
  <http://www.yahoo.com/Society_and_Culture/Disabilities/Adaptive_Technology/>

  The Linux Lab Project  <http://www.fu-berlin.de/~clausi/>.

  The BLYNX pages: Lynx Support Files Tailored For Blind and Visually
  Handicapped Users  <http://leb.net/blinux/blynx/>.


  7.4.  Suppliers

  This is a UK supplier for the Braillex.


  Alphavision Limited



  7.5.  Manufacturers

  7.5.1.  Alphavision

  I think that they are a manufacturer?  RNIB only lists them as a
  supplier, but others say they make the Braillex.


  Alphavision Ltd
  Seymour House
  Copyground Lane
  High Wycombe
  Bucks HP12 3HE
  England
  U.K.



     Phone
        +44 1494-530 555


  7.5.1.1.  Linux Supported Alphavision AT Products


  o  Braillex


  7.5.2.  Blazie Engineering

  The Braille Lite was supported in the original version of BRLTTY.
  That support has now been discontinued.  If you have one and want to
  use it with Linux then that may be possible by using this version of
  the software.


  Blazie Engineering
  105 East Jarrettsville Rd.
  Forest Hill, MD 21050
  U.S.A.



     Phone
        +1 (410) 893-9333

     FAX
        +1 (410) 836-5040

     BBS
        +1 (410) 893-8944

     E-Mail
        <info@blazie.com>

     WWW
        <http://www.blazie.com/>



  7.5.2.1.  Blazie AT Products


  o  Braille Lite (support discontinued)



  7.5.3.  Digital Equipment Corporation


  Digital Equipment Corporation
  P.O. Box CS2008
  Nashua
  NH 03061-2008
  U.S.A



     Order
        +1 800-722-9332

     Tech info
        +1 800-722-9332

     FAX
        +1 603-884-5597

     WWW
        <http://www.digital.com/>


  7.5.3.1.  Linux Supported DEC AT Products


  o  DECTalk Express


  7.5.4.  Kommunikations-Technik Stolper GmbH


  KTS Stolper GmbH
  Herzenhaldenweg 10
  73095 Albershausen
  Germany



     Phone
        +49 7161 37023

     Fax
        +49 7161 32632


  7.5.4.1.  Linux Supported KTG AT Products


  o  Brailloterm


  8.  Software Packages

  References in this section are taken directly from the Linux Software
  map which can be found in all standard places for Linux documentation
  and which lists almost all of the software available for Linux.


  8.1.  Emacspeak

  Emacspeak is the software side of a speech interface to Linux.  Any
  other character based program, such as a WWW browser, or telnet or
  another editor can potentially be used within emacspeak.  The main
  difference between it and normal screen reader software for such
  operating systems as DOS is that it also has a load more extra
  features.  It is based in the emacs text editor.

  A text editor is generally just a program which allows you to change
  the contents of a file, for example, adding new information to a
  letter.  Emacs is in fact far beyond a normal text editor, and so this
  package is much more useful than you might imagine.  You can run any
  other program from within emacs, getting any output it generates to
  appear in the emacs terminal emulator.

  The reason that emacs is a better environment for Emacspeak is that it
  can can understand the layout of the screen and can intelligently
  interpret the meaning of, for example, a calendar, which would just be
  a messy array of numbers otherwise.  The originator of the package
  manages to look after his own Linux machine entirely, doing all of the
  administration from within emacs.  He also uses it to control a wide
  variety of other machines and software directly from that machine.

  Emacspeak is included within the Debian Linux distribution and is
  included as contributed software within the Slakware distribution.
  This means that it is available on many of the CDROM distributions of
  Linux.  By the time this is published, the version included should be
  5 or better, but at present I only have version 4 available for
  examination.



  Begin3
  Title:          emacspeak - a speech output interface to Emacs
  Version:        4.0
  Entered-date:   30MAY96
  Description:    Emacspeak is the first full-fledged speech output
                  system that will allow someone who cannot see to work
                  directly on a UNIX system. (Until now, the only option
                  available to visually impaired users has been to use a
                  talking PC as a terminal.) Emacspeak is built on top
                  of Emacs. Once you start emacs with emacspeak loaded,
                  you get spoken feedback for everything you do. Your
                  mileage will vary depending on how well you can use
                  Emacs.  There is nothing that you cannot do inside
                  Emacs:-)
  Keywords:       handicap access visually impaired blind speech emacs
  Author:         raman@adobe.com (T. V. Raman)
  Maintained-by:  jrv@vanzandt.mv.com (Jim Van Zandt)
  Primary-site:   sunsite.unc.edu apps/sound/speech
                  124kB   emacspeak-4.0.tgz
  Alternate-site:
  Original-site:  http://www.cs.cornell.edu /pub/raman/emacspeak
                  123kB   emacspeak.tar.gz/Info/People/raman/emacspeak/emacspeak.tar.gz
  Platforms:      DECtalk Express or DEC Multivoice speech synthesizer,
                  GNU FSF Emacs 19 (version 19.23 or later) and TCLX
                  7.3B (Extended TCL).
  Copying-policy: GPL
  End



  8.2.  BRLTTY

  This is a program for running a serial port Braille terminal.  It has
  been widely tested and used, and supports a number of different kinds
  of hardware (see the Linux Software Map entry below).

  The maintainer is, Nikhil Nair <nn201@cus.cam.ac.uk>.  The other
  people working on it are Nicolas Pitre <nico@cam.org> and Stephane
  Doyon <doyons@jsp.umontreal.ca>.  Send any comments to all of them.

  The authors seem keen to get support in for more different devices, so
  if you have one you should consider contacting them.  They will almost
  certainly need programming information for the device, so if you can
  contact your manufacturer and get that they are much more likely to be
  able to help you.

  A brief feature list (from their README file) to get you interested


  o  Full implementation of the standard screen review facilities.

  o  A wide range of additional optional features, including blinking
     cursor and capital letters, screen freezing for leisurely review,
     attribute display to locate highlighted text, hypertext links, etc.

  o  `Intelligent' cursor routing.  This allows easy movement of the
     cursor in text editors etc. without moving the hands from the
     Braille display.

  o  A cut & paste function.  This is particularly useful for copying
     long filenames, complicated commands etc.

  o  An on-line help facility.

  o  Support for multiple Braille codes.

  o  Modular design allows relatively easy addition of drivers for other
     Braille displays, or even (hopefully) porting to other Unix-like
     platforms.



       Begin3
       Title:          BRLTTY - Access software for Unix for a blind person
                                using a soft Braille terminal
       Version:        1.0.2, 17SEP96
       Entered-date:   17SEP96
       Description:    BRLTTY is a daemon which provides access to a Unix console
                       for a blind person using a soft Braille display (see the
                       README file for a full explanation).

                       BRLTTY only works with text-mode applications.

                       We hope that this system will be expanded to support
                       other soft Braille displays, and possibly even other
                       Unix-like platforms.
       Keywords:       Braille console access visually impaired blind
       Author:         nn201@cus.cam.ac.uk (Nikhil Nair)
                       nico@cam.org (Nicolas Pitre)
                       doyons@jsp.umontreal.ca (Stephane Doyon)
                       jrbowden@bcs.org.uk (James Bowden)
       Maintained-by:  nn201@cus.cam.ac.uk (Nikhil Nair)
       Primary-site:   sunsite.unc.edu /pub/Linux/system/Access
                       110kb brltty-1.0.2.tar.gz (includes the README file)
                         6kb brltty-1.0.2.README
                         1kb brltty-1.0.2.lsm
       Platforms:      Linux (kernel 1.1.92 or later) running on a PC or DEC Alpha.
                       Not X/graphics.
                       Supported Braille displays (serial communication only):
                         - Tieman B.V.: CombiBraille 25/45/85;
                         - Alva B.V.: ABT3xx series;
                         - Telesensory Systems Inc.: PowerBraille 40 (not 65/80),
                           Navigator 20/40/80 (latest firmware version only?).
       Copying-policy: GPL
       End



  8.3.  Screen

  Screen is a standard piece of software to allow many different
  programs to run at the same time on one terminal.  It has been
  enhanced to support some Braille terminals (those from Telesensory)
  directly.


  8.4.  Rsynth

  This is a speech synthesiser listed in the Linux Software Map.  It
  doesn't apparently work well enough for use by a visually impaired
  person.  Use hardware instead, or improve it.. a free speech
  synthesiser would be really really useful.


  8.5.  xocr

  xocr is a package which implements optical character recognition for
  Linux.  As with Rsynth, I don't think that this will be acceptable as
  a package for use as a sole means of input by a visually impaired
  person.  I suspect that the algorithm used means that it will need to
  be watched over by someone who can check that it is reading correctly.
  I would love to be proved wrong.


  8.6.  xzoom

  xzoom is a screen magnifier, in the same vein as xmag, but
  sufficiently better to be very useful to a visually impaired person.
  The main disadvantages of xzoom are that it can't magnify under
  itself, that some of the key controls aren't compatible with fvwm, the
  normal Linux window manager and that it's default configuration
  doesn't run over a network (this can be fixed at some expense to
  speed).  Apart from that though, it's excellent.  It does continuous
  magnification which allows you to, for example, scroll a document up
  and down, whilst keeping the section you are reading magnified.
  Alternatively, you can move a little box around the screen, magnifying
  the contents and letting you search for the area you want to see.
  xzoom is also available as an rpm from the normal RedHat sites, making
  it very easy to install for people using the rpm system (such as
  Redhat users).



       Begin3
       Title:          xzoom
       Version:        0.1
       Entered-date:   Mar 30 1996
       Description:    xzoom can magnify (by integer value) rotate
                       (by a multiple if 90 degrees) and mirror about
                       the X or Y axes areas on X11 screen
                       and display them in it's window.
       Keywords:       X11 zoom magnify xmag
       Author:         Itai Nahshon <nahshon@best.com>
       Maintained-by:  Itai Nahshon <nahshon@best.com>
       Primary-site:   sunsite.unc.edu
                       probably in /pub/Linux/X11/xutils/xzoom-0.1.tgz
       Platforms:      Linux+11. Support only for 8-bit depth.
                       Tested only in Linux 1.3.* with the XSVGA 3.1.2
       driver.
                                       Needs the XSHM extension.
       Copying-policy: Free
       End



  8.7.  NFBtrans

  nfbtrans is a multi-grade Braille translation program distributed by
  the National Federation for the Blind in the U.S.A.  It is released
  for free in the hope that someone will improve it.  Languages covered
  are USA English, UK English, Spanish, Russian, Esperanto, German,
  Biblical Hebrew and Biblical Greek, though others could be added just
  by writing a translation table.  Also covered are some computer and
  math forms.  I have managed to get it to compile under Linux, though,
  not having a Braille embosser available at the present moment I have
  not been able to test it.

  NFBtrans is available from <ftp://nfb.org/ftp/nfb/braille/nfbtrans/>.
  After downloading it, you will have to compile it.



  8.7.1.  Compiling NFBtrans on Linux

  I have returned this patch to the maintainer of NFBtrans and he says
  that he has included it, so if you get a version later than 740, you
  probably won't have to do anything special.  Just follow the
  instructions included in the package.



          unzip -L NFBTR740.ZIP   #or whatever filename you have
          mv makefile Makefile



  Next save the following to a file (e.g. patch-file)



       *** nfbpatch.c.orig     Tue Mar 12 11:37:28 1996
       --- nfbpatch.c  Tue Mar 12 11:37:06 1996
       ***************
       *** 185,190 ****
       --- 185,193 ----
           return (finfo.st_size);
         }                /* filelength */

       + #ifndef linux
       + /* pretty safe to assume all linux has usleep I think ?? this should be
       + done properly anyway */
         #ifdef SYSVR4
         void usleep(usec)
           int usec;
       ***************
       *** 195,200 ****
       --- 198,204 ----
       UKP  }                /* usleep */

         #endif
       + #endif

         void beep(count)
           int count;



  and run



       patch < patch-file



  then type



       make



  and the program should compile.


  8.8.  UnWindows

  UnWindows is a package of access utilities for X which provides many
  useful facilities for the visually impaired (not blind).  It includes
  a screen magnifier and other customised utilities to help locate the
  pointer.  UnWindows can be downloaded from
  <ftp://ftp.cs.rpi.edu/pub/unwindows>.

  As it comes by default, the package will not work on Linux because it
  relies on special features of Suns.  However, some of the utilities do
  work and I have managed to port most of the rest so this package may
  be interesting to some people.  My port will either be incorporated
  back into the original or will be available in the BLINUX archives
  (see ``WWW references'').  The remaining utility which doesn't yet
  work is the configuration utility.

  In my version the programs, instead of generating sounds themselves,
  just call another program.  The other program could for example be



       play /usr/lib/games/xboing/sounds/ouch.au



  Which would make the xboing ouch noise, for example it could do this
  as the pointer hit the left edge of the screen.


  8.8.1.  dynamag

  dynamag is a screen magnification program.  please see the section on
  Screen magnification (``magnification'').  This program worked in the
  default distribution.


  8.8.2.  coloreyes

  coloreyes makes it easy to find the pointer (mouse) location.  It
  consists of a pair of eyes which always look in the direction of the
  pointer (like xeyes) and change color depending on how far away the
  mouse is (unlike xeyes).  This doesn't work in the default
  distribution, but the test version, at the same location, seems to
  work.


  8.8.3.  border

  border is a program which detects when the pointer (mouse) has moved
  to the edge of the screen and makes a sound according to which edge of
  the screen has been approached.  The version which is available uses a
  SUN specific sound system.  I have now changed this so that instead of
  that, it just runs a command, which could be any Linux sound program.


  8.8.4.  un-twm

  The window manager is a special program which controls the location of
  all of the other windows (programs) displayed on the X screen.  un-twm
  is a special version which will make a sound as the pointer enters
  different windows.  The sound will depend on what window has been
  entered.  The distributed version doesn't work on linux because, like
  border it relies on SUN audio facilities.  Again I already have a
  special version which will be avaliable by the time you read this.


  9.  Hardware

  9.1.  Braille terminals driven from Screen Memory

  These are Braille terminals that can read the screen memory directly
  in a normal text mode.  It is possible to use it to work with Linux
  for almost all of the things that a seeing user can do on the console,
  including installation.  However, it has a problem with the scrolling
  of the normal Linux kernel, so a kernel patch needs to be applied.
  See ``Patching the Kernel for Braillex and Brailloterm''.


  9.1.1.  Braillex

  The Braillex is a terminal which is designed to read directly from the
  Screen memory, thus getting round any problems with MS-DOS programs
  which don't behave strangely.  If you could see it on screen, then
  this terminal should be able to display it in Braille.  In Linux,
  unfortunately, screen handling is done differently from MS-DOS, so
  this has to be changed somewhat.

  To get this terminal to work, you have to apply the patch given below
  in section ``Patching the Kernel''.  Once this is done, the Braillex
  becomes one of the most convenient ways to use Linux as it allows all
  of the information normally available to a seeing person to be read.
  Other terminals don't start working until the operating system has
  completely booted.

  The Braillex is available with two arrangements of Braille cells (80x1
  or 40x2) and there is a model, called the IB 2-D which also has a
  vertical bar to show information about all of the lines of the screen
  (using 4 programmable dots per screen line)


  Price: 8,995  (pounds sterling) or 11495 UKP for 2-D
  Manufacturer: Alphavision Limited (UK)
  Suppliers: ????



  9.1.2.  Brailloterm

  ``What is Brailloterm?


  It's a refreshable display Braille, made by KTS Kommunikations-Technik
  Stolper GmbH.  It has 80 Braille cells in an unique line. Each cell
  has 8 dots that are combined (up/down) to represent a character. By
  default, Brailloterm shows me the line in which the screen cursor is.
  I can use some functions in Brailloterm to see any line in the
  screen.'' - Jose Vilmar Estacio de Souza <jvilmar@embratel.net.br>


  Jose then goes on to say that the terminal can also use the serial
  port under DOS but that it needs a special program.  I don't know if
  any of the ones for Linux would work.


  As with Braillex, this needs a special patch to the kernel work
  properly.  See section ``Patching the Kernel''.

  Price: about 23.000,- DM /  $ 15.000,
  Manufacturer: Kommunikations-Technik Stolper GmbH
  Suppliers: ????



  9.1.3.  Patching the Kernel for Braillex and Brailloterm

  This probably also applies to any other terminals which read directly
  from screen memory to work under MS-DOS.  Mail me to confirm any
  terminals that you find work.  This does not apply and will actually
  lose some features for terminals driven using the BRLTTY software.


  I am told this patch applies to all Kernels version 1.2.X.  It should
  also work on all Kernel versions from 1.1.X to 1.3.72, with just a
  warning from patch (I've tested that the patch applies to 1.3.68 at
  least).  From 1.3.75 the patch is no longer needed because the Kernel
  can be configured not to scroll using `linux no-scroll' at the LILO
  prompt.  See the Boot Prompt HOWTO for more details.



       *** drivers/char/console.c~     Fri Mar 17 07:31:40 1995
       --- drivers/char/console.c      Tue Mar  5 04:34:47 1996
       ***************
       *** 601,605 ****
         static void scrup(int currcons, unsigned int t, unsigned int b)
         {
       !       int hardscroll = 1;

               if (b > video_num_lines || t >= b)
       --- 601,605 ----
         static void scrup(int currcons, unsigned int t, unsigned int b)
         {
       !       int hardscroll = 0;

               if (b > video_num_lines || t >= b)



  To apply it:


  1. Save the above text to a file (say patch-file)

  2. change to the drivers/char directory of your kernel sources

  3. run


                       patch < patch-file



  4. Compile your kernel as normal


  Apply those patches and you should be able to use the Braille terminal
  as normal to read the Linux Console.
  Put in words, the patch just means `change the 1 to a 0 in the first
  line of the function scrup which should be near line 603 in the file
  drivers/char/console.c'.  The main thing about patch is that program
  understands this, and that it knows how to guess what to do when the
  Linux developers change things in that file.


  If you want to use a more modern kernel with completely disabled
  scrolling, (instead of the boot prompt solution I already mentioned),
  please use the following patch.  This does not apply to kernels
  earlier than 1.3.75.



       *** console.c~  Fri Mar 15 04:01:45 1996
       --- console.c   Thu Apr  4 13:29:48 1996
       ***************
       *** 516,520 ****
         unsigned char has_wrapped;          /* all of videomem is data of fg_console */
         static unsigned char hardscroll_enabled;
       ! static unsigned char hardscroll_disabled_by_init = 0;

         void no_scroll(char *str, int *ints)
       --- 516,520 ----
         unsigned char has_wrapped;          /* all of videomem is data of fg_console */
         static unsigned char hardscroll_enabled;
       ! static unsigned char hardscroll_disabled_by_init = 1;

         void no_scroll(char *str, int *ints)



  9.2.  Software Driven Braille Terminals

  The principle of operation of these terminal is very close to that of
  a CRT terminal such as the vt100.  They connect to the serial port and
  the computer has to run a program which sends them output.  At present
  there are two known programs for Linux.  BRLTTY, see section
  ``BRLTTY'') and Braille enhanced screen.


  9.2.1.  Tieman B.V.

  9.2.1.1.  CombiBraille

  This Braille terminal is supported by the BRLTTY software.  It comes
  in three versions with 25, 45 or 85 Braille cells.  The extra five
  cells over a standard display are used for status information.


  Price: around 4600 UKP for the 45 cell model ...
  Manufacturer: Tieman B.V.
  Suppliers: Concept Systems, Nottingham, England (voice +44 115 925 5988)



  9.2.2.  Alva B.V.

  The ABT3xx series is supported in BRLTTY.  Only the ABT340 has been
  confirmed to work at this time.  Please pass back information to the
  BRLTTY authors on other models.


  Price: 20 cell - 2200 UKP; 40 cell 4500 UKP; 80 cell 8000 UKP
  Manufacturer: Alva
  Suppliers: Professional Vision Services LTD, Hertshire, England
             (+44 1462 677331)



  9.2.3.  Telesensory Systems Inc. displays

  Because they have provided programming information to the developers,
  the Telesensory displays are supported both by BRLTTY and screen.


  9.2.3.1.  Powerbraille

  There are three models the 40, the 65 and the 80.  Only the 40 is
  known to be supported by BRLTTY.


  Price: 20 cell - 2200 UKP; 40 cell 4500 UKP; 80 cell 8000 UKP
  Manufacturer: Alva
  Suppliers: Professional Vision Services LTD, Hertshire, England
             (+44 1462 677331)



  9.2.3.2.  Navigator

  Again there are three models the 20, the 60 and the 80.  Recent
  versions are all known to work with BRLTTY but whether earlier ones
  (with earlier firmware) also work has not been confirmed.


  Price: 80 cell 7800 UKP
  Manufacturer: Alva
  Suppliers: Professional Vision Services LTD, Hertshire, England
             (+44 1462 677331)



  9.2.4.  Braille Lite

  This is more a portable computer than a terminal.  It could, however,
  be used with BRLTTY version 0.22 (but not newer versions) as if it was
  a normal Braille terminal.  Unfortunately, many of the features
  available with the CombiBraille cannot be used with the Braille Lite.
  This means that it should be avoided for Linux use where possible.


  Price: $3,395.00
  Manufacturer: Blazie Engineering



  9.3.  Speech Synthesisers

  Speech synthesisers normally connect to the serial port of a PC.
  Useful features include

  o  Braille labels on parts

  o  Many voices to allow different parts of document to be spoken
     differently

  o  Use with headphones (not available on all models)

  The critical problem is that the quality of the speech.  This is much
  more important to someone who is using the speech synthesiser as their
  main source of information than to someone who is just getting neat
  sounds out of a game.  For this reason T.V. Raman seems to only
  recommend the DECTalk.  Acceptable alternatives would be good.


  9.3.1.  DECTalk Express

  This is a hardware speech synthesiser.  It is recommended for use with
  Emacspeak and in fact the DECTalk range are the only speech
  synthesisers which work with that package at present.  This
  synthesiser has every useful feature that I know about.  The only
  disadvantage that I know of at present is price.


  Price: $1195.00
  Manufacturer: Digital Equipment Corporation

  Suppliers: Many.  I'd like details of those with Specific Linux
          support / delivering international or otherwise of note only
          please.  Otherwise refer to local organisations.
          Digital themselves or the Emacspeak WWW pages.



  9.3.2.  Accent SA

  This is a synthesiser made by Aicom Corporation.  An effort has begun
  to write a driver for it however help is needed.  Please see
  <http://www.cyberspc.mb.ca/~astrope/speak.html> if you think you can
  help.



  9.3.3.  SPO256-AL2 Speak and Spell chip.

  Some interest has been expressed in using this chip in self built
  talking circuits.  I'd be interested to know if anyone has found this
  useful.  A software package speak-0.2pl1.tar.gz was produced by David
  Sugar <dyfet@tycho.com>.  My suspicion, though, is that the quality of
  the output wouldn't be good enough for regular use.


  10.  Acknowledgements

  Much of this document was created from various information sources on
  the Internet, many found from Yahoo and DEC's Alta Vista Search
  engine.  Included in this was the documentation of most of the
  software packages mentioned in the text.  Some information was also
  gleaned from the Royal National Institute for the Blind's helpsheets.

  T.V. Raman, the author of Emacspeak has reliably contributed comments,
  information and text as well as putting me in touch with other people
  who he knew on the Internet.


  Kenneth Albanowski <kjahds@kjahds.com> provided the patch needed for
  the Brailloterm and information about it.

  Roland Dyroff of S.u.S.E. GmbH (Linux distributors and makers of
  S.u.S.E. Linux (English/German)) looked up KTS Stolper GmbH at my
  request and got some hardware details and information on the
  Brailloterm.

  The most major and careful checks over of this document were done by
  James Bowden, <jrbowden@bcs.org.uk> and Nikhil Nair
  <nn201@cus.cam.ac.uk>, the BRLTTY authors who suggested a large number
  of corrections as well as extra information for some topics.

  The contributors to the blinux and linux-access mailing lists have
  contributed to this document by providng information for me to read.

  Mark E. Novak of the Trace R&D centre <http://trace.wisc.edu/> pointed
  me in the direction of several packages of software and information
  which I had not seen before.  He also made some comments on the
  structure of the document which I have partially taken into account
  and should probably do more about.

  Other contributors include Nicolas Pitrie and Stephane Doyon.

  A number of other people have contributed comments and information.
  Specific contributions are acknowledged within the document.

  This version was specifically produced for RedHat's Dr. Linux book.
  This is because they provided warning of it's impending release to
  myself and other LDP authors.  Their doing this is strongly
  appreciated since wrong or old information sits around much longer in
  a book than on the Internet.

  No doubt you made a contribution and I haven't mentioned it.  Don't
  worry, it was an accident.  I'm sorry.  Just tell me and I will add
  you to the next version.



  Linux 2.4 Advanced Routing HOWTO
  Netherlabs BV (bert hubert <bert.hubert@netherlabs.nl>)
  Gregory Maxwell <greg@linuxpower.cx>
  Remco van Mook <remco@virtu.nl>
  Martijn van Oosterhout <kleptog@cupid.suninternet.com>
  Paul B Schroeder  <paulsch@us.ibm.com>
  howto@ds9a.nl
  v0.1.0 $Date: 2000/05/26 15:42:43 $

  A very hands-on approach to iproute2, traffic shaping and a bit of
  netfilter
  ______________________________________________________________________

  Table of Contents



  1. Dedication

  2. Introduction

     2.1 Disclaimer & License
     2.2 Prior knowledge
     2.3 What Linux can do for you
     2.4 Housekeeping notes
     2.5 Access, CVS & submitting updates
     2.6 Layout of this document

  3. Introduction to iproute2

     3.1 Why iproute2?
     3.2 Iproute2 tour
     3.3 Prerequisites
     3.4 Exploring your current configuration
        3.4.1 (TT
        3.4.2 (TT
        3.4.3 (TT
     3.5 ARP

  4. Rules - routing policy database

     4.1 Simple source routing

  5. GRE and other tunnels

     5.1 A few general remarks about tunnels:
     5.2 IP in IP tunneling
     5.3 GRE tunneling
        5.3.1 IPv4 Tunneling
        5.3.2 IPv6 Tunneling
     5.4 Userland tunnels

  6. IPsec: secure IP over the internet

  7. Multicast routing

  8. Using Class Based Queueing for bandwidth management

     8.1 What is queueing?
     8.2 First attempt at bandwidth division
     8.3 What to do with excess bandwidth
     8.4 Class subdivisions
     8.5 Loadsharing over multiple interfaces

  9. More queueing disciplines

     9.1 pfifo_fast
     9.2 Stochastic Fairness Queueing
     9.3 Token Bucket Filter
     9.4 Random Early Detect
     9.5 Ingress policer qdisc

  10. Netfilter & iproute - marking packets

  11. More classifiers

     11.1 The "fw" classifier
     11.2 The "u32" classifier
        11.2.1 U32 selector
        11.2.2 General selectors
        11.2.3 Specific selectors
     11.3 The "route" classifier
     11.4 The "rsvp" classifier
     11.5 The "tcindex" classifier

  12. Kernel network parameters

     12.1 Reverse Path Filtering
     12.2 Obscure settings
        12.2.1 Generic ipv4
        12.2.2 Per device settings
        12.2.3 Neighbor pollicy
        12.2.4 Routing settings

  13. Backbone applications of traffic control

     13.1 Router queues

  14. Shaping Cookbook

     14.1 Running multiple sites with different SLAs
     14.2 Protecting your host from SYN floods
     14.3 Ratelimit ICMP to prevent dDoS
     14.4 Prioritising interactive traffic

  15. Advanced Linux Routing

     15.1 How does packet queueing really work?
     15.2 Advanced uses of the packet queueing system
     15.3 Other packet shaping systems

  16. Dynamic routing - OSPF and BGP

  17. Further reading

  18. Acknowledgements



  ______________________________________________________________________

  1.  Dedication

  This document is dedicated to lots of people, and is my attempt to do
  something back. To list but a few:


    Rusty Russel

    Alexey N. Kuznetsov

    The good folks from Google

    The staff of Casema Internet



  2.  Introduction

  Welcome, gentle reader.

  This document hopes to enlighten you on how to do more with Linux
  2.2/2.4 routing. Unbeknownst to most users, you already run tools
  which allow you to do spectacular things. Commands like 'route' and
  'ifconfig' are actually very thin wrappers for the very powerful
  iproute2 infrastructure

  I hope that this HOWTO will become as readable as the ones by Rusty
  Russel of (amongst other things) netfilter fame.
  You can always reach us by writing the HOWTO team
  <mailto:HOWTO@ds9a.nl>.

  2.1.  Disclaimer & License

  This document is distributed in the hope that it will be useful, but
  WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

  In short, if your STM-64 backbone breaks down and distributes
  pornography to your most esteemed customers - it's never our fault.
  Sorry.

  Copyright (c) 2000 by bert hubert, Gregory Maxwell and Martijn van
  Oosterhout

  Please freely copy and distribute (sell or give away) this document in
  any format. It's requested that corrections and/or comments be
  fowarded to the document maintainer. You may create a derivative work
  and distribute it provided that you:


  1. Send your derivative work (in the most suitable format such as
     sgml) to the LDP (Linux Documentation Project) or the like for
     posting on the Internet.  If not the LDP, then let the LDP know
     where it is available.

  2. License the derivative work with this same license or use GPL.
     Include a copyright notice and at least a pointer to the license
     used.

  3. Give due credit to previous authors and major contributors.

     If you're considering making a derived work other than a
     translation, it's requested that you discuss your plans with the
     current maintainer.

  It is also requested that if you publish this HOWTO in hardcopy that
  you send the authors some samples for 'review purposes' :-)



  2.2.  Prior knowledge

  As the title implies, this is the 'Advanced' HOWTO. While by no means
  rocket science, some prior knowledge is assumed. This document is
  meant as a sequel to the Linux 2.4 Networking HOWTO
  <http://www.ds9a.nl/2.4Networking/> by the same authors. You should
  probably read that first.

  Here are some orther references which might help learn you more:

     Rusty Russels networking-concepts-HOWTO <http://netfilter.kernel
        notes.org/unreliable-guides/networking-concepts-HOWTO.html>
        Very nice introduction, explaining what a network is, and how it
        is connected to other networks

     Linux Networking-HOWTO (Previously the Net-3 HOWTO)
        Great stuff, although very verbose. It learns you a lot of stuff
        that's already configured if you are able to connect to the
        internet.  Should be located in /usr/doc/HOWTO/NET3-4-HOWTO.txt
        but can be also be found online
        <http://www.linuxports.com/howto/networking>



  2.3.  What Linux can do for you

  A small list of things that are possible:


    Throttle bandwidth for certain computers

    Throttle bandwidth to certain computers

    Help you to fairly share your bandwidth

    Protect your network from DoS attacks

    Protect the internet from your customers

    Multiplex several servers as one, for load balancing or enhanced
     availability

    Restrict access to your computers

    Limit access of your users to other hosts

    Do routing based on user id (yes!), MAC address, source IP address,
     port, type of service, time of day or content

  Currently not many people are using these advanced features. This has
  several reasons. While the provided documentation is verbose, it is
  not very hands on. Traffic control is almost undocumented.

  2.4.  Housekeeping notes

  There are several things which should be noted about this document.
  While I wrote most of it, I really don't want it to stay that way. I
  am a strong believer in Open Source, so I encourage you to send
  feedback, updates, patches etcetera. Do not hesitate to inform me of
  typos or plain old errors.  If my English sounds somewhat wooden,
  please realise that I'm not a native speaker. Feel free to send
  suggestions.

  If you feel to you are better qualified to maintain a section, or
  think that you can author and maintain new sections, you are welcome
  to do so. The SGML of this HOWTO is available via CVS, I very much
  envision more people working on it.

  In aid of this, you will find lots of FIXME notices. Patches are
  always welcome! Wherever you find a FIXME, you should know that you
  are treading unknown territory. This is not to say that there are no
  errors elsewhere, but be extra careful. If you have validated
  something, please let us know so we can remove the FIXME notice.

  About this HOWTO, I will take some liberties along the road. For
  example, I postulate a 10Mbit internet connection, while I know full
  well that those are not very common.

  2.5.  Access, CVS & submitting updates

  The canonical location for the HOWTO is here
  <http://www.ds9a.nl/2.4Routing>.

  We now have anonymous CVS access available for the world at large.
  This is good in several ways. You can easily upgrade to newer versions
  of this HOWTO and submitting patches is no work at all.

  Furthermore, it allows the authors to work on the source
  independently, which is good too.

       $ export CVSROOT=:pserver:anon@outpost.ds9a.nl:/var/cvsroot
       $ cvs login
       CVS password: [enter 'cvs' (without 's)]
       $ cvs co 2.4routing
       cvs server: Updating 2.4routing
       U 2.4routing/2.4routing.sgml



  If you spot an error, or want to add something, just fix it locally,
  and run cvs diff -u, and send the result off to us.

  A Makefile is supplied which should help you create postscript, dvi,
  pdf, html and plain text. You may need to install sgml-tools,
  ghostscript and tetex to get all formats.


  2.6.  Layout of this document

  We will be doing interesting stuff almost immediately, which also
  means that there will initially be parts that are explained
  incompletely or are not perfect. Please gloss over these parts and
  assume that all will become clear.

  Routing and filtering are two distinct things. Filtering is documented
  very well by Rusty's HOWTOs, available here:


    Rusty's Remarkably Unreliable Guides
     <http://netfilter.kernelnotes.org/unreliable-guides/>

  We will be focusing mostly on what is possible by combining netfilter
  and iproute2.

  3.  Introduction to iproute2

  3.1.  Why iproute2?

  Most Linux distributions, and most UNIX's, currently use the venerable
  'arp', 'ifconfig' and 'route' commands. While these tools work, they
  show some unexpected behaviour under Linux 2.2 and up. For example,
  GRE tunnels are an integral part of routing these days, but require
  completely different tools.

  With iproute2, tunnels are an integral part of the tool set

  The 2.2 and above Linux kernels include a completely redesigned
  network subsystem. This new networking code brings Linux performance
  and a feature set with little competition in the general OS arena. In
  fact, the new routing filtering, and classifying code is more
  featureful then that provided by many dedicated routers and firewalls
  and traffic shaping products.

  As new networking concepts have been invented, people have found ways
  to plaster them on top of the existing framework in existing OSes.
  This constant layering of cruft has lead to networking code that is
  filled with strange behaviour, much like most human languages. In the
  past, Linux emulated SunOS's handling of many of these things, which
  was not ideal.

  This new framework has made it possible to clearly express features
  previously not possible.



  3.2.  Iproute2 tour

  Linux has a sophisticated system for bandwidth provisioning called
  Traffic Control. This system supports various method for classifying,
  prioritising, sharing, and limiting both inbound and outbound traffic.


  We'll start off with a tiny tour of iproute2 possibilities.

  3.3.  Prerequisites

  You should make sure that you have the userland tools installed. This
  package is called 'iproute' on both RedHat and Debian, and may
  otherwise be found at ftp://ftp.inr.ac.ru/ip-
  routing/iproute2-2.2.4-now-ss??????.tar.gz".  Some parts of iproute
  require you to have certain kernel options enabled.

  FIXME: We should mention  <ftp://ftp.inr.ac.ru/ip-
  routing/iproute2-current.tar.gz> is always the latest


  3.4.  Exploring your current configuration

  This may come as a surprise, but iproute2 is already configured! The
  current commands ifconfig and route are already using the advanced
  syscalls, but mostly with very default (ie, boring) settings.

  The ip tool is central, and we'll ask it do display our interfaces for
  us.

  3.4.1.  ip  shows us our links



       [ahu@home ahu]$ ip link list
       1: lo: <LOOPBACK,UP> mtu 3924 qdisc noqueue
           link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
       2: dummy: <BROADCAST,NOARP> mtu 1500 qdisc noop
           link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff
       3: eth0: <BROADCAST,MULTICAST,PROMISC,UP> mtu 1400 qdisc pfifo_fast qlen 100
           link/ether 48:54:e8:2a:47:16 brd ff:ff:ff:ff:ff:ff
       4: eth1: <BROADCAST,MULTICAST,PROMISC,UP> mtu 1500 qdisc pfifo_fast qlen 100
           link/ether 00:e0:4c:39:24:78 brd ff:ff:ff:ff:ff:ff
       3764: ppp0: <POINTOPOINT,MULTICAST,NOARP,UP> mtu 1492 qdisc pfifo_fast qlen 10
           link/ppp



  Your mileage may vary, but this is what it shows on my NAT router at
  home. I'll only explain part of the output as not everything is
  directly relevant.

  We first see the loopback interface. While your computer may function
  somewhat without one, I'd advise against it. The mtu size (maximum
  transfer unit) is 3924 octects, and it is not supposed to queue. Which
  makes sense because the loopback interface is a figment of your
  kernels imagination.

  I'll skip the dummy interface for now, and it may not be present on
  your computer. Then there are my two network interfaces, one at the
  side of my cable modem, the other serves my home ethernet segment.
  Furthermore, we see a ppp0 interface.

  Note the absence of IP addresses. Iproute disconnects the concept of
  'links' and 'IP addresses'. With IP aliasing, the concept of 'the' IP
  address had become quite irrelevant anyhow.

  It does show us the MAC addresses though, the hardware identifier of
  our ethernet interfaces.

  3.4.2.  ip  shows us our IP addresses



       [ahu@home ahu]$ ip address show
       1: lo: <LOOPBACK,UP> mtu 3924 qdisc noqueue
           link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
           inet 127.0.0.1/8 brd 127.255.255.255 scope host lo
       2: dummy: <BROADCAST,NOARP> mtu 1500 qdisc noop
           link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff
       3: eth0: <BROADCAST,MULTICAST,PROMISC,UP> mtu 1400 qdisc pfifo_fast qlen 100
           link/ether 48:54:e8:2a:47:16 brd ff:ff:ff:ff:ff:ff
           inet 10.0.0.1/8 brd 10.255.255.255 scope global eth0
       4: eth1: <BROADCAST,MULTICAST,PROMISC,UP> mtu 1500 qdisc pfifo_fast qlen 100
           link/ether 00:e0:4c:39:24:78 brd ff:ff:ff:ff:ff:ff
       3764: ppp0: <POINTOPOINT,MULTICAST,NOARP,UP> mtu 1492 qdisc pfifo_fast qlen 10
           link/ppp
           inet 212.64.94.251 peer 212.64.94.1/32 scope global ppp0



  This contains more information. It shows all our addresses, and to
  which cards they belong. 'inet' stands for Internet. There are lots of
  other address families, but these don't concern us right now.

  Lets examine eth0 somewhat closer. It says that it is related to the
  inet address '10.0.0.1/8'. What does this mean? The /8 stands for the
  number of bits that are in the Network Address. There are 32 bits, so
  we have 24 bits left that are part of our network. The first 8 bits of
  10.0.0.1 correspond to 10.0.0.0, our Network Address, and our netmask
  is 255.0.0.0.

  The other bits are connected to this interface, so 10.250.3.13 is
  directly available on eth0, as is 10.0.0.1 for example.

  With ppp0, the same concept goes, though the numbers are different.
  It's address is 212.64.94.251, without a subnet mask. This means that
  we have a point-to-point connection and that every address, with the
  exception of 212.64.94.251, is remote. There is more information
  however, it tells us that on the other side of the link is yet again
  only one address, 212.64.94.1. The /32 tells us that there are no
  'network bits'.

  It is absolutely vital that you grasp these concepts. Refer to the
  documentation mentioned at the beginning of this HOWTO if you have
  trouble.

  You may also note 'qdisc', which stands for Queueing Discipline. This
  will become vital later on.


  3.4.3.  ip  shows us our routes

  Well, we now know how to find 10.x.y.z addresses, and we are able to
  reach 212.64.94.1. This is not enough however, so we need instructions
  on how to reach the world. The internet is available via our ppp
  connection, and it appears that 212.64.94.1 is willing to spread our
  packets around the world, and deliver results back to us.


       [ahu@home ahu]$ ip route show
       212.64.94.1 dev ppp0  proto kernel  scope link  src 212.64.94.251
       10.0.0.0/8 dev eth0  proto kernel  scope link  src 10.0.0.1
       127.0.0.0/8 dev lo  scope link
       default via 212.64.94.1 dev ppp0



  This is pretty much self explanatory. The first 4 lines of output
  explicitly state what was already implied by ip address show, the last
  line tells us that the rest of the world can be found via 212.64.94.1,
  our default gateway. We can see that it is a gateway because of the
  word via, which tells us that we need to send packets to 212.64.94.1,
  and that it will take care of things.

  For reference, this is what the old 'route' utility shows us:


       [ahu@home ahu]$ route -n
       Kernel IP routing table
       Destination     Gateway         Genmask         Flags Metric Ref    Use
       Iface
       212.64.94.1     0.0.0.0         255.255.255.255 UH    0      0        0 ppp0
       10.0.0.0        0.0.0.0         255.0.0.0       U     0      0        0 eth0
       127.0.0.0       0.0.0.0         255.0.0.0       U     0      0        0 lo
       0.0.0.0         212.64.94.1     0.0.0.0         UG    0      0        0 ppp0



  3.5.  ARP

  ARP is the Address Resolution Protocol as described in RFC 826
  <http://www.faqs.org/rfcs/rfc826.html>.  ARP is used by a networked
  machine to resolve the hardware location/address of another machine on
  the same local network.  Machines on the Internet are generally known
  by their names which resolve to IP addresses.  This is how a machine
  on the foo.com network is able to communicate with another machine
  which is on the bar.net network.  An IP address, though, cannot tell
  you the physical location of a machine.  This is where ARP comes into
  the picture.

  Let's take a very simple example.  Suppose I have a network composed
  of several machines.  Two of the machines which are currently on my
  network are foo with an IP address of 10.0.0.1 and bar with an IP
  address of 10.0.0.2.  Now foo wants to ping bar to see that he is
  alive, but alas, foo has no idea where bar is.  So when foo decides to
  ping bar he will need to send out an ARP request.  This ARP request is
  akin to foo shouting out on the network "Bar (10.0.0.2)!  Where are
  you?"  As a result of this every machine on the network will hear foo
  shouting, but only bar (10.0.0.2) will respond.  Bar will then send an
  ARP reply directly back to foo which is akin bar saying, "Foo
  (10.0.0.1) I am here at 00:60:94:E9:08:12."  After this simple
  transaction used to locate his friend on the network foo is able to
  communicate with bar until he (his arp cache) forgets where bar is.

  Now let's see how this works.  You can view your machines current
  arp/neighbor cache/table like so:


       [root@espa041 /home/src/iputils]# ip neigh show
       9.3.76.42 dev eth0 lladdr 00:60:08:3f:e9:f9 nud reachable
       9.3.76.1 dev eth0 lladdr 00:06:29:21:73:c8 nud reachable

  As you can see my machine espa041 (9.3.76.41) knows where to find
  espa042 (9.3.76.42) and espagate (9.3.76.1).  Now let's add another
  machine to the arp cache.



       [root@espa041 /home/paulsch/.gnome-desktop]# ping -c 1 espa043
       PING espa043.austin.ibm.com (9.3.76.43) from 9.3.76.41 : 56(84) bytes of data.
       64 bytes from 9.3.76.43: icmp_seq=0 ttl=255 time=0.9 ms

       --- espa043.austin.ibm.com ping statistics ---
       1 packets transmitted, 1 packets received, 0% packet loss
       round-trip min/avg/max = 0.9/0.9/0.9 ms

       [root@espa041 /home/src/iputils]# ip neigh show
       9.3.76.43 dev eth0 lladdr 00:06:29:21:80:20 nud reachable
       9.3.76.42 dev eth0 lladdr 00:60:08:3f:e9:f9 nud reachable
       9.3.76.1 dev eth0 lladdr 00:06:29:21:73:c8 nud reachable



  As a result of espa041 trying to contact espa043, espa043's hardware
  address/location has now been added to the arp/nieghbor cache.  So
  until the entry for espa043 times out (as a result of no communication
  between the two) espa041 knows where to find espa043 and has no need
  to send an ARP request.

  Now let's delete espa043 from our arp cache:



       [root@espa041 /home/src/iputils]# ip neigh delete 9.3.76.43 dev eth0
       [root@espa041 /home/src/iputils]# ip neigh show
       9.3.76.43 dev eth0  nud failed
       9.3.76.42 dev eth0 lladdr 00:60:08:3f:e9:f9 nud reachable
       9.3.76.1 dev eth0 lladdr 00:06:29:21:73:c8 nud stale



  Now espa041 has again forgotten where to find espa043 and will need to
  send another ARP request the next time he needs to communicate with
  espa043.  You can also see from the above output that espagate
  (9.3.76.1) has been changed to the "stale" state.  This means that the
  location shown is still valid, but it will have to be confirmed at the
  first transaction to that machine.


  4.  Rules - routing policy database

  If you have a large router, you may well cater for the needs of
  different people, who should be served differently. The routing policy
  database allows you to do this by having multiple sets of routing
  tables.

  If you want to use this feature, make sure that your kernel is
  compiled with the "IP: policy routing" feature.

  When the kernel needs to make a routing decision, it finds out which
  table needs to be consulted. By default, there are three tables. The
  old 'route' tool modifies the main and local tables, as does the ip
  tool (by default).

  The default rules:

  [ahu@home ahu]$ ip rule list
  0:      from all lookup local
  32766:  from all lookup main
  32767:  from all lookup default



  This lists the priority of all rules. We see that all rules apply to
  all packets ('from all'). We've seen the 'main' table before, it's
  output by ip route ls, but the 'local' and 'default' table are new.

  If we want to do fancy things, we generate rules which point to
  different tables which allow us to override system wide routing rules.

  For the exact semantics on what the kernel does when there are more
  matching rules, see Alexey's ip-cfref documentation.


  4.1.  Simple source routing

  Let's take a real example once again, I have 2 (actually 3, about time
  I returned them) cable modems, connected to a Linux NAT
  ('masquerading') router. People living here pay me to use the
  internet. Suppose one of my house mates only visits hotmail and wants
  to pay less. This is fine with me, but you'll end up using the low-end
  cable modem.

  The 'fast' cable modem is known as 212.64.94.251 and is an PPP link to
  212.64.94.1. The 'slow' cable modem is known by various ip addresses,
  212.64.78.148 in this example and is a link to 195.96.98.253.

  The local table:


       [ahu@home ahu]$ ip route list table local
       broadcast 127.255.255.255 dev lo  proto kernel  scope link  src 127.0.0.1
       local 10.0.0.1 dev eth0  proto kernel  scope host  src 10.0.0.1
       broadcast 10.0.0.0 dev eth0  proto kernel  scope link  src 10.0.0.1
       local 212.64.94.251 dev ppp0  proto kernel  scope host  src 212.64.94.251
       broadcast 10.255.255.255 dev eth0  proto kernel  scope link  src 10.0.0.1
       broadcast 127.0.0.0 dev lo  proto kernel  scope link  src 127.0.0.1
       local 212.64.78.148 dev ppp2  proto kernel  scope host  src 212.64.78.148
       local 127.0.0.1 dev lo  proto kernel  scope host  src 127.0.0.1
       local 127.0.0.0/8 dev lo  proto kernel  scope host  src 127.0.0.1



  Lots of obvious things, but things that need to specified somewhere.
  Well, here they are. The default table is empty.

  Let's view the 'main' table:


       [ahu@home ahu]$ ip route list table main
       195.96.98.253 dev ppp2  proto kernel  scope link  src 212.64.78.148
       212.64.94.1 dev ppp0  proto kernel  scope link  src 212.64.94.251
       10.0.0.0/8 dev eth0  proto kernel  scope link  src 10.0.0.1
       127.0.0.0/8 dev lo  scope link
       default via 212.64.94.1 dev ppp0



  We now generate a new rule which we call 'John', for our hypothetical
  house mate. Although we can work with pure numbers, it's far easier if
  we add our tables to /etc/iproute2/rt_tables.



       # echo 200 John >> /etc/iproute2/rt_tables
       # ip rule add from 10.0.0.10 table John
       # ip rule ls
       0:      from all lookup local
       32765:  from 10.0.0.10 lookup John
       32766:  from all lookup main
       32767:  from all lookup default



  Now all that is left is to generate Johns table, and flush the route
  cache:


       # ip route add default via 195.96.98.253 dev ppp2 table John
       # ip route flush cache



  And we are done. It is left as an exercise for the reader to implement
  this in ip-up.

  5.  GRE and other tunnels

  There are 3 kinds of tunnels in Linux. There's IP in IP tunneling, GRE
  tunneling and tunnels that live outside the kernel (like, for example
  PPTP).

  5.1.  A few general remarks about tunnels:

  Tunnels can be used to do some very unusual and very cool stuff. They
  can also make things go horribly wrong when you don't configure them
  right. Don't point your default route to a tunnel device unless you
  know _exactly_ what you are doing :-). Furthermore, tunneling
  increases overhead, because it needs an extra set of IP headers.
  Typically this is 20 bytes per packet, so if the normal packet size
  (MTU) on a network is 1500 bytes, a packet that is sent through a
  tunnel can only be 1480 bytes big. This is not necessarily a problem,
  but be sure to read up on IP packet fragmentation/reassembly when you
  plan to connect large networks with tunnels. Oh, and of course, the
  fastest way to dig a tunnel is to dig at both sides.


  5.2.  IP in IP tunneling

  This kind of tunneling has been available in Linux for a long time. It
  requires 2 kernel modules, ipip.o and new_tunnel.o.

  Let's say you have 3 networks: Internal networks A and B, and
  intermediate network C (or let's say, Internet).  So we have network
  A:



       network 10.0.1.0
       netmask 255.255.255.0
       router  10.0.1.1

  The router has address 172.16.17.18 on network C.

  and network B:


       network 10.0.2.0
       netmask 255.255.255.0
       router  10.0.2.1



  The router has address 172.19.20.21 on network C.

  As far as network C is concerned, we assume that it will pass any
  packet sent from A to B and vice versa. You might even use the
  Internet for this.

  Here's what you do:

  First, make sure the modules are installed:



       insmod ipip.o
       insmod new_tunnel.o



  Then, on the router of network A, you do the following:


       ifconfig tunl0 10.0.1.1 pointopoint 172.19.20.21
       route add -net 10.0.2.0 netmask 255.255.255.0 dev tunl0



  And on the router of network B:


       ifconfig tunl0 10.0.2.1 pointopoint 172.16.17.18
       route add -net 10.0.1.0 netmask 255.255.255.0 dev tunl0



  And if you're finished with your tunnel:


       ifconfig tunl0 down



  Presto, you're done. You can't forward broadcast or IPv6 traffic
  through an IP-in-IP tunnel, though. You just connect 2 IPv4 networks
  that normally wouldn't be able to talk to each other, that's all. As
  far as compatibility goes, this code has been around a long time, so
  it's compatible all the way back to 1.3 kernels. Linux IP-in-IP tun
  neling doesn't work with other Operating Systems or routers, as far as
  I know. It's simple, it works. Use it if you have to, otherwise use
  GRE.


  5.3.  GRE tunneling

  GRE is a tunneling protocol that was originally developed by Cisco,
  and it can do a few more things than IP-in-IP tunneling. For example,
  you can also transport multicast traffic and IPv6 through a GRE
  tunnel.

  In Linux, you'll need the ip_gre module.


  5.3.1.  IPv4 Tunneling

  Let's do IPv4 tunneling first:

  Let's say you have 3 networks: Internal networks A and B, and
  intermediate network C (or let's say, Internet).

  So we have network A:


       network 10.0.1.0
       netmask 255.255.255.0
       router  10.0.1.1



  The router has address 172.16.17.18 on network C.  Let's call this
  network neta (ok, hardly original)

  and network B:


       network 10.0.2.0
       netmask 255.255.255.0
       router  10.0.2.1



  The router has address 172.19.20.21 on network C.  Let's call this
  network netb (still not original)

  As far as network C is concerned, we assume that it will pass any
  packet sent from A to B and vice versa. How and why, we do not care.

  On the router of network A, you do the following:


       ip tunnel add netb mode gre remote 172.19.20.21 local 172.16.17.18 ttl 255
       ip addr add 10.0.1.1 dev netb
       ip route add 10.0.2.0/24 dev netb



  Let's discuss this for a bit. In line 1, we added a tunnel device, and
  called it netb (which is kind of obvious because that's where we want
  it to go). Furthermore we told it to use the GRE protocol (mode gre),
  that the remote address is 172.19.20.21 (the router at the other end),
  that our tunneling packets should originate from 172.16.17.18 (which
  allows your router to have several IP addresses on network C and let
  you decide which one to use for tunneling) and that the TTL field of
  the packet should be set to 255 (ttl 255).


  In the second line we gave the newly born interface netb the address
  10.0.1.1. This is OK for smaller networks, but when you're starting up
  a mining expedition (LOTS of tunnels), you might want to consider
  using another IP range for tunneling interfaces (in this example, you
  could use 10.0.3.0).


  In the third line we set the route for network B. Note the different
  notation for the netmask. If you're not familiar with this notation,
  here's how it works: you write out the netmask in binary form, and you
  count all the ones. If you don't know how to do that, just remember
  that 255.0.0.0 is /8, 255.255.0.0 is /16 and 255.255.255.0 is /24. Oh,
  and 255.255.254.0 is /23, in case you were wondering.

  But enough about this, let's go on with the router of network B.


       ip tunnel add neta mode gre remote 172.16.17.18 local 172.19.20.21 ttl 255
       ip addr add 10.0.2.1 dev neta
       ip route add 10.0.1.0/24 dev neta



  And when you want to remove the tunnelon router A:


       ip link set netb down
       ip tunnel del netb



  Of course, you can replace netb with neta for router B.


  5.3.2.  IPv6 Tunneling


  BIG FAT WARNING !!

  The following is untested and might therefore be completely and utter
  BOLLOCKS. Proceed at your own risk. Don't say I didn't warn you.

  FIXME: check & try all this


  A short bit about IPv6 addresses:

  IPv6 addresses are, compared to IPv4 addresses, monstrously big. An
  example:

  3ffe:2502:200:40:281:48fe:dcfe:d9bc


  So, to make writing them down easier, there are a few rules:

    Don't use leading zeroes. Same as in IPv4.

    Use colons to separate every 16 bits or two bytes.

    When you have lots of consecutive zeroes, you can write this down
     as ::. You can only do this once in an address and only for
     quantities of 16 bits, though.

     Using these rules, the address
     3ffe:0000:0000:0000:0000:0020:34A1:F32C can be written down as
     3ffe::20:34A1:F32C, which is a lot shorter.

  On with the tunnels.

  Let's assume that you have the following IPv6 network, and you want to
  connect it to 6bone, or a friend.



       Network 3ffe:406:5:1:5:a:2:1/96



  Your IPv4 address is 172.16.17.18, and the 6bone router has IPv4
  address 172.22.23.24.



       ip tunnel add sixbone mode sit remote 172.22.23.24 local 172.16.17.18 ttl 255
       ip link set sixbone up
       ip addr add 3ffe:406:5:1:5:a:2:1/96 dev sixbone
       ip route add 3ffe::/15 dev sixbone



  Let's discuss this. In the first line, we created a tunnel device
  called sixbone. We gave it mode sit (which is IPv6 in IPv4 tunneling)
  and told it where to go to (remote) and where to come from (local).
  TTL is set to maximum, 255. Next, we made the device active (up).
  After that, we added our own network address, and set a route for
  3ffe::/15 (which is currently all of 6bone) through the tunnel.

  GRE tunnels are currently the preferred type of tunneling. It's a
  standard that's also widely adopted outside the Linux community and
  therefore a Good Thing.


  5.4.  Userland tunnels

  There are literally dozens of implementations of tunneling outside the
  kernel. Best known are of course PPP and PPTP, but there are lots more
  (some proprietary, some secure, some that don't even use IP) and that
  is really beyond the scope of this HOWTO.


  6.  IPsec: secure IP over the internet

  FIXME: Waiting for our feature editor Stefan to finish his stuf


  7.  Multicast routing

  FIXME: Editor Vacancy!

  8.  Using Class Based Queueing for bandwidth management

  Now, when I discovered this, it *really* blew me away. Linux 2.2 comes
  with everything to manage bandwidth in ways comparable to high-end
  dedicated bandwidth management systems.

  Linux even goes far beyond what Frame and ATM provide.


  The two basic units of Traffic Control are filters and queues. Filters
  place traffic into queues, and queues gather traffic and decide what
  to send first, send later, or drop. There are several flavours of
  filters and queues.

  The most common filters are fwmark and u32, the first lets you use the
  Linux netfilter code to select traffic, and the second allows you to
  select traffic based on ANY header. The most notable queue is Class
  Based Queue. CBQ is a super-queue, in that it contains other queues
  (even other CBQs).

  It may not be immediately clear what queueing has to do with bandwidth
  management, but it really does work.

  For our frame of reference, I have modelled this section on an ISP
  where I learned the ropes, so to speak, Casema Internet in The
  Netherlands. Casema, which is actually a cable company, has internet
  needs both for their customers and for their own office. Most
  corporate computers there have access to the internet. In reality,
  they have lots of money to spend and do not use Linux for bandwidth
  management.

  We will explore how our ISP could have used Linux to manage their
  bandwidth.


  8.1.  What is queueing?

  With queueing we determine the order in which data is *sent*. It it
  important to realise this, we can only shape data that we transmit.
  How this changing the order determine the speed of transmission?
  Imagine a cash register which is able to process 3 customers per
  minute.

  People wishing to pay go stand in line at the 'tail end' of the queue.
  This is 'fifo queueing'. Let's suppose however that we let certain
  people always join in the middle of the queue, in stead of at the end.
  These people spend a lot less time in the queue and are therefore able
  to shop faster.

  With the way the internet works, we have no direct control of what
  people send us. It's a bit like your (physical!) mailbox at home.
  There is no way you can influence the world to modify the amount of
  mail they send you, short of contacting everybody.

  However, the internet is mostly based on TCP/IP which has a few
  features that help us. TCP/IP has no way of knowing the capacity of
  the network between two hosts, so it just starts sending data faster
  and faster ('slow start') and when packets start getting lost, because
  there is no room to send them, it will slow down.

  This is the equivalent of not reading half of your mail, and hoping
  that people will stop sending it to you. With the difference that it
  works for the Internet :-)

  FIXME: explain that normally, ACKs are used to determine speed



       [The Internet] ---<E3, T3, whatever>--- [Linux router] --- [Office+ISP]
                                             eth1          eth0



  Now, our Linux router has two interfaces which I shall dub eth0 and
  eth1.  Eth1 is connected to our router which moves packets from to and
  from our fibre link.

  Eth0 is connected to a subnet which contains both the corporate
  firewall and our network head ends, through which we can connect to
  our customers.

  Because we can only limit what we send, we need two separate but
  possibly very similar sets of rules. By modifying queueing on eth0, we
  determine how fast data gets sent to our customers, and therefor how
  much downstream bandwidth is available for them. Their 'download
  speed' in short.

  On eth1, we determine how fast we send data to The Internet, how fast
  our users, both corporate and commercial can upload data.


  8.2.  First attempt at bandwidth division

  CBQ enables us to generate several classes, and even classes within
  classes.  The larger devisions might be called 'agencies'. Within
  these classes may be things like 'bulk' or 'interactive'.

  For example, we may have a 10 megabit internet connection to 'the
  internet' which is to be shared by our customers, and our corporate
  needs. We should not allow a few people at the office to steal away
  large amounts of bandwidth which we should sell to our customers.

  On the other hand, or customers should not be able to drown out the
  traffic from our field offices to the customer database.

  Previously, one way to solve this was either to use Frame relay/ATM
  and create virtual circuits. This works, but frame is not very fine
  grained, ATM is terribly inefficient at carrying IP traffic, and
  neither have standardised ways to segregate different types of traffic
  into different VCs.

  Hover, if you do use ATM, Linux can also happily perform deft acts of
  fancy traffic classification for you too. Another way is to order
  separate connections, but this is not very practical and also not very
  elegant, and still does not solve all your problems.

  CBQ to the rescue!

  Clearly we have two main classes, 'ISP' and 'Office'. Initially, we
  really don't care what the divisions do with their bandwidth, so we
  don't further subdivide their classes.

  We decide that the customers should always be guaranteed 8 megabits of
  downstream traffic, and our office 2 megabits.

  Setting up traffic control is done with the iproute2 tool tc.


       # tc qdisc add dev eth0 root handle 10: cbq bandwidth 10Mbit avpkt 1000



  Ok, lots of numbers here. What has happened? We have configured the
  'queueing discipline' of eth0. With 'root' we denote that this is the
  root discipline. We have given it the handle '10:'. We want to do CBQ,
  so we mention that on the command line as well. We tell the kernel
  that it can allocate 10Mbit and that the average packet size is
  somewhere around 1000 octets.
  FIXME: Double check with Alexey the the built in cell calculation is
  sufficient.

  FIXME: With a 1500 mtu, the default cell is calculated same as the old
  example.

  FIXME: I checked the sources (userspace and kernel), so we should be
  safe omitting it.

  Now we need to generate our root class, from which all others descend:


       # tc class add dev eth0 parent 10:0 classid 10:1 cbq bandwidth 10Mbit rate \
         10Mbit allot 1514 weight 1Mbit prio 8 maxburst 20 avpkt 1000



  Even more numbers to worry about - the Linux CBQ implementation is
  very generic. With 'parent 10:0' we indicate that this class descends
  from the root of qdisc handle '10:' we generated earlier. With
  'classid 10:1' we name this class.

  We really don't tell the kernel a lot more, we just generate a class
  that completely fills the available device. We also specify that the
  MTU (plus some overhead) is 1514 octets. We also 'weigh' this class
  with 1Mbit - a tuning parameter.

  We now generate our ISP class:


       # tc class add dev eth0 parent 10:1 classid 10:100 cbq bandwidth 10Mbit rate \
         8Mbit allot 1514 weight 800Kbit prio 5 maxburst 20 avpkt 1000 \
         bounded



  We allocate 8Mbit, and indicate that this class must not exceed this
  by adding the 'bounded' parameter. Otherwise this class would have
  started borrowing bandwidth from other classes, something we will
  discuss later on.

  To top it off, we generate the root Office class:


       # tc class add dev eth0 parent 10:1 classid 10:200 cbq bandwidth 10Mbit rate \
         2Mbit allot 1514 weight 200Kbit prio 5 maxburst 20 avpkt 1000 \
         bounded



  To make this a bit clearer, a diagram which shows our classes:



  +-------------[10: 10Mbit]----------------------+
  |+-------------[10:1 root 10Mbit]--------------+|
  ||                                             ||
  || +-[10:100 8Mbit]-+ +--[10:200 2Mbit]-----+  ||
  || |                | |                     |  ||
  || | ISP            | |  Office             |  ||
  || |                | |                     |  ||
  || +----------------+ +---------------------+  ||
  ||                                             ||
  |+---------------------------------------------+|
  +-----------------------------------------------+



  Ok, now we have told the kernel what our classes are, but not yet how
  to manage the queues. We do this presently, in one fell swoop for both
  classes.



       # tc qdisc add dev eth0 parent 10:100 sfq quantum 1514b perturb 15
       # tc qdisc add dev eth0 parent 10:200 sfq quantum 1514b perturb 15



  In this case we install the Stochastic Fairness Queueing discipline
  (sfq), which is not quite fair, but works well up to high bandwidths
  without burning up CPU cycles. There are other queueing disciplines
  available which are better, but need more CPU. The Token Bucket Filter
  is often used.

  Now there is only one thing left to do and that is to explain to the
  kernel which packets belong to which class. Initially we will do this
  natively with iproute2, but more interesting applications are possible
  in combination with netfilter.



       # tc filter add dev eth0 parent 10:0 protocol ip prio 100 u32 match ip dst \
         150.151.23.24 flowid 10:200

       # tc filter add dev eth0 parent 10:0 protocol ip prio 25 u32 match ip dst \
         150.151.0.0/16 flowid 10:100



  Here is is assumed that our office hides behind a firewall with IP
  address 150.151.23.24 and that all our other IP addresses should be
  considered to be part of the ISP.

  The u32 match is a very simple one - more sophisticated matching rules
  are possible when using netfilter to mark our packets, which we can
  than match on in tc.

  Now we have fairly divided the downstream bandwidth, we need to do the
  same for the upstream. For brevity's sake, all in one go:



  # tc qdisc add dev eth1 root handle 20: cbq bandwidth 10Mbit avpkt 1000

  # tc class add dev eth1 parent 20:0 classid 20:1 cbq bandwidth 10Mbit rate \
    10Mbit allot 1514 weight 1Mbit prio 8 maxburst 20 avpkt 1000

  # tc class add dev eth1 parent 20:1 classid 20:100 cbq bandwidth 10Mbit rate \
    8Mbit allot 1514 weight 800Kbit prio 5 maxburst 20 avpkt 1000 \
    bounded

  # tc class add dev eth1 parent 20:1 classid 20:200 cbq bandwidth 10Mbit rate \
    2Mbit allot 1514 weight 200Kbit prio 5 maxburst 20 avpkt 1000 \
    bounded

  # tc qdisc add dev eth1 parent 20:100 sfq quantum 1514b perturb 15
  # tc qdisc add dev eth1 parent 20:200 sfq quantum 1514b perturb 15

  # tc filter add dev eth1 parent 20:0 protocol ip prio 100 u32 match ip src \
    150.151.23.24 flowid 20:200

  # tc filter add dev eth1 parent 20:0 protocol ip prio 25 u32 match ip src \
    150.151.0.0/16 flowid 20:100



  8.3.  What to do with excess bandwidth

  In our hypothetical case, we will find that even when the ISP
  customers are mostly offline (say, at 8AM), our office still gets only
  2Mbit, which is rather wasteful.

  By removing the 'bounded' statements, classes will be able to borrow
  bandwidth from each other.

  Some classes may not wish to borrow their bandwidth to other classes.
  Two rival ISPs on a single link may not want to offer each other
  freebees. In such a case, you can add the keyword 'isolated' at the
  end of your 'tc class add' lines.


  8.4.  Class subdivisions

  FIXME: completely untested suppositions! Try this!

  We can go further than this. Should the employees at the office decide
  to all fire up their 'napster' clients, it is still possible that our
  database runs out of bandwidth. Therefore, we create two subclasses,
  'Human' and 'Database'.

  Our database always needs 500Kbit, so we have 1.5Mbit left for Human
  consumption.

  We now need to create two new classes, within our Office class:


       # tc class add dev eth0 parent 10:200 classid 10:250 cbq bandwidth 10Mbit rate \
         500Kbit allot 1514 weight 50Kbit prio 5 maxburst 20 avpkt 1000 \
         bounded

       # tc class add dev eth0 parent 10:200 classid 10:251 cbq bandwidth 10Mbit rate \
         1500Kbit allot 1514 weight 150Kbit prio 5 maxburst 20 avpkt 1000 \
         bounded



  FIXME: Finish this example!


  8.5.  Loadsharing over multiple interfaces

  FIXME: document TEQL


  9.  More queueing disciplines

  The Linux kernel offers us lots of queueing disciplines. By far the
  most widely used is the pfifo_fast queue - this is the default. This
  also explains why these advanced features are so robust. They are
  nothing more than 'just another queue'.

  Each of these queues has specific strengths and weaknesses. Not all of
  them may be as well tested.

  9.1.  pfifo_fast

  This queue is, as the name says, First In, First Out, which means that
  no packet receives special treatment. At least, not quite. This queue
  has 3 so called 'bands'. Within each band, FIFO rules apply. However,
  as long as there are packets waiting in band 0, band 1 won't be
  processed. Same goes for band 1 and band 2.

  9.2.  Stochastic Fairness Queueing

  SFQ, as said earlier, is not quite deterministic, but works (on
  average).  Its main benefits are that it requires little CPU and
  memory. 'Real' fair queueing requires that the kernel keep track of
  all running sessions.

  Stochastic Fairness Queueing (SFQ) is a simple implementation of fair
  queueing algorithms family. It's less accurate than others, but it
  also requires less calculations while being almost perfectly fair.

  The key word in SFQ is conversation (or flow), being a sequence of
  data packets having enough common parameters to distinguish it from
  other conversations. The parameters used in case of IP packets are
  source and destination address, and the protocol number.

  SFQ consists of dynamically allocated number of FIFO queues, one queue
  for one conversation. The discipline runs in round-robin, sending one
  packet from each FIFO in one turn, and this is why it's called fair.
  The main advantage of SFQ is that it allows fair sharing the link
  between several applications and prevent bandwidth take-over by one
  client. SFQ however cannot determine interactive flows from bulk ones
  -- one usually needs to do the selection with CBQ before, and then
  direct the bulk traffic into SFQ.



  9.3.  Token Bucket Filter

  The Token Bucket Filter (TBF) is a simple queue, that only passes
  packets arriving at rate in bounds of some administratively set limit,
  with possibility to buffer short bursts.

  The TBF implementation consists of a buffer (bucket), constatly filled
  by some virtual pieces of information called tokens, at specific rate
  (token rate). The most important parameter of the bucket is its size,
  that is number of tokens it can store.

  Each arriving token lets one incoming data packet of out the queue and
  is then deleted from the bucket. Associating this algorithm with the
  two flows -- token and data, gives us three possible scenarios:


    The data arrives into TBF at rate equal the rate of incoming
     tokens. In this case each incoming packet has its matching token
     and passes the queue without delay.

    The data arrives into TBF at rate smaller than the token rate.
     Only some tokens are deleted at output of each data packet sent out
     the queue, so the tokens accumulate, up to the bucket size. The
     saved tokens can be then used to send data over the token rate, if
     short data burst occurs.

    The data arrives into TBF at rate bigger than the token rate. In
     this case filter overrun occurs -- incoming data can be only sent
     out without loss until all accumulated tokens are used. After that,
     overlimit packets are dropped.


  The last scenario is very important, because it allows to
  administratively shape the bandwidth available to data, passing the
  filter.  The accumulation of tokens allows short burst of overlimit
  data to be still passed without loss, but any lasting overload will
  cause packets to be constantly dropped.

  The Linux kernel seems to go beyond this specification, and also
  allows us to limit the speed of the burst transmission. However,
  Alexey warns us:


       Note that the peak rate TBF is much more tough: with MTU 1500 P_crit =
       150Kbytes/sec. So, if you need greater peak rates, use alpha with
       HZ=1000 :-)


  FIXME: is this still true with TSC (pentium+)? Well sort of

  FIXME: if not, add section on raising HZ


  9.4.  Random Early Detect

  RED has some extra smartness built in. When a TCP/IP session starts,
  neither end knows the amount of bandwidth available. So TCP/IP starts
  to transmit slowly and goes faster and faster, though limited by the
  latency at which ACKs return.

  Once a link is filling up, RED starts dropping packets, which indicate
  to TCP/IP that the link is congested, and that it should slow down.
  The smart bit is that RED simulates real congestion, and starts to
  drop some packets some time before the link is entirely filled up.
  Once the link is completely saturated, it behaves like a normal
  policer.

  For more information on this, see the Backbone chapter.


  9.5.  Ingress policer qdisc



  The Ingress qdisc comes in handy if you need to ratelimit a host
  without help from routers or other Linux boxes. You can police
  incoming bandwidth and drop packets when this bandwidth exceeds your
  desired rate. This can save your host from a SYN flood, for example,
  and also works to slow down TCP/IP, which responds to dropped packets
  by reducing speed.

  FIXME: instead of dropping, can we also assign it to a real queue?

  FIXME: shaping by dropping packets seems less desirable than using,
  for example, a token bucket filter. Not sure though, Cisco CAR works
  this way, and people appear happy with it.

  See the reference to ``IOS Committed Access Rate'' at the end of this
  document.


  In short: you can use this to limit how fast your computer downloads
  files, thus leaving more of the available bandwidth for others.

  See the section on protecting your host from SYN floods for an example
  on how this works.

  10.  Netfilter & iproute - marking packets

  So far we've seen how iproute works, and netfilter was mentioned a few
  times. This would be a good time to browse through Rusty's Remarkably
  Unreliable guides <http://netfilter.kernelnotes.org/unreliable-
  guides/>. Netfilter itself can be found here
  <http://antarctica.penguincomputing.com/~netfilter/>.

  Netfilter allows us to filter packets, or mangle their headers. One
  special feature is that we can mark a packet with a number. This is
  done with the --set-mark facility.

  As an example, this command marks all packets destined for port 25,
  outgoing mail:



       # iptables -A PREROUTING -i eth0 -t mangle -p tcp --dport 25 \
        -j MARK --set-mark 1



  Let's say that we have multiple connections, one that is fast (and
  expensive, per megabyte) and one that is slower, but flat fee. We
  would most certainly like outgoing mail to go via the cheap route.

  We've already marked the packets with a '1', we now instruct the
  routing policy database to act on this:



       # echo 201 mail.out >> /etc/iproute2/rt_tables
       # ip rule add fwmark 1 table mail.out
       # ip rule ls
       0:      from all lookup local
       32764:  from all fwmark        1 lookup mail.out
       32766:  from all lookup main
       32767:  from all lookup default



  Now we generate the mail.out table with a route to the slow but cheap
  link:



  # /sbin/ip route add default via 195.96.98.253 dev ppp0 table mail.out



  And we are done. Should we want to make exceptions, there are lots of
  ways to achieve this. We can modify the netfilter statement to exclude
  certain hosts, or we can insert a rule with a lower priority that
  points to the main table for our excepted hosts.

  We can also use this feature to honour TOS bits by marking packets
  with a different type of service with different numbers, and creating
  rules to act on that. This way you can even dedicate, say, an ISDN
  line to interactive sessions.

  Needless to say, this also works fine on a host that's doing NAT
  ('masquerading').

  Note: for this to work, you need to have some options enabled in your
  kernel:



       IP: advanced router (CONFIG_IP_ADVANCED_ROUTER) [Y/n/?]
        IP: policy routing (CONFIG_IP_MULTIPLE_TABLES) [Y/n/?]
         IP: use netfilter MARK value as routing key (CONFIG_IP_ROUTE_FWMARK) [Y/n/?]



  11.  More classifiers


  Classifiers are the way by which the kernel decides which queue a
  packet should be placed into. There are various different classifiers,
  each of which can be used for different purposes.


     fw Bases the decision on how the firewall has marked the packet.


     u32
        Bases the decision on fields within the packet (i.e. source IP
        address, etc)


     route
        Bases the decision on which route the packet will be routed by.


     rsvp, rsvp6
        Bases the decision on the target (destination,protocol) and
        optionally the source as well. (I think)


     tcindex
        FIXME: Fill me in

  Note that in general there are many ways in which you can classify
  packet and that it generally comes down to preference as to which
  system you wish to use.

  Classifiers in general accept a few arguments in common. They are
  listed here for convenience:

     protocol
        The protocol this classifier will accept. Generally you will
        only be accepting only IP traffic. Required.


     parent
        The handle this classifier is to be attached to. This handle
        must be an already existing class. Required.


     prio
        The priority of this classifier. Higher numbers get tested
        first.


     handle
        This handle means different things to different filters.

        FIXME: Add more

  All the following sections will assume you are trying to shape the
  traffic going to HostA. They will assume that the root class has been
  configured on 1: and that the class you want to send the selected
  traffic to is 1:1.


  11.1.  The "fw" classifier

  The "fw" classifier relies on the firewall tagging the packets to be
  shaped. So, first we will setup the firewall to tag them:



       # iptables -I PREROUTING -t mangle -p tcp -d HostA \
        -j MARK --set-mark 1



  Now all packets to that machine are tagged with the mark 1. Now we
  build the packet shaping rules to actually shape the packets.  Now we
  just need to indicate that we want the packets that are tagged with
  the mark 1 to go to class 1:1. This is accomplished with the command:



       # tc filter add dev eth1 protocol ip parent 1:0 prio 1 handle 1 fw classid 1:1



  This should be fairly self-explanatory. Attach to the 1:0 class a
  filter with priority 1 to filter all packet marked with 1 in the
  firewall to class 1:1. Note how the handle here is used to indicate
  what the mark should be.

  That's all there is to it! This is the (IMHO) easy way, the other ways
  are I think harder to understand. Note that you can apply the full
  power of the firewalling code with this classifier, including matching
  MAC addresses, user IDs and anything else the firewall can match.


  11.2.  The "u32" classifier

  The U32 filter is the most advanced filter available in the current
  implementation. It entirely based on hashing tables, which make it
  robust when there are many filter rules.

  In its simplest form the U32 filter is a list of records, each
  consisting of two fields: a selector and an action. The selectors,
  described below, are compared with the currently processed IP packet
  until the first match and the associated action is performed. The
  simplest type of action would be directing the packet into defined CBQ
  class.

  The commandline of tc filter program, used to configure the filter,
  consists of three parts: filter specification, a selector and an
  action.  The filter specification can be defined as:



       tc filter add dev IF [ protocol PROTO ]
                            [ (preference|priority) PRIO ]
                            [ parent CBQ ]



  The protocol field describes protocol that the filter will be applied
  to. We will only discuss case of ip protocol. The preference field
  (priority can be used alternatively) sets the priority of currently
  defined filter. This is important, since you can have several filters
  (lists of rules) with different priorities.  Each list will be passed
  in the order the rules were added, then list with lower priority
  (higher preference number) will be processed. The parent field defines
  the CBQ tree top (e.g. 1:0), the filter should be attached to.

  The options decribed apply to all filters, not only U32.


  11.2.1.  U32 selector

  The U32 selector contains definition of the pattern, that will be
  matched to the currently processed packet. Precisely, it defines which
  bits are to be matched in the packet header and nothing more, but this
  simple method is very powerful. Let's take a look at the following
  examplesm taken directly from a pretty complex, real-world filter:



       # filter parent 1: protocol ip pref 10 u32 fh 800::800 order 2048 key ht 800 bkt 0 flowid 1:3 \
         match 00100000/00ff0000 at 0



  For now, leave the first line alone - all these parameters describe
  the filter's hash tables. Focus on the selector line, containing match
  keyword. This selector will match to IP headers, whose second byte
  will be 0x10 (0010). As you can guess, the 00ff number is the match
  mask, telling the filter exactly which bits to match. Here it's 0xff,
  so the byte will match if it's exactly 0x10. The at keyword means that
  the match is to be started at specified offset (in bytes) -- in this
  case it's beginning of the packet.  Translating all that to human
  language, the packet will match if its Type of Service field will have
  ,,low delay'' bits set. Let's analyze another rule:



  # filter parent 1: protocol ip pref 10 u32 fh 800::803 order 2051 key ht 800 bkt 0 flowid 1:3 \
    match 00000016/0000ffff at nexthdr+0



  The nexthdr option means next header encapsulated in the IP packet,
  i.e. header of upper-layer protocol. The match will also start here at
  the beginning of the next header. The match should occur in the
  second, 32-bit word of the header. In TCP and UDP protocols this field
  contains packet's destination port. The number is given in big-endian
  format, i.e. older bits first, so we simply read 0x0016 as 22 decimal,
  which stands for SSH service if this was TCP. As you guess, this match
  is ambigous without a context, and we will discuss this later.


  Having understood all the above, we will find the following selector
  quite easy to read: match c0a80100/ffffff00 at 16. What we got here is
  a three byte match at 17-th byte, counting from the IP header start.
  This will match for packets with destination address anywhere in
  192.168.1/24 network. After analyzing the examples, we can summarize
  what we have learnt.


  11.2.2.  General selectors

  General selectors define the pattern, mask and offset the pattern will
  be matched to the packet contents. Using the general selectors you can
  match virtually any single bit in the IP (or upper layer) header. They
  are more difficult to write and read, though, than specific selectors
  that described below. The general selector syntax is:



       match [ u32 | u16 | u8 ] PATTERN MASK [ at OFFSET | nexthdr+OFFSET]



  One of the keywords u32, u16 or u8 specifies length of the pattern in
  bits. PATTERN and MASK should follow, of length defined by the
  previous keyword. The OFFSET parameter is the offset, in bytes, to
  start matching. If nexthdr+ keyword is given, the offset is relative
  to start of the upper layer header.


  Some examples:



       # tc filter add dev ppp14 parent 1:0 prio 10 u32 \
            match u8 64 0xff at 8 \
            flowid 1:4



  Packet will match to this rule, if its time to live (TTL) is 64.  TTL
  is the field starting just after 8-th byte of the IP header.



  # tc filter add dev ppp14 parent 1:0 prio 10 u32 \
       match u8 0x10 0xff at nexthdr+13 \
       protocol tcp \
       flowid 1:3 \



  This rule will only match TCP packets with ACK bit set. Here we can
  see an example of using two selectors, the final result will be
  logical AND of their results. If we take a look at TCP header diagram,
  we can see that the ACK bit is second older bit (0x10) in the 14-th
  byte of the TCP header (at nexthdr+13).  As for the second selector,
  if we'd like to make our life harder, we could write match u8 0x06
  0xff at 9 instead if using the specific selector protocol tcp, because
  6 is the number of TCP protocol, present in 10-th byte of the IP
  header.  On the other hand, in this example we couldn't use any
  specific selector for the first match - simply because there's no
  specific selector to match TCP ACK bits.


  11.2.3.  Specific selectors

  The following table contains a list of all specific selectors the
  author of this section has found in the tc program source code. They
  simply make your life easier and increase readability of your filter's
  configuration.

  FIXME: table placeholder - the table is in separate file
  ,,selector.html''

  FIXME: it's also still in Polish :-(

  FIXME: must be sgml'ized

  Some examples:



       # tc filter add dev ppp0 parent 1:0 prio 10 u32 \
            match ip tos 0x10 0xff \
            flowid 1:4



  The above rule will match packets, which have the TOS field set to
  0x10.  The TOS field starts at second byte of the packet and is one
  byte big, so we coul write an equivalent general selector: match u8
  0x10 0xff at 1. This gives us hint to the internals of U32 filter --
  the specific rules are always translated to general ones, and in this
  form they are stored in the kernel memory. This leads to another
  conclusion -- the tcp and udp selectors are exactly the same and this
  is why you can't use single match tcp dst 53 0xffff selector to match
  TCP packets sent to given port -- they will also match UDP packets
  sent to this port. You must remember to also specify the protocol and
  end up with the following rule:



       # tc filter add dev ppp0 parent 1:0 prio 10 u32 \
               match tcp dst 53 0xffff \
               match ip protocol 0x6 0xff \
               flowid 1:2

  11.3.  The "route" classifier


  This classifier filters based on the results of the routing tables.
  When a packet that is traversing through the classes reaches one that
  is marked with the "route" filter, it splits the packets up based on
  information in the routing table.



       # tc filter add dev eth1 parent 1:0 protocol ip prio 100 route



  Here we add a route classifier onto the parent node 1:0 with priority
  100.  When a packet reaches this node (which, since it is the root,
  will happen immediately) it will consult the routing table and if one
  matches will send it to the given class and give it a priority of 100.
  Then, to finally kick it into action, you add the appropriate routing
  entry:

  The trick here is to define 'realm' based on either destination or
  source.  The way to do it is like this:



       # ip route add Host/Network via Gateway dev Device realm RealmNumber



  For instance, we can define our destination network 192.168.10.0 with
  a realm number 10:



       # ip route add 192.168.10.0/24 via 192.168.10.1 dev eth1 realm 10



  When adding route filters, we can use realm numbers to represent the
  networks or hosts and specify how the routes match the filters.



       # tc filter add dev eth1 parent 1:0 protocol ip prio 100 \
         route to 10 classid 1:10



  The above rule says packets going to the network 192.168.10.0 match
  class id 1:10.

  Route filter can also be used to match source routes. For example,
  there is a subnetwork attached to the Linux router on eth2.



  # ip route add 192.168.2.0/24 dev eth2 realm 2
  # tc filter add dev eth1 parent 1:0 protocol ip prio 100 \
    route from 2 classid 1:2



  Here the filter specifies that packets from the subnetwork 192.168.2.0
  (realm 2) will match class id 1:2.



  11.4.  The "rsvp" classifier

  FIXME: Fill me in


  11.5.  The "tcindex" classifier

  FIXME: Fill me in


  12.  Kernel network parameters

  The kernel has lots of parameters which can be tuned for different
  circumstances. While, as usual, the default parameters serve 99% of
  installations very well, we don't call this the Advanced HOWTO for the
  fun of it!

  The interesting bits are in /proc/sys/net, take a look there. Not
  everything will be documented here initially, but we're working on it.

  12.1.  Reverse Path Filtering

  By default, routers route everything, even packets which 'obviously'
  don't belong on your network. A common example is private IP space
  escaping onto the internet. If you have an interface with a route of
  195.96.96.0/24 to it, you do not expect packets from 212.64.94.1 to
  arrive there.

  Lots of people will want to turn this feature off, so the kernel
  hackers have made it easy. There are files in /proc where you can tell
  the kernel to do this for you. The method is called "Reverse Path
  Filtering". Basically, if the reply to this packet wouldn't go out the
  interface this packet came in, then this is a bogus packet and should
  be ignored.

  The following fragment will turn this on for all current and future
  interfaces.



       # for i in /proc/sys/net/ipv4/conf/*/rp_filter ; do
       >  echo 2 > $i
       > done



  Going by the example above, if a packet arrived on the Linux router on
  eth1 claiming to come from the Office+ISP subnet, it would be dropped.
  Similarly, if a packet came from the Office subnet, claiming to be
  from somewhere outside your firewall, it would be dropped also.

  The above is full reverse path filtering. The default is to only
  filter based on IPs that are on directly connected networks. This is
  because the full filtering breaks in the case of asymmetric routing
  (where packets come in one way and go out another, like satellite
  traffic, or if you have dynamic (bgp, ospf, rip) routes in your
  network. The data comes down through the satellite dish and replies go
  back through normal land-lines).

  If this exception applies to you (and you'll probably know if it does)
  you can simply turn off the rp_filter on the interface where the
  satellite data comes in. If you want to see if any packets are being
  dropped, the log_martians file in the same directory will tell the
  kernel to log them to your syslog.



       # echo 1 >/proc/sys/net/ipv4/conf/<interfacename>/log_martians



  FIXME: is setting the conf/{default,all}/* files enough? - martijn


  12.2.  Obscure settings

  Ok, there are a lot of parameters which can be modified. We try to
  list them all. Also documented (partly) in Documentation/ip-
  sysctl.txt.

  Some of these settings have different defaults based on wether you
  answered 'Yes' to 'Configure as router and not host' while compiling
  your kernel.


  12.2.1.  Generic ipv4

  As a generic note, most rate limiting features don't work on loopback,
  so don't test them locally.

     /proc/sys/net/ipv4/icmp_destunreach_rate
        FIXME: fill this in

     /proc/sys/net/ipv4/icmp_echo_ignore_all
        FIXME: fill this in

     /proc/sys/net/ipv4/icmp_echo_ignore_broadcasts [Useful]
        If you ping the broadcast address of a network, all hosts are
        supposed to respond. This makes for a dandy denial-of-service
        tool. Set this to 1 to ignore these broadcast messages.

     /proc/sys/net/ipv4/icmp_echoreply_rate
        FIXME: fill this in

     /proc/sys/net/ipv4/icmp_ignore_bogus_error_responses
        FIXME: fill this in

     /proc/sys/net/ipv4/icmp_paramprob_rate
        FIXME: fill this in

     /proc/sys/net/ipv4/icmp_timeexceed_rate
        This the famous cause of the 'Solaris middle star' in
        traceroutes. Limits number of ICMP Time Exceeded messages sent.
        FIXME: Units of these rates - either I'm stupid, or this just
        doesn't work

     /proc/sys/net/ipv4/igmp_max_memberships
        FIXME: fill this in

     /proc/sys/net/ipv4/inet_peer_gc_maxtime
        FIXME: fill this in

     /proc/sys/net/ipv4/inet_peer_gc_mintime
        FIXME: fill this in

     /proc/sys/net/ipv4/inet_peer_maxttl
        FIXME: fill this in

     /proc/sys/net/ipv4/inet_peer_minttl
        FIXME: fill this in

     /proc/sys/net/ipv4/inet_peer_threshold
        FIXME: fill this in

     /proc/sys/net/ipv4/ip_autoconfig
        FIXME: fill this in

     /proc/sys/net/ipv4/ip_default_ttl
        Time To Live of packets. Set to a safe 64. Raise it if you have
        a huge network. Don't do so for fun - routing loops cause much
        more damage that way. You might even consider lowering it in
        some circumstances.

     /proc/sys/net/ipv4/ip_dynaddr
        You need to set this if you use dial-on-demand with a dynamic
        interface address. Once your demand interface comes up, any
        queued packets will be rebranded to have the right address. This
        solves the problem that the connection that brings up your
        interface itself does not work, but the second try does.

     /proc/sys/net/ipv4/ip_forward
        If the kernel should attempt to forward packets. Off by default
        for hosts, on by default when configured as a router.

     /proc/sys/net/ipv4/ip_local_port_range
        Range of local ports for outgoing connections. Actually quite
        small by default, 1024 to 4999.

     /proc/sys/net/ipv4/ip_no_pmtu_disc
        Set this if you want to disable Path MTU discovery - a technique
        to determince the largest Maximum Transfer Unit possible on you
        path.

     /proc/sys/net/ipv4/ipfrag_high_thresh
        FIXME: fill this in

     /proc/sys/net/ipv4/ipfrag_low_thresh
        FIXME: fill this in

     /proc/sys/net/ipv4/ipfrag_time
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_abort_on_overflow
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_fin_timeout
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_keepalive_intvl
        FIXME: fill this in


     /proc/sys/net/ipv4/tcp_keepalive_probes
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_keepalive_time
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_max_orphans
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_max_syn_backlog
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_max_tw_buckets
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_orphan_retries
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_retrans_collapse
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_retries1
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_retries2
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_rfc1337
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_sack
        Use Selective ACK which can be used to signify that only a
        single packet is missing - therefore helping fast recovery.

     /proc/sys/net/ipv4/tcp_stdurg
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_syn_retries
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_synack_retries
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_timestamps
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_tw_recycle
        FIXME: fill this in

     /proc/sys/net/ipv4/tcp_window_scaling
        TCP/IP normally allows windows up to 65535 bytes big. For really
        fast networks, this may not be enough. The window scaling
        options allows for almost gigabyte windows, which is good for
        high bandwidth*delay products.


  12.2.2.  Per device settings

  DEV can either stand for a real interface, or for 'all' or 'default'.
  Default also changes settings for interfaces yet to be created.

     /proc/sys/net/ipv4/conf/DEV/accept_redirects
        If a router decides that you are using it for a wrong purpose
        (ie, it needs to resend your packet on the same interface), it
        will send us a ICMP Redirect. This is a slight security risk
        however, so you may want to turn it off, or use secure
        redirects.

     /proc/sys/net/ipv4/conf/DEV/accept_source_route
        Not used very much anymore. You used to be able to give a packet
        a list of IP addresses it should visit on its way. Linux can be
        made to honor this IP option.

     /proc/sys/net/ipv4/conf/DEV/bootp_relay
        FIXME: fill this in

     /proc/sys/net/ipv4/conf/DEV/forwarding
        FIXME:

     /proc/sys/net/ipv4/conf/DEV/log_martians
        See the section on reverse path filters.

     /proc/sys/net/ipv4/conf/DEV/mc_forwarding
        If we do multicast forwarding on this interface

     /proc/sys/net/ipv4/conf/DEV/proxy_arp
        FIXME: fill this in

     /proc/sys/net/ipv4/conf/DEV/rp_filter
        See the section on reverse path filters.

     /proc/sys/net/ipv4/conf/DEV/secure_redirects
        FIXME: fill this in

     /proc/sys/net/ipv4/conf/DEV/send_redirects
        If we send the above mentioned redirects.

     /proc/sys/net/ipv4/conf/DEV/shared_media
        FIXME: fill this in

     /proc/sys/net/ipv4/conf/DEV/tag
        FIXME: fill this in



  12.2.3.  Neighbor pollicy

  Dev can either stand for a real interface, or for 'all' or 'default'.
  Default also changes settings for interfaces yet to be created.

     /proc/sys/net/ipv4/neigh/DEV/anycast_delay
        FIXME: fill this in

     /proc/sys/net/ipv4/neigh/DEV/app_solicit
        FIXME: fill this in

     /proc/sys/net/ipv4/neigh/DEV/base_reachable_time
        FIXME: fill this in

     /proc/sys/net/ipv4/neigh/DEV/delay_first_probe_time
        FIXME: fill this in

     /proc/sys/net/ipv4/neigh/DEV/gc_stale_time
        FIXME: fill this in

     /proc/sys/net/ipv4/neigh/DEV/locktime
        FIXME: fill this in

     /proc/sys/net/ipv4/neigh/DEV/mcast_solicit
        FIXME: fill this in


     /proc/sys/net/ipv4/neigh/DEV/proxy_delay
        FIXME: fill this in

     /proc/sys/net/ipv4/neigh/DEV/proxy_qlen
        FIXME: fill this in

     /proc/sys/net/ipv4/neigh/DEV/retrans_time
        FIXME: fill this in

     /proc/sys/net/ipv4/neigh/DEV/ucast_solicit
        FIXME: fill this in

     /proc/sys/net/ipv4/neigh/DEV/unres_qlen
        FIXME: fill this in



  12.2.4.  Routing settings


     /proc/sys/net/ipv4/route/error_burst
        FIXME: fill this in

     /proc/sys/net/ipv4/route/error_cost
        FIXME: fill this in

     /proc/sys/net/ipv4/route/flush
        FIXME: fill this in

     /proc/sys/net/ipv4/route/gc_elasticity
        FIXME: fill this in

     /proc/sys/net/ipv4/route/gc_interval
        FIXME: fill this in

     /proc/sys/net/ipv4/route/gc_min_interval
        FIXME: fill this in

     /proc/sys/net/ipv4/route/gc_thresh
        FIXME: fill this in

     /proc/sys/net/ipv4/route/gc_timeout
        FIXME: fill this in

     /proc/sys/net/ipv4/route/max_delay
        FIXME: fill this in

     /proc/sys/net/ipv4/route/max_size
        FIXME: fill this in

     /proc/sys/net/ipv4/route/min_adv_mss
        FIXME: fill this in

     /proc/sys/net/ipv4/route/min_delay
        FIXME: fill this in

     /proc/sys/net/ipv4/route/min_pmtu
        FIXME: fill this in

     /proc/sys/net/ipv4/route/mtu_expires
        FIXME: fill this in

     /proc/sys/net/ipv4/route/redirect_load
        FIXME: fill this in


     /proc/sys/net/ipv4/route/redirect_number
        FIXME: fill this in

     /proc/sys/net/ipv4/route/redirect_silence
        FIXME: fill this in



  13.  Backbone applications of traffic control

  This chapter is meant as an introduction to backbone routing, which
  often involves >100 megabit bandwidths, which requires a different
  approach then your ADSL modem at home.


  13.1.  Router queues

  The normal behaviour of router queues on the Internet is called tail-
  drop.  Tail-drop works by queueing up to a certain amount, then
  dropping all traffic that 'spills over'. This is very unfair, and also
  leads to retransmit synchronisation. When retransmit synchronisation
  occurs, the sudden burst of drops from a router that has reached its
  fill will cause a delayed burst of retransmits, which will over fill
  the congested router again.

  In order to cope with transient congestion on links, backbone routers
  will often implement large queues. Unfortunately, while these queues
  are good for throughput, they can substantially increase latency and
  cause TCP connections to behave very bursty during congestion.

  These issues with tail-drop are becoming increasingly troublesome on
  the Internet because the use of network unfriendly applications is
  increasing.  The Linux kernel offers us RED, short for Random Early
  Detect.

  RED isn't a cure-all for this, applications which inappropriately fail
  to implement exponential backoff still get an unfair share of the
  bandwidth, however, with RED they do not cause as much harm to the
  throughput and latency of other connections.

  RED statistically drops packets from flows before it reaches its hard
  limit. This causes a congested backbone link to slow more gracefully,
  and prevents retransmit synchronisation. This also helps TCP find its
  'fair' speed faster by allowing some packets to get dropped sooner
  keeping queue sizes low and latency under control. The probability of
  a packet being dropped from a particular connection is proportional to
  its bandwidth usage rather then the number of packets it transmits.

  RED is a good queue for backbones, where you can't afford the
  complexity of per-session state tracking needed by fairness queueing.

  In order to use RED, you must decide on three parameters: Min, Max,
  and burst. Min sets the minimum queue size in bytes before dropping
  will begin, Max is a soft maximum that the algorithm will attempt to
  stay under, and burst sets the maximum number of packets that can
  'burst through'.

  You should set the min by calculating that highest acceptable base
  queueing latency you wish, and multiply it by your bandwidth. For
  instance, on my 64kbit/s ISDN link, I might want a base queueing
  latency of 200ms so I set min to 1600 bytes. Setting min too small
  will degrade throughput and too large will degrade latency. Setting a
  small min is not a replacement for reducing the MTU on a slow link to
  improve interactive response.


  You should make max at least twice min to prevent synchronisation. On
  slow links with small min's it might be wise to make max perhaps four
  or more times large then min.

  Burst controls how the RED algorithm responds to bursts. Burst must be
  set large then min/avpkt. Experimentally, I've found
  (min+min+max)/(3*avpkt) to work okay.

  Additionally, you need to set limit and avpkt. Limit is a safety
  value, after there are limit bytes in the queue, RED 'turns into'
  tail-drop. I typical set limit to eight times max. Avpkt should be
  your average packet size. 1000 works okay on high speed Internet links
  with a 1500byte MTU.

  Read the paper on RED queueing
  <http://www.aciri.org/floyd/papers/red/red.html> by Sally Floyd and
  Van Jacobson for technical information.

  FIXME: more needed. This means *you* greg :-) - ahu



  14.  Shaping Cookbook

  This section contains 'cookbook' entries which may help you solve
  problems.  A cookbook is no replacement for understanding however, so
  try and comprehend what is going on.


  14.1.  Running multiple sites with different SLAs

  You can do this in several ways. Apache has some support for this with
  a module, but we'll show how Linux can do this for you, and do so for
  other services as well. These commands are stolen from a presentation
  by Jamal Hadi that's referenced below.

  Let's say we have two customers, with http, ftp and streaming audio,
  and we want to sell them a limited amount of bandwidth. We do so on
  the server itself.

  Customer A should have at most 2 megabits, cusomer B has paid for 5
  megabits. We separate our customers by creating virtual IP addresses
  on our server.



       # ip address add 188.177.166.1 dev eth0
       # ip address add 188.177.166.2 dev eth0



  It is up to you to attach the different servers to the right IP
  address. All popular daemons have support for this.

  We first attach a CBQ qdisc to eth0:


       # tc qdisc add dev eth0 root handle 1: bandwidth 10Mbit cell 8 avpkt 1000 \
         mpu 64



  We then create classes for our customers:

       # tc class add dev eth0 parent 1:0 classid 1:1 cbq bandwidth 10Mbit rate \
         2MBit avpkt 1000 prio 5 bounded isolated allot 1514 weight 1 maxburst 21
       # tc class add dev eth0 parent 1:0 classid 1:2 cbq bandwidth 10Mbit rate \
         5Mbit avpkt 1000 prio 5 bounded isolated allot 1514 weight 1 maxburst 21



  Then we add filters for our two classes:


       ##FIXME: Why this line, what does it do?, what is a divisor?:
       ##FIXME: A divisor has something to do with a hash table, and the number of
       ##       buckets - ahu
       # tc filter add dev eth0 parent 1:0 protocol ip prio 5 handle 1: u32 divisor 1
       # tc filter add dev eth0 parent 1:0 prio 5 u32 match ip src 188.177.166.1
         flowid 1:1
       # tc filter add dev eth0 parent 1:0 prio 5 u32 match ip src 188.177.166.2
         flowid 1:2



  And we're done.

  FIXME: why no token bucket filter? is there a default pfifo_fast
  fallback somewhere?


  14.2.  Protecting your host from SYN floods

  From Alexeys iproute documentation, adapted to netfilter and with more
  plausible paths. If you use this, take care to adjust the numbers to
  reasonable values for your system.

  If you want to protect an entire network, skip this script, which is
  best suited for a single host.



  #! /bin/sh -x
  #
  # sample script on using the ingress capabilities
  # this script shows how one can rate limit incoming SYNs
  # Useful for TCP-SYN attack protection. You can use
  # IPchains to have more powerful additions to the SYN (eg
  # in addition the subnet)
  #
  #path to various utilities;
  #change to reflect yours.
  #
  TC=/sbin/tc
  IP=/sbin/ip
  IPTABLES=/sbin/iptables
  INDEV=eth2
  #
  # tag all incoming SYN packets through $INDEV as mark value 1
  ############################################################
  $iptables -A PREROUTING -i $INDEV -t mangle -p tcp --syn \
    -j MARK --set-mark 1
  ############################################################
  #
  # install the ingress qdisc on the ingress interface
  ############################################################
  $TC qdisc add dev $INDEV handle ffff: ingress
  ############################################################

  #
  #
  # SYN packets are 40 bytes (320 bits) so three SYNs equals
  # 960 bits (approximately 1kbit); so we rate limit below
  # the incoming SYNs to 3/sec (not very sueful really; but
  #serves to show the point - JHS
  ############################################################
  $TC filter add dev $INDEV parent ffff: protocol ip prio 50 handle 1 fw \
  police rate 1kbit burst 40 mtu 9k drop flowid :1
  ############################################################


  #
  echo "---- qdisc parameters Ingress  ----------"
  $TC qdisc ls dev $INDEV
  echo "---- Class parameters Ingress  ----------"
  $TC class ls dev $INDEV
  echo "---- filter parameters Ingress ----------"
  $TC filter ls dev $INDEV parent ffff:

  #deleting the ingress qdisc
  #$TC qdisc del $INDEV ingress



  14.3.  Ratelimit ICMP to prevent dDoS

  Recently, distributed denial of service attacks have become a major
  nuisance on the internet. By properly filtering and ratelimiting your
  network, you can both prevent becoming a casualty or the cause of
  these attacks.

  You should filter your networks so that you do not allow non-local IP
  source addressed packets to leave your network. This stops people from
  anonymously sending junk to the internet.



  Rate limiting goes much as shown earlier. To refresh your memory, our
  ASCIIgram again:



       [The Internet] ---<E3, T3, whatever>--- [Linux router] --- [Office+ISP]
                                             eth1          eth0



  We first set up the prerequisite parts:



       # tc qdisc add dev eth0 root handle 10: cbq bandwidth 10Mbit avpkt 1000
       # tc class add dev eth0 parent 10:0 classid 10:1 cbq bandwidth 10Mbit rate \
         10Mbit allot 1514 prio 5 maxburst 20 avpkt 1000



  If you have 100Mbit, or more, interfaces, adjust these numbers. Now
  you need to determine how much ICMP traffic you want to allow. You can
  perform measurements with tcpdump, by having it write to a file for a
  while, and seeing how much ICMP passes your network. Do not forget to
  raise the snapshot length!

  If measurement is impractical, you might want to choose 5% of your
  available bandwidth. Let's set up our class:


       # tc class add dev eth0 parent 10:1 classid 10:100 cbq bandwidth 10Mbit rate \
         100Kbit allot 1514 weight 800Kbit prio 5 maxburst 20 avpkt 250 \
         bounded



  This limits at 100Kbit. Now we need a filter to assign ICMP traffic to
  this class:


       # tc filter add dev eth0 parent 10:0 protocol ip prio 100 u32 match ip
         protocol 1 0xFF flowid 10:100



  14.4.  Prioritising interactive traffic

  If lots of data is coming down your link, or going up for that matter,
  and you are trying to do some maintenance via telnet or ssh, this may
  not go too well. Other packets are blocking your keystrokes. Wouldn't
  it be great if there were a way for your interactive packets to sneak
  past the bulk traffic? Linux can do this for you!

  As before, we need to handle traffic going both ways. Evidently, this
  works best if there are Linux boxes on both ends of your link,
  although other UNIX's are able to do this. Consult your local
  Solaris/BSD guru for this.

  The standard pfifo_fast scheduler has 3 different 'bands'. Traffic in
  band 0 is transmitted first, after which traffic in band 1 and 2 gets
  considered.  It is vital that our interactive traffic be in band 0!
  We blatantly adapt from the (soon to be obsolete) ipchains HOWTO:

  There are four seldom-used bits in the IP header, called the Type of
  Service (TOS) bits. They effect the way packets are treated; the four
  bits are "Minimum Delay", "Maximum Throughput", "Maximum Reliability"
  and "Minimum Cost". Only one of these bits is allowed to be set. Rob
  van Nieuwkerk, the author of the ipchains TOS-mangling code, puts it
  as follows:


       Especially the "Minimum Delay" is important for me. I switch it on for
       "interactive" packets in my upstream (Linux) router. I'm behind a 33k6
       modem link. Linux prioritises packets in 3 queues. This way I get
       acceptable interactive performance while doing bulk downloads at the
       same time.


  The most common use is to set telnet & ftp control connections to
  "Minimum Delay" and FTP data to "Maximum Throughput". This would be
  done as follows, on your upstream router:



       # iptables -A PREROUTING -t mangle -p tcp --sport telnet \
         -j TOS --set-tos Minimize-Delay
       # iptables -A PREROUTING -t mangle -p tcp --sport ftp \
         -j TOS --set-tos Minimize-Delay
       # iptables -A PREROUTING -t mangle -p tcp --sport ftp-data \
         -j TOS --set-tos Maximize-Throughput



  Now, this only works for data going from your telnet foreign host to
  your local computer. The other way around appears to be done for you,
  ie, telnet, ssh & friends all set the TOS field on outgoing packets
  automatically.

  Should you have a client that does not do this, you can always do it
  with netfilter. On your local box:



       # iptables -A OUTPUT -t mangle -p tcp --dport telnet \
         -j TOS --set-tos Minimize-Delay
       # iptables -A OUTPUT -t mangle -p tcp --dport ftp \
         -j TOS --set-tos Minimize-Delay
       # iptables -A OUTPUT -t mangle -p tcp --dport ftp-data \
         -j TOS --set-tos Maximize-Throughput



  15.  Advanced Linux Routing

  This section is for all you people who either want to understand why
  the whole system works or have a configuration that's so bizarre that
  you need the low down to make it work.

  This section is completely optional. It's quite possible that this
  section will be quite complex and really not intended for normal
  users. You have been warned.

  FIXME: Decide what really need to go in here.

  15.1.  How does packet queueing really work?

  This is the low-down on how the packet queueing system really works.

  Lists the steps the kernel takes to classify a packet, etc...

  FIXME: Write this.


  15.2.  Advanced uses of the packet queueing system

  Go through Alexeys extremely tricky example involving the unused bits
  in the TOS field.

  FIXME: Write this.


  15.3.  Other packet shaping systems

  I'd like to include a brief description of other packet shaping
  systems in other operating systems and how they compare to the Linux
  one. Since Linux is one of the few OSes that has a completely original
  (non-BSD derived) TCP/IP stack, I think it would be useful to see how
  other people do it.

  Unfortunately I have no experiene with other systems so cannot write
  this.

  FIXME: Anyone? - Martijn


  16.  Dynamic routing - OSPF and BGP

  Once your network starts to get really big, or you start to consider
  'the internet' as your network, you need tools which dynamically route
  your data.  Sites are often connected to each other with multiple
  links, and more are popping up all the time.

  The Internet has mostly standardised on OSPF and BGP4 (rfc1771). Linux
  supports both, by way of gated and zebra

  While currently not within the scope of this document, we would like
  to point you to the definitive works:

  Overview:

  Cisco Systems Designing large-scale IP internetworks
  <http://www.cisco.com/univercd/cc/td/doc/cisintwk/idg4/nd2003.htm>



  For OSPF:

  Moy, John T.  "OSPF.  The anatomy of an Internet routing protocol"
  Addison Wesley. Reading, MA. 1998.

  Halabi has also written a good guide to OSPF routing design, but this
  appears to have been dropped from the Cisco web site.


  For BGP:

  Halabi, Bassam "Internet routing architectures" Cisco Press (New
  Riders Publishing). Indianapolis, IN. 1997.


  also

  Cisco Systems

  Using the Border Gateway Protocol for interdomain routing
  <http://www.cisco.com/univercd/cc/td/doc/cisintwk/ics/icsbgp4.htm>


  Although the examples are Cisco-specific, they are remarkably similar
  to the configuration language in Zebra :-)

  17.  Further reading


     http://snafu.freedom.org/linux2.2/iproute-notes.html
        <http://snafu.freedom.org/linux2.2/iproute-notes.html>
        Contains lots of technical information, comments from the kernel

     http://www.davin.ottawa.on.ca/ols/
        <http://www.davin.ottawa.on.ca/ols/>
        Slides by Jamal Hadi, one of the authors of Linux traffic
        control

     http://defiant.coinet.com/iproute2/ip-cref/ <http://defi
        ant.coinet.com/iproute2/ip-cref/>
        HTML version of Alexeys LaTeX documentation - explains part of
        iproute2 in great detail

     http://www.aciri.org/floyd/cbq.html
        <http://www.aciri.org/floyd/cbq.html>
        Sally Floyd has a good page on CBQ, including her original
        papers. None of it is Linux specific, but it does a fair job
        discussing the theory and uses of CBQ.  Very technical stuff,
        but good reading for those so inclined.


     http://ceti.pl/%7ekravietz/cbq/NET4_tc.html
        <http://ceti.pl/%7ekravietz/cbq/NET4_tc.html>
        Yet another HOWTO, this time in Polish! You can copy/paste
        command lines however, they work just the same in every
        language. The author is cooperating with us and may soon author
        sections of this HOWTO.


     Differentiated Services on Linux <http://snafu.free
        dom.org/linux2.2/docs/draft-almesberger-wajhak-diffserv-
        linux-00.txt>
        Discussion on how to use Linux in a diffserv compliant
        environment. Pretty far removed from your everyday routing
        needs, but very interesting none the less. We may include a
        section on this at a later date.


     IOS Committed Access Rate <http://www.cisco.com/uni
        vercd/cc/td/doc/product/software/ios111/cc111/car.htm>

        >From the helpful folks of Cisco who have the laudable habit of
        putting their documentation online. Cisco syntax is different
        but the concepts are the same, except that we can do more and do
        it without routers the price of cars :-)


     TCP/IP Illustrated, volume 1, W. Richard Stevens, ISBN
        0-201-63346-9
        Required reading if you truly want to understand TCP/IP.
        Entertaining as well.
  18.  Acknowledgements


  It is our goal to list everybody who has contributed to this HOWTO, or
  helped us demistify how things work. While there are currently no
  plans for a Netfilter type scoreboard, we do like to recognise the
  people who are helping.


    Jamal Hadi <hadi%cyberus.ca>

    Nadeem Hasan <nhasan@usa.net>

    Jason Lunz <j@cc.gatech.edu>

    Alexey Mahotkin <alexm@formulabez.ru>

    Pawel Krawczyk <kravietz%alfa.ceti.pl>

    Wim van der Most

    Glen Turner <glen.turner%aarnet.edu.au>

    Song Wang <wsong@ece.uci.edu>



  Brief Introduction to Alpha Systems and Processors
  Neal Crook, Digital Equipment (Editor: David Mosberger
  <mailto:davidm@azstarnet.com>)
  V0.11, 6 June 1997

  This document is a brief overview of existing Alpha CPUs, chipsets and
  systems. It has something of a hardware bias, reflecting my own area
  of expertese. Although I am an employee of Digital Equipment Corpora-
  tion, this is not an official statement by Digital and any opinions
  expressed are mine and not Digital's.
  ______________________________________________________________________

  Table of Contents


  1. What is Alpha

  2. What is Digital Semiconductor

  3. Alpha CPUs

  4. 21064 performance vs 21066 performance

  5. A Few Notes On Clocking

  6. The chip-sets

  7. The Systems

  8. Bytes and all that stuff

  9. PALcode and all that stuff

  10. Porting

  11. More Information

  12. References



  ______________________________________________________________________

  1.  What is Alpha

  "Alpha" is the name given to Digital's 64-bit RISC architecture. The
  Alpha project in Digital began in mid-1989, with the goal of providing
  a high-performance migration path for VAX customers. This was not the
  first RISC architecture to be produced by Digital, but it was the
  first to reach the market. When Digital announced Alpha, in March
  1992, it made the decision to enter the merchant semicondutor market
  by selling Alpha microprocessors.


  Alpha is also sometimes referred to as Alpha AXP, for obscure and
  arcane reasons that aren't worth persuing. Suffice it to say that they
  are one and the same.


  2.  What is Digital Semiconductor

  Digital Semiconductor <http://www.digital.com/info/semiconductor/>
  (DS) is the business unit within Digital Equipment Corporation
  (Digital - we don't like the name DEC) that sells semiconductors on
  the merchant market. Digital's products include CPUs, support
  chipsets, PCI-PCI bridges and PCI peripheral chips for comms and
  multimedia.


  3.  Alpha CPUs

  There are currently 2 generations of CPU core that implement the Alpha
  architecture:


  o  EV4

  o  EV5


  Opinions differ as to what "EV" stands for (Editor's note: the true
  answer is of course "Electro Vlassic" ``[1]''), but the number
  represents the first generation of Digital's CMOS technology that the
  core was implemented in. So, the EV4 was originally implemented in
  CMOS4. As time goes by, a CPU tends to get a mid-life performance kick
  by being optically shrunk into the next generation of CMOS process.
  EV45, then, is the EV4 core implemented in CMOS5 process. There is a
  big difference between shrinking a design into a particular technology
  and implementing it from scratch in that technology (but I don't want
  to go into that now). There are a few other wildcards in here: there
  is also a CMOS4S (optical shrink in CMOS4) and a CMOS5L.


  True technophiles will be interested to know that CMOS4 is a 0.75
  micron process, CMOS5 is a 0.5 micron process and CMOS6 is a 0.35
  micron process.


  To map these CPU cores to chips we get:


     21064-150,166
        EV4 (originally), EV4S (now)

     21064-200
        EV4S

     21064A-233,275,300
        EV45

     21066
        LCA4S (EV4 core, with EV4 FPU)

     21066A-233
        LCA45 (EV4 core, but with EV45 FPU)

     21164-233,300,333
        EV5

     21164A-417
        EV56

     21264
        EV6 <http://www.mdronline.com/report/articles/21264/21264.html>



  The EV4 core is a dual-issue (it can issue 2 instructions per CPU
  clock) superpipelined core with integer unit, floating point unit and
  branch prediction. It is fully bypassed and has 64-bit internal data
  paths and tightly coupled 8Kbyte caches, one each for Instruction and
  Data. The caches are write-through (they never get dirty).
  The EV45 core has a couple of tweaks to the EV4 core: it has a
  slightly improved floating point unit, and 16KB caches, one each for
  Instruction and Data (it also has cache parity).  (Editor's note: Neal
  Crook indicated in a separate mail that the changes to the floating
  point unit (FPU) improve the performance of the divider.  The EV4 FPU
  divider takes 34 cycles for a single-precision divide and 63 cycles
  for a double-precision divide (non data-dependent).  In constrast, the
  EV45 divider takes typically 19 cycles (34 cycles max) for single-
  precision and typically 29 cycles (63 cycles max) for a double-
  precision division (data-dependent).)


  The EV5 core is a quad-issue core, also superpipelined, fully bypassed
  etc etc. It has tightly-coupled 8Kbyte caches, one each for I and D.
  These caches are write-through. It also has a tightly-coupled 96Kbyte
  on-chip second-level cache (the Scache) which is 3-way set associative
  and write-back (it can be dirty). The EV4->EV5 performance increase is
  better than just the increase achieved by clock speed improvements. As
  well as the bigger caches and quad issue, there are microarchitectural
  improvements to reduce producer/consumer latencies in some paths.


  The EV56 core is fundamentally the same microarchitecture as the EV5,
  but it adds some new instructions for 8 and 16-bit loads and stores
  (see Section ``Bytes and all that stuff''). These are primarily
  intended for use by device drivers. The EV56 core is implemented in
  CMOS6, which is a 2.0V process.


  The 21064 was anounced in March 1992. It uses the EV4 core, with a
  128-bit bus interface. The bus interface supports the 'easy'
  connection of an external second-level cache, with a block size of
  256-bits (2 data beats on the bus). The Bcache timing is completely
  software configurable. The 21064 can also be configured to use a
  64-bit external bus, (but I'm not sure if any shipping system uses
  this mode). The 21064 does not impose any policy on the Bcache, but it
  is usually configured as a write-back cache. The 21064 does contain
  hooks to allow external hardware to maintain cache coherence with the
  Bcache and internal caches, but this is hairy.


  The 21066 uses the EV4 core and integrates a memory controller and PCI
  host bridge. To save pins, the memory controller has a 64-bit data bus
  (but the internal caches have a block size of 256 bits, just like the
  21064, therefore a block fill takes 4 beats on the bus). The memory
  controller supports an external Bcache and external DRAMs. The timing
  of the Bcache and DRAMs is completely software configurable, and can
  be controlled to the resolution of the CPU clock period. Having a
  4-beat process to fill a cache block isn't as bad as it sounds because
  the DRAM access is done in page mode. Unfortunately, the memory
  controller doesn't support any of the new esoteric DRAMs (SDRAM, EDO
  or BEDO) or synchronous cache RAMs. The PCI bus interface is fully
  rev2.0 compliant and runs at upto 33MHz.


  The 21164 has a 128-bit data bus and supports split reads, with upto 2
  reads outstanding at any time (this allows 100% data bus utilisation
  under best-case dream-on conditions, i.e., you can theoretically
  transfer 128-bits of data on every bus clock). The 21164 supports easy
  connection of an external 3-rd level cache (Bcache) and has all the
  hooks to allow external systems to maintain full cache coherence with
  all caches. Therefore, symmetric multiprocessor designs are 'easy'.


  The 21164A was announced in October, 1995. It uses the EV56 core. It
  is nominally pin-compatible with the 21164, but requires split power
  rails; all of the power pins that were +3.3V power on the 21164 have
  now been split into two groups; one group provided 2.0V power to the
  CPU core, the other group supplies 3.3V to the I/O cells. Unlike older
  implementations, the 21164 pins are not 5V-tolerant. The end result of
  this change is that 21164 systems are, in general, not upgradeable to
  the 21164A (though note that it would be relatively straightforward to
  design a 21164A system that could also accommodate a 21164). The
  21164A also has a couple of new pins to support the new 8 and 16-bit
  loads and stores. It also improves the 21164 support for using
  synchronus SRAMs to implement the external Bcache.



  4.  21064 performance vs 21066 performance

  The 21064 and the 21066 have the same (EV4) CPU core. If the same
  program is run on a 21064 and a 21066, at the same CPU speed, then the
  difference in performance comes only as a result of system
  Bcache/memory bandwidth. Any code thread that has a high hit-rate on
  the internal caches will perform the same. There are 2 big performance
  killers:


  1. Code that is write-intensive. Even though the 21064 and the 21066
     have write buffers to swallow some of the delays, code that is
     write-intensive will be throttled by write bandwidth at the system
     bus. This arises because the on-chip caches are write-through.

  2. Code that wants to treat floats as integers. The Alpha architecture
     does not allow register-register transfers from integer registers
     to floating point registers. Such a conversion has to be done via
     memory (And therefore, because the on-chip caches are write-
     through, via the Bcache).  (Editor's note: it seems that both the
     EV4 and EV45 can perform the conversion through the primary data
     cache (Dcache), provided that the memory is cached already.  In
     such a case, the store in the conversion sequence will update the
     Dcache and the subsequent load is, under certain circumstances,
     able to read the updated d-cache value, thus avoiding a costly
     roundtrip to the Bcache.  In particular, it seems best to execute
     the stq/ldt or stt/ldq instructions back-to-back, which is somewhat
     counter-intuitive.)


  If you make the same comparison between a 21064A and a 21066A, there
  is an additional factor due to the different Icache and Dcache sizes
  between the two chips.


  Now, the 21164 solves both these problems: it achieve much higher
  system bus bandwidths (despite having the same number of signal pins -
  yes, I know it's got about twice as many pins as a 21064, but all
  those extra ones are power and ground! (yes, really!!))  and it has
  write-back caches. The only remaining problem is the answer to the
  question "how much does it cost?"



  5.  A Few Notes On Clocking

  All of the current Alpha CPUs use high-speed clocks, because their
  microarchitectures have been designed as so-called short-tick designs.
  None of the sytem busses have to run at horrendous speeds as a result
  though:


  o  on the 21066(A), 21064(A), 21164 the off-chip cache (Bcache) timing
     is completely programmable, to the resolution of the CPU clock. For
     example, on a 275MHz CPU, the Bcache read access time can be
     controller with a resolution of 3.6ns

  o  on the 21066(A), the DRAM timing is completely programmable, to the
     resolution of the CPU clock (not the PCI clock, the CPU clock).

  o  on the 21064(A), 21164(A), the system bus frequency is a sub-
     multiple of the CPU clock frequency. Most of the 21064 motherboards
     use a 33MHz system bus clock.

  o  Systems that use the 21066 can run the PCI at any frequency
     relative to the CPU. Generally, the PCI runs at 33MHz.

  o  Systems that use the APECs chipset (see Section ``'') always have
     their CPU system bus equal to their PCI bus frequency. This means
     that both busses tends to run at either 25MHz or 33MHz (since these
     are the frequencies that scale up to match the CPU frequencies). On
     APECs systems, the DRAM controller timings are software
     programmable in terms of the CPU system bus frequency

  Aside: someone suggested that they were getting bad performance on a
  21066 because the 21066 memory controller was only running at 33MHz.
  Actually, it's the superfast 21064A systems that have memory
  controllers that 'only' run at 33MHz.



  6.  The chip-sets

  DS sells two CPU support chipsets. The 2107x chipset (aka APECS) is a
  21064(A) support chiset. The 2117x chipset (aka ALCOR) is a 21164
  support chipset. There will also be 2117xA chipset (aka ALCOR 2) as a
  21164A support chipset.


  Both chipsets provide memory controllers and PCI host bridges for
  their CPU. APECS provides a 32-bit PCI host bridge, ALCOR provides a
  64-bit PCI host bridge which (in accordance with the requirements of
  the PCI spec) can support both 32-bit and 64-bit PCI devices.


  APECS consists of 6, 208-pin chips (4, 32-bit data slices (DECADE), 1
  system controller (COMANCHE), 1 PCI controller (EPIC)). It provides a
  DRAM controller (128-bit memory bus) and a PCI interface. It also does
  all the work to maintain memory coherence when a PCI device DMAs into
  (or out of) memory.


  ALCOR consists of 5 chips (4, 64-bit data slices (Data Switch, DSW) -
  208-pin PQFP and 1 control (Control, I/O Address, CIA) - a 383 pin
  plastic PGA).  It provides a DRAM controller (256-bit memory bus) and
  a PCI interface. It also does all the work required to support an
  external Bcache and to maintain memory coherence when a PCI device
  DMAs into (or out of) memory.


  There is no support chipset for the 21066, since the memory controller
  and PCI host bridge functionality are integrated onto the chip.


  7.  The Systems

  The applications engineering group in DS produces example designs
  using the CPUs and support chipsets. These are typically PC-AT size
  motherboards, with all the functionality that you'd typically find on
  a high-end Pentium motherboard. Originally, these example designs were
  intended to be used as starting points for third-parties to produce
  motherboard designs from. These first-generation designs were called
  Evaluation Boards (EBs). As the amount of engineering required to
  build a motherboard has increased (due to higher-speed clocks and the
  need to meet RF emission and susceptibility regulations) the emphasis
  has shifted towards providing motherboards that are suitable for
  volume manufacture.


  Digital's system groups have produced several generations of machines
  using Alpha processors. Some of these systems use support logic that
  is designed by the systems groups, and some use commodity chipsets
  from DS. In some cases, systems use a combination of both.


  Various third-parties build systems using Alpha processors. Some of
  these companies design systems from scratch, and others use DS support
  chipsets, clone/modify DS example designs or simply package systems
  using build and tested boards from DS.


  The EB64: Obsolete design using 21064 with memory controller
  implemented using programmable logic. I/O provided by using
  programmable logic to interface a 486<->ISA bridge chip. On-board
  Ethernet, SuperI/O (2S, 1P, FD), Ethernet and ISA. PC-AT size. Runs
  from standard PC power supply.


  The EB64+: Uses 21064 or 21064A and APECs. Has ISA and PCI expansion
  (3 ISA, 2 PCI, one pair are on a shared slot). Supports 36-bit DRAM
  SIMs. ISA bus generated by Intel SaturnI/O PCI-ISA bridge. On-board
  SCSI (NCR 810 on PCI) Ethernet (Digital 21040), KBD, MOUSE (PS2
  style), SuperI/O (2S, 1P, FD), RTC/NVRAM. Boot ROM is EPROM. PC-AT
  size. Runs from standard PC power supply.


  The EB66: Uses 21066 or 21066A. I/O sub-system is identical to EB64+.
  Baby PC-AT size. Runs from standard PC power supply. The EB66
  schematic was published as a marketing poster advertising the 21066 as
  "the first microprocessor in the world with embedded PCI" (for trivia
  fans: there are actually 2 versions of this poster - I drew the
  circuits and wrote the spiel for the first version, and some Americans
  mauled the spiel for the second version)


  The EB164: Uses 21164 and ALCOR. Has ISA and PCI expansion (3 ISA
  slots, 2 64-bit PCI slots (one is shared with an ISA slot) and 2
  32-bit PCI slots.  Uses plus-in Bcache SIMMs. I/O sub-system provides
  SuperI/O (2S, 1P, FD), KBD, MOUSE (PS2 style), RTC/NVRAM. Boot ROM is
  Flash. PC-AT-sized motherboard.  Requires power supply with 3.3V
  output.


  The AlphaPC64 (aka Cabriolet): derived from EB64+ but now baby-AT with
  Flash boot ROM, no on-board SCSI or Ethernet. 3 ISA slots, 4 PCI slots
  (one pair are on a shared slot), uses plug-in Bcache SIMMs.  Requires
  power supply with 3.3V output.


  The AXPpci33 (aka NoName), is based on the EB66. This design is
  produced by Digital's Technical OEM (TOEM) group. It uses the 21066
  processor running at 166MHz or 233MHz. It is a baby-AT size, and runs
  from a standard PC power supply. It has 5 ISA slots and 3 PCI slots
  (one pair are a shared slot). There are 2 versions, with either PS/2
  or large DIN connectors for the keyboard.


  Other 21066-based motherboards: most if not all other 21066-based
  motherboards on the market are also based on EB66 - there's really not
  many system options when designing a 21066 system, because all the
  control is done on-chip.


  Multia (aka the Universal Desktop Box): This is a very compact
  pedestal desktop system based on the 21066. It includes 2 PCMCIA
  sockets, 21030 (TGA) graphics, 21040 Ethernet and NCR 810 SCSI disk
  along with floppy, 2 serial ports and a parallel port. It has limited
  expansion capability (one PCI slot) due to its compact size. (There is
  some restriction on when you can use the PCI slot, can't remember
  what) (Note that 21066A-based and Pentium-based Multia's are also
  available).


  DEC PC 150 AXP (aka Jensen): This is a very old Digital system - one
  of the first-generation Alpha systems. It is only mentioned here
  because a number of these systems seem to be available on the second-
  hand market. The Jensen is a floor-standing tower system which used a
  150MHz 21064 (later versions used faster CPUs but I'm not sure what
  speeds). It used programmable logic to interface a 486 EISA I/O bridge
  to the CPU.


  Other 21064(A) systems: There are 3 or 4 motherboard designs around
  (I'm not including Digital systems here) and all the ones I know of
  are derived from the EB64+ design. These include:


  o  EB64+ (some vendors package the board and sell it unmodified); AT
     form-factor.

  o  Aspen Systems motherboard: EB64+ derivative; baby-AT form-factor.

  o  Aspen Systems server board: many PCI slots (includes PCI bridge).

  o  AlphaPC64 (aka Cabriolet), baby AT form-factor.


  Other 21164(A) systems: The only one I'm aware of that isn't simply an
  EB164 clone is a system made by DeskStation. That system is
  implemented using a memory and I/O controller proprietary to Desk
  Station. I don't know what their attitude towards Linux is.



  8.  Bytes and all that stuff

  When the Alpha architecture was introduced, it was unique amongst RISC
  architectures for eschewing 8-bit and 16-bit loads and stores. It
  supported 32-bit and 64-bit loads and stores (longword and quadword,
  in Digital's nomenclature). The co-architects (Dick Sites, Rich Witek)
  justified this decision by citing the advantages:


  1. Byte support in the cache and memory sub-system tends to slow down
     accesses for 32-bit and 64-bit quantities.

  2. Byte support makes it hard to build high-speed error-correction
     circuitry into the cache/memory sub-system.


  Alpha compensates by providing powerful instructions for manipulating
  bytes and byte groups within 64-bit registers. Standard benchmarks for
  string operations (e.g., some of the Byte benchmarks) show that Alpha
  performs very well on byte manipulation.


  The absence of byte loads and stores impacts some software semaphores
  and impacts the design of I/O sub-systems. Digital's solution to the
  I/O problem is to use some low-order address lines to specify the data
  size during I/O transfers, and to decode these as byte enables. This
  so-called Sparse Addressing wastes address space and has the
  consequence that I/O space is non-contiguous (more on the intricacies
  of Sparse Addressing when I get around to writing it). Note that I/O
  space, in this context, refers to all system resources present on the
  PCI and therefore includes both PCI memory space and PCI I/O space.


  With the 21164A introduction, the Alpha archtecture was ECO'd to
  include byte addressing. Executing these new instructions on an
  earlier CPU will cause an OPCDEC PALcode exception, so that the
  PALcode will handle the access. This will have a performance impact.
  The ramifications of this are that use of these new instructions (IMO)
  should be restricted to device drivers rather than applications code.


  These new byte load and stores mean that future support chipsets will
  be able to support contiguous I/O space.



  9.  PALcode and all that stuff

  This is a placeholder for a section explaining PALcode. I will write
  it if there is sufficient interest.


  10.  Porting

  The ability of any Alpha-based machine to run Linux is really only
  limited by your ability to get information on the gory details of its
  innards. Since there are Linux ports for the E66, EB64+ and EB164
  boards, all systems based on the 21066, 21064/APECS or 21164/ALCOR
  should run Linux with little or no modification. The major thing that
  is different between any of these motherboards is the way that they
  route interrupts. There are three sources of interrupts:


  o  on-board devices

  o  PCI devices

  o  ISA devices


  All the systems use an Intel System I/O bridge (SIO) to act as a
  bridge between PCI and ISA (the main I/O bus is PCI, the ISA bus is a
  secondary bus used to support slow-speed and 'legacy' I/O devices).
  The SIO contains the traditional pair of daisy-chained 8259s.


  Some systems (e.g., the Noname) route all of their interrupts through
  the SIO and thence to the CPU. Some systems have a separate interrupt
  controller and route all PCI interrupts plus the SIO interrupt (8259
  output) through that, and all ISA interrupts through the SIO.


  Other differences between the systems include:


  o  how many slots they have

  o  what on-board PCI devices they have

  o  whether they have Flash or EPROM


  11.  More Information

  All of the DS evaluation boards and motherboard designs are license-
  free and the whole documentation kit for a design costs about \$50.
  That includes all the schematics, programmable parts sources, data
  sheets for CPU and support chipset. The doc kits are available from
  Digital Semiconductor distributors. I'm not suggesting that many
  people will want to rush out and buy this, but I do want to point out
  that the information is available.



  Hope that was helpful. Comments/updates/suggestions for expansion to
  Neal Crook <mailto:neal.crook@reo.mts.digital.com>.


  12.  References

  [1]
  <http://www.research.digital.com/wrl/publications/abstracts/TN-13.html>
  Bill Hamburgen, Jeff Mogul, Brian Reid, Alan Eustace, Richard Swan,
  Mary Jo Doherty, and Joel Bartlett.  Characterization of Organic
  Illumination Systems.  DEC WRL, Technical Note 13, April 1989.



  Linux Assembly HOWTO
  Konstantin Boldyshev <mailto:konst@linuxassembly.org> and
  Francois-Rene Rideau <mailto:fare@tunes.org>
  v0.5k, July 11, 2000

  This is the Linux Assembly HOWTO.  This document describes how to pro
  gram in assembly language using FREE programming tools, focusing on
  development for or from the Linux Operating System, mostly on IA-32
  (i386) platform.  Included material may or may not be applicable to
  other hardware and/or software platforms.  Contributions about them
  are gladly accepted.  Keywords: assembly, assembler, asm, inline asm,
  macroprocessor, preprocessor, 32-bit, IA-32, i386, x86, nasm, gas,
  as86, OS, kernel, system, libc, system call, interrupt, small, fast,
  embedded, hardware, port
  ______________________________________________________________________

  Table of Contents



  1. INTRODUCTION

     1.1 Legal Blurb
     1.2 Foreword
     1.3 Contributions
     1.4 Credits
     1.5 History

  2. DO YOU NEED ASSEMBLY?

     2.1 Pros and Cons
        2.1.1 The advantages of Assembly
        2.1.2 The disadvantages of Assembly
        2.1.3 Assessment
     2.2 How to NOT use Assembly
        2.2.1 General procedure to achieve efficient code
        2.2.2 Languages with optimizing compilers
        2.2.3 General procedure to speed your code up
        2.2.4 Inspecting compiler-generated code
     2.3 Linux and assembly

  3. ASSEMBLERS

     3.1 GCC Inline Assembly
        3.1.1 Where to find GCC
        3.1.2 Where to find docs for GCC Inline Asm
        3.1.3 Invoking GCC to build proper inline assembly code
     3.2 GAS
        3.2.1 Where to find it
        3.2.2 What is this AT&T syntax
        3.2.3 16-bit mode
        3.2.4 GASP
     3.3 NASM
        3.3.1 Where to find NASM
        3.3.2 What it does
     3.4 AS86
        3.4.1 Where to get AS86
        3.4.2 How to invoke the assembler?
        3.4.3 Where to find docs
        3.4.4 What if I can't compile Linux anymore with this new version ?
     3.5 OTHER ASSEMBLERS
        3.5.1 Win32Forth assembler
        3.5.2 Terse
        3.5.3 HLA
        3.5.4 TALC
        3.5.5 Non-free and/or Non-32bit x86 assemblers.

  4. METAPROGRAMMING/MACROPROCESSING

     4.1 What's integrated into the above
        4.1.1 GCC
        4.1.2 GAS
        4.1.3 GASP
        4.1.4 NASM
        4.1.5 AS86
        4.1.6 OTHER ASSEMBLERS
     4.2 External Filters
        4.2.1 CPP
        4.2.2 M4
        4.2.3 Macroprocessing with your own filter
        4.2.4 Metaprogramming
           4.2.4.1 Backends from compilers
           4.2.4.2 The New-Jersey Machine-Code Toolkit
           4.2.4.3 TUNES

  5. CALLING CONVENTIONS
     5.1 Linux
        5.1.1 Linking to GCC
        5.1.2 ELF vs a.out problems
        5.1.3 Direct Linux syscalls
        5.1.4 Hardware I/O under Linux
        5.1.5 Accessing 16-bit drivers from Linux/i386
     5.2 DOS
     5.3 Windows and Co.
     5.4 Your own OS

  6. QUICK START

     6.1 Tools you need
     6.2 Hello, world!
        6.2.1 NASM (hello.asm)
        6.2.2 GAS (hello.S)
     6.3 Producing object code
     6.4 Producing executable

  7. RESOURCES

     7.1 Mailing list
     7.2 Frequently asked questions (with answers)
        7.2.1 How do I do graphics programming in Linux?
        7.2.2 How do I debug pure assembly code under Linux?
        7.2.3 Any other useful debugging tools?
        7.2.4 How do I access BIOS functions from Linux (BSD, BeOS, etc)?


  ______________________________________________________________________

  1.  INTRODUCTION


  You can skip this section if you are familiar with HOWTOs, or just
  hate to read all this assembly-nonrelated crap.


  1.1.  Legal Blurb

  Copyright  1999-2000 Konstantin Boldyshev.

  Copyright  1996-1999 Francois-Rene Rideau.

  This document may be distributed only subject to the terms and
  conditions set forth in the LDP License
  <http://linuxdoc.org/COPYRIGHT.html>.  It may be reproduced and
  distributed in whole or in part, in any medium physical or electronic,
  provided that this license notice is displayed in the reproduction.
  Commercial redistribution is permitted and encouraged.

  All modified documents, including translations, anthologies, and
  partial documents, must meet the following requirements:


    The modified version must be labeled as such

    The person making the modifications must be identified

    Acknowledgement of the original author must be retained

    The location of the original unmodified document be identified

    The original author's (or authors') name(s) may not be used to
     assert or imply endorsement of the resulting document without the
     original author's (or authors') permission
  The most recent official version of this document is available from
  Linux Assembly <http://linuxassembly.org> and LDP
  <http://linuxdoc.org> sites.  If you are reading a few-months-old
  copy, consider checking urls above for a new version.


  1.2.  Foreword

  This document aims answering questions of those who program or want to
  program 32-bit x86 assembly using free software
  <http://www.gnu.org/philosophy/>, particularly under the Linux
  operating system.  At many places, Universal Resource Locators (URL)
  are given for some software or documentation repository.  This
  document also points to other documents about non-free, non-x86, or
  non-32-bit assemblers, although this is not its primary goal.  Also
  note that there are FAQs and docs about programming on your favorite
  platform (whatever it is), which you should consult for platform-
  specific issues, not related directly to assembly programming.

  Because the main interest of assembly programming is to build the guts
  of operating systems, interpreters, compilers, and games, where C
  compiler fails to provide the needed expressiveness (performance is
  more and more seldom as issue), we are focusing on development of such
  kind of software.

  If you don't know what free software is, please do read carefully the
  GNU General Public License, which is used in a lot of free software,
  and is the model for most of their licenses.  It generally comes in a
  file named COPYING (or COPYING.LIB).  Literature from the FSF
  <http://www.fsf.org> (free software foundation) might help you, too.
  Particularly, the interesting feature of free software is that it
  comes with sources that you can consult and correct, or sometimes even
  borrow from.  Read your particular license carefully and do comply to
  it.


  1.3.  Contributions

  This is an interactively evolving document: you are especially invited
  to ask questions, to answer questions, to correct given answers, to
  give pointers to new software, to point the current maintainer to bugs
  or deficiencies in the pages.  In one word, contribute!

  To contribute, please contact the Assembly-HOWTO maintainer.  At the
  time of this writing, it is Konstantin Boldyshev
  <mailto:konst@linuxassembly.org> and no more Francois-Rene Rideau
  <mailto:fare@tunes.org>.  I (Fare) had been looking for some time for
  a serious hacker to replace me as maintainer of this document, and am
  pleased to announce Konstantin as my worthy successor.


  1.4.  Credits

  I would like to thank following persons, by order of appearance:

    Linus Torvalds <mailto:buried.alive@in.mail> for Linux

    Bruce Evans <mailto:bde@zeta.org.au> for bcc from which as86 is
     extracted

    Simon Tatham <mailto:anakin@pobox.com> and Julian Hall
     <mailto:jules@earthcorp.com> for NASM

    Greg Hankins <mailto:gregh@metalab.unc.edu> and now Tim Bynum
     <mailto:linux-howto@metalab.unc.edu> for maintaining HOWTOs

    Raymond Moon <mailto:raymoon@moonware.dgsys.com> for his FAQ

    Eric Dumas <mailto:dumas@linux.eu.org> for his translation of the
     mini-HOWTO into French (sad thing for the original author to be
     French and write in English)

    Paul Anderson <mailto:paul@geeky1.ebtech.net> and Rahim Azizarab
     <mailto:rahim@megsinet.net> for helping me, if not for taking over
     the HOWTO.

    Marc Lehman <mailto:pcg@goof.com> for his insight on GCC
     invocation.

    Abhijit Menon-Sen <mailto:ams@wiw.org> for helping me figure out
     the argument passing convention

    All the people who have contributed ideas, answers, remarks, and
     moral support.


  1.5.  History

  Each version includes a few fixes and minor corrections, that need not
  to be repeatedly mentioned every time.

     Version 0.5k    11 Jul 2000
        Few additions to FAQ


     Version 0.5j    14 Jun 2000
        Complete rearrangement of INTRODUCTION and RESOURCES; FAQ added
        to RESOURCES, misc cleanups and additions (and more to come)


     Version 0.5i    04 May 2000
        Added HLA, TALC; rearrangements in RESOURCES, QUICK START,
        ASSEMBLERS; few new pointers


     Version 0.5h    09 Apr 2000
        finally managed to state LDP license on document, new resources
        added, misc fixes


     Version 0.5g    26 Mar 2000
        new resources on different CPUs


     Version 0.5f    02 Mar 2000
        new resources, misc corrections


     Version 0.5e    10 Feb 2000
        url updates, changes in GAS example


     Version 0.5d    01 Feb 2000
        RESOURCES (former POINTERS) section completely redone, various
        url updates.


     Version 0.5c    05 Dec 1999
        New pointers, updates and some rearrangements.  Rewrite of sgml
        source.


     Version 0.5b    19 Sep 1999
        Discussion about libc or not libc continues.  New web pointers
        and and overall updates.


     Version 0.5a    01 Aug 1999
        "QUICK START" section rearranged, added GAS example.  Several
        new web pointers.


     Version 0.5     25 July 1999
        GAS has 16-bit mode.  New maintainer (at last): Konstantin
        Boldyshev.  Discussion about libc or not libc.  Added section
        "QUICK START" with examples of using assembly.


     Version 0.4q    22 June 1999
        process argument passing (argc,argv,environ) in assembly.  This
        is yet another "last release by Fare before new maintainer takes
        over".  Nobody knows who might be the new maintainer.


     Version 0.4p    6 June 1999
        clean up and updates.


     Version 0.4o    1 December 1998
        *


     Version 0.4m    23 March 1998
        corrections about gcc invocation


     Version 0.4l    16 November 1997
        release for LSL 6th edition.


     Version 0.4k    19 October 1997
        *


     Version 0.4j    7 September 1997
        *


     Version 0.4i    17 July 1997
        info on 16-bit mode access from Linux.


     Version 0.4h    19 Jun 1997
        still more on "how not to use assembly"; updates on NASM, GAS.


     Version 0.4g    30 Mar 1997
        *


     Version 0.4f    20 Mar 1997
        *


     Version 0.4e    13 Mar 1997
        Release for DrLinux


     Version 0.4d    28 Feb 1997
        Vapor announce of a new Assembly-HOWTO maintainer.


     Version 0.4c    9 Feb 1997
        Added section "DO YOU NEED ASSEMBLY?"


     Version 0.4b    3 Feb 1997
        NASM moved: now is before AS86


     Version 0.4a    20 Jan 1997
        CREDITS section added


     Version 0.4     20 Jan 1997
        first release of the HOWTO as such.


     Version 0.4pre1 13 Jan 1997
        text mini-HOWTO transformed into a full linuxdoc-sgml HOWTO, to
        see what the SGML tools are like.


     Version 0.3l    11 Jan 1997
        *


     Version 0.3k    19 Dec 1996
        What? I had forgotten to point to terse???


     Version 0.3j    24 Nov 1996
        point to French translated version


     Version 0.3i    16 Nov 1996
        NASM is getting pretty slick


     Version 0.3h    6 Nov 1996
        more about cross-compiling -- See on sunsite: devel/msdos/


     Version 0.3g    2 Nov 1996
        Created the History. Added pointers in cross-compiling section.
        Added section about I/O programming under Linux (particularly
        video).


     Version 0.3f    17 Oct 1996
        *


     Version 0.3c    15 Jun 1996
        *


     Version 0.2     04 May 1996
        *


     Version 0.1     23 Apr 1996
        Francois-Rene "Fare" Rideau <fare@tunes.org> creates and
        publishes the first mini-HOWTO, because "I'm sick of answering
        ever the same questions on comp.lang.asm.x86"



  2.  DO YOU NEED ASSEMBLY?

  Well, I wouldn't want to interfere with what you're doing, but here is
  some advice from hard-earned experience.



  2.1.  Pros and Cons



  2.1.1.  The advantages of Assembly

  Assembly can express very low-level things:

    you can access machine-dependent registers and I/O.

    you can control the exact behavior of code in critical sections
     that might otherwise involve deadlock between multiple software
     threads or hardware devices.

    you can break the conventions of your usual compiler, which might
     allow some optimizations (like temporarily breaking rules about
     memory allocation, threading, calling conventions, etc).

    you can build interfaces between code fragments using incompatible
     such conventions (e.g. produced by different compilers, or
     separated by a low-level interface).

    you can get access to unusual programming modes of your processor
     (e.g. 16 bit mode to interface startup, firmware, or legacy code on
     Intel PCs)

    you can produce reasonably fast code for tight loops to cope with a
     bad non-optimizing compiler (but then, there are free optimizing
     compilers available!)

    you can produce hand-optimized code perfectly tuned for your
     particular hardware setup, though not to anyone else's.

    you can write some code for your new language's optimizing compiler
     (that's something few will ever do, and even they, not often).



  2.1.2.  The disadvantages of Assembly

  Assembly is a very low-level language (the lowest above hand-coding
  the binary instruction patterns).  This means

    it's long and tedious to write initially,

    it's quite bug-prone,

    your bugs can be very difficult to chase,

    it's very difficult to understand and modify, i.e. to maintain.

    the result is very non-portable to other architectures, existing or
     future,

    your code will be optimized only for a certain implementation of a
     same architecture: for instance, among Intel-compatible platforms,
     each CPU design and its variations (relative latency, throughput,
     and capacity, of processing units, caches, RAM, bus, disks,
     presence of FPU, MMX, 3DNOW, SIMD extensions, etc) implies
     potentially completely different optimization techniques.  CPU
     designs already include: Intel 386, 486, Pentium, PPro, Pentium II,
     Pentium III; Cyrix 5x86, 6x86; AMD K5, K6 (K6-2, K6-III), K7
     (Athlon).  New designs keep popping up, so don't expect either this
     listing or your code to be up-to-date.

    you spend more time on a few details, and can't focus on small and
     large algorithmic design, that are known to bring the largest part
     of the speed up.  [e.g. you might spend some time building very
     fast list/array manipulation primitives in assembly; only a hash
     table would have sped up your program much more; or, in another
     context, a binary tree; or some high-level structure distributed
     over a cluster of CPUs]

    a small change in algorithmic design might completely invalidate
     all your existing assembly code.  So that either you're ready (and
     able) to rewrite it all, or you're tied to a particular algorithmic
     design;

    On code that ain't too far from what's in standard benchmarks,
     commercial optimizing compilers outperform hand-coded assembly
     (well, that's less true on the x86 architecture than on RISC
     architectures, and perhaps less true for widely available/free
     compilers; anyway, for typical C code, GCC is fairly good);

    And in any case, as says moderator John Levine on comp.compilers
     <news:comp.compilers>, "compilers make it a lot easier to use
     complex        data structures, and compilers don't get bored
     halfway through and generate reliably pretty good code." They will
     also correctly propagate code transformations throughout the whole
     (huge) program when optimizing code between procedures and module
     boundaries.



  2.1.3.  Assessment

  All in all, you might find that though using assembly is sometimes
  needed, and might even be useful in a few cases where it is not,
  you'll want to:

    minimize the use of assembly code,

    encapsulate this code in well-defined interfaces

    have your assembly code automatically generated from patterns
     expressed in a higher-level language than assembly (e.g. GCC inline
     assembly macros).

    have automatic tools translate these programs into assembly code

    have this code be optimized if possible

    All of the above, i.e. write (an extension to) an optimizing
     compiler back-end.

  Even in cases when assembly is needed (e.g. OS development), you'll
  find that not so much of it is, and that the above principles hold.
  See the Linux kernel sources concerning this: as little assembly as
  needed, resulting in a fast, reliable, portable, maintainable OS.
  Even a successful game like DOOM was almost massively written in C,
  with a tiny part only being written in assembly for speed up.



  2.2.  How to NOT use Assembly



  2.2.1.  General procedure to achieve efficient code

  As says Charles Fiterman on comp.compilers <news:comp.compilers> about
  human vs computer-generated assembly code,

  " The human should always win and here is why.

    First the human writes the whole thing in a high level language.

    Second he profiles it to find the hot spots where it spends its
     time.

    Third he has the compiler produce assembly for those small sections
     of code.

    Fourth he hand tunes them looking for tiny improvements over the
     machine generated code.

     The human wins because he can use the machine.  "



  2.2.2.  Languages with optimizing compilers

  Languages like ObjectiveCAML, SML, CommonLISP, Scheme, ADA, Pascal, C,
  C++, among others, all have free optimizing compilers that will
  optimize the bulk of your programs, and often do better than hand-
  coded assembly even for tight loops, while allowing you to focus on
  higher-level details, and without forbidding you to grab a few percent
  of extra performance in the above-mentioned way, once you've reached a
  stable design.  Of course, there are also commercial optimizing
  compilers for most of these languages, too!

  Some languages have compilers that produce C code, which can be
  further optimized by a C compiler: LISP, Scheme, Perl, and many other.
  Speed is fairly good.


  2.2.3.  General procedure to speed your code up

  As for speeding code up, you should do it only for parts of a program
  that a profiling tool has consistently identified as being a
  performance bottleneck.

  Hence, if you identify some code portion as being too slow, you should

    first try to use a better algorithm;

    then try to compile it rather than interpret it;

    then try to enable and tweak optimization from your compiler;

    then give the compiler hints about how to optimize (typing
     information in LISP; register usage with GCC; lots of options in
     most compilers, etc).
    then possibly fallback to assembly programming

  Finally, before you end up writing assembly, you should inspect
  generated code, to check that the problem really is with bad code
  generation, as this might really not be the case: compiler-generated
  code might be better than what you'd have written, particularly on
  modern multi-pipelined architectures!  Slow parts of a program might
  be intrinsically so.  Biggest problems on modern architectures with
  fast processors are due to delays from memory access, cache-misses,
  TLB-misses, and page-faults; register optimization becomes useless,
  and you'll more profitably re-think data structures and threading to
  achieve better locality in memory access.  Perhaps a completely
  different approach to the problem might help, then.



  2.2.4.  Inspecting compiler-generated code

  There are many reasons to inspect compiler-generated assembly code.
  Here are what you'll do with such code:

    check whether generated code can be obviously enhanced with hand-
     coded assembly (or by tweaking compiler switches)

    when that's the case, start from generated code and modify it
     instead of starting from scratch

    more generally, use generated code as stubs to modify, which at
     least gets right the way your assembly routines interface to the
     external world

    track down bugs in your compiler (hopefully rarer)

  The standard way to have assembly code be generated is to invoke your
  compiler with the -S flag.  This works with most Unix compilers,
  including the GNU C Compiler (GCC), but YMMV.  As for GCC, it will
  produce more understandable assembly code with the -fverbose-asm
  command-line option.  Of course, if you want to get good assembly
  code, don't forget your usual optimization options and hints!



  2.3.  Linux and assembly

  In general case you don't need to use assembly language in Linux
  programming.  Unlike DOS, you do not have to write Linux drivers in
  assembly (well, actually you can do it if you really want).  And with
  modern optimizing compilers, if you care of speed optimization for
  different CPU's, it's much simpler to write in C.  However, if you're
  reading this, you might have some reason to use assembly instead of
  C/C++.

  You may need to use assembly, or you may want to use assembly.
  Shortly, main practical reasons why you may need to get into Linux
  assembly are small code and libc independence.  Non-practical (and
  most often) reason is being just an old crazy hacker, who has twenty
  years old habit of doing everything in assembly language.

  Also, if you're porting Linux to some embedded hardware you can be
  quite short at size of whole system: you need to fit kernel, libc and
  all that stuff of (file|find|text|sh|etc.) utils into several hundreds
  of kilobytes, and every kilobyte costs much.  So, one of the ways
  you've got is to rewrite some (or all) parts of system in assembly,
  and this will really save you a lot of space.  For instance, a simple
  httpd written in assembly can take less than 600 bytes; you can fit a
  webserver, consisting of kernel and httpd, in 400 KB or less... Think
  about it.


  3.  ASSEMBLERS



  3.1.  GCC Inline Assembly

  The well-known GNU C/C++ Compiler (GCC), an optimizing 32-bit compiler
  at the heart of the GNU project, supports the x86 architecture quite
  well, and includes the ability to insert assembly code in C programs,
  in such a way that register allocation can be either specified or left
  to GCC.  GCC works on most available platforms, notably Linux, *BSD,
  VSTa, OS/2, *DOS, Win*, etc.


  3.1.1.  Where to find GCC

  The original GCC site is the GNU FTP site
  <ftp://prep.ai.mit.edu/pub/gnu/gcc/> together with all released
  application software from the GNU project.  Linux-configured and
  precompiled versions can be found in
  <ftp://metalab.unc.edu/pub/Linux/GCC/> There are a lot of FTP mirrors
  of both sites, everywhere around the world, as well as CD-ROM copies.

  GCC development has split into two branches some time ago (GCC 2.8 and
  EGCS), but they merged back, and current GCC webpage is
  <http://gcc.gnu.org>.

  Sources adapted to your favorite OS and precompiled binaries should be
  found at your usual FTP sites.

  For most popular DOS port of GCC is named DJGPP, and can be found in
  directories of such name in FTP sites.  See
  <http://www.delorie.com/djgpp/>.

  There are two Win32 GCC ports: cygwin
  <http://sourceware.cygnus.com/cygwin/> and mingw
  <http://www.mingw.org>

  There is also a port of GCC to OS/2 named EMX, that also works under
  DOS, and includes lots of unix-emulation library routines.  See around
  the following site: <ftp://ftp-os2.cdrom.com/pub/os2/emx09c/>.



  3.1.2.  Where to find docs for GCC Inline Asm

  The documentation of GCC includes documentation files in TeXinfo
  format.  You can compile them with TeX and print then result, or
  convert them to .info, and browse them with emacs, or convert them to
  .html, or nearly whatever you like; convert (with the right tools) to
  whatever you like, or just read as is.  The .info files are generally
  found on any good installation for GCC.

  The right section to look for is C Extensions::Extended Asm::

  Section Invoking GCC::Submodel Options::i386 Options:: might help too.
  Particularly, it gives the i386 specific constraint names for
  registers: abcdSDB correspond to %eax, %ebx, %ecx, %edx, %esi, %edi
  and %ebp respectively (no letter for %esp).

  The DJGPP Games resource (not only for game hackers) had page
  specifically about assembly, but it's down.  Its data have nonetheless
  been recovered on the DJGPP site <http://www.delorie.com/djgpp/>, that
  contains a mine of other useful information:
  <http://www.delorie.com/djgpp/doc/brennan/>, and in the DJGPP Quick
  ASM Programming Guide <http://www.castle.net/~avly/djasm.html>.


  GCC depends on GAS for assembling, and follow its syntax (see below);
  do mind that inline asm needs percent characters to be quoted so they
  be passed to GAS.  See the section about GAS below.

  Find lots of useful examples in the linux/include/asm-i386/
  subdirectory of the sources for the Linux kernel.



  3.1.3.  Invoking GCC to build proper inline assembly code

  Because assembly routines from the kernel headers (and most likely
  your own headers, if you try making your assembly programming as clean
  as it is in the linux kernel) are embedded in extern inline functions,
  GCC must be invoked with the -O flag (or -O2, -O3, etc), for these
  routines to be available.  If not, your code may compile, but not link
  properly, since it will be looking for non-inlined extern functions in
  the libraries against which your program is being linked!  Another way
  is to link against libraries that include fallback versions of the
  routines.

  Inline assembly can be disabled with -fno-asm, which will have the
  compiler die when using extended inline asm syntax, or else generate
  calls to an external function named asm() that the linker can't
  resolve.  To counter such flag, -fasm restores treatment of the asm
  keyword.

  More generally, good compile flags for GCC on the x86 platform are

  ______________________________________________________________________
          gcc -O2 -fomit-frame-pointer -W -Wall
  ______________________________________________________________________



  -O2 is the good optimization level in most cases.  Optimizing besides
  it takes longer, and yields code that is a lot larger, but only a bit
  faster; such overoptimization might be useful for tight loops only (if
  any), which you may be doing in assembly anyway.  In cases when you
  need really strong compiler optimization for a few files, do consider
  using up to -O6.

  -fomit-frame-pointer allows generated code to skip the stupid frame
  pointer maintenance, which makes code smaller and faster, and frees a
  register for further optimizations.  It precludes the easy use of
  debugging tools (gdb), but when you use these, you just don't care
  about size and speed anymore anyway.

  -W -Wall enables all warnings and helps you catch obvious stupid
  errors.

  You can add some CPU-specific -m486 or such flag so that GCC will
  produce code that is more adapted to your precise computer.  Note that
  modern GCC has -mpentium and such flags (and PGCC
  <http://goof.com/pcg/> has even more), whereas GCC 2.7.x and older
  versions do not.  A good choice of CPU-specific flags should be in the
  Linux kernel.  Check the TeXinfo documentation of your current GCC
  installation for more.


  -m386 will help optimize for size, hence also for speed on computers
  whose memory is tight and/or loaded, since big programs cause swap,
  which more than counters any "optimization" intended by the larger
  code.  In such settings, it might be useful to stop using C, and use
  instead a language that favors code factorization, such as a
  functional language and/or FORTH, and use a bytecode- or wordcode-
  based implementation.

  Note that you can vary code generation flags from file to file, so
  performance-critical files will use maximum optimization, whereas
  other files will be optimized for size.

  To optimize even more, option -mregparm=2 and/or corresponding
  function attribute might help, but might pose lots of problems when
  linking to foreign code, including libc.  There are ways to correctly
  declare foreign functions so the right call sequences be generated, or
  you might want to recompile the foreign libraries to use the same
  register-based calling convention...

  Note that you can add make these flags the default by editing file
  /usr/lib/gcc-lib/i486-linux/2.7.2.3/specs or wherever that is on your
  system (better not add -W -Wall there, though).  The exact location of
  the GCC specs files on your system can be found by asking gcc -v.



  3.2.  GAS

  GAS is the GNU Assembler, that GCC relies upon.



  3.2.1.  Where to find it

  Find it at the same place where you found GCC, in a package named
  binutils.

  The latest version is available from HJLu at
  <ftp://ftp.varesearch.com/pub/support/hjl/binutils/>.



  3.2.2.  What is this AT&T syntax

  Because GAS was invented to support a 32-bit unix compiler, it uses
  standard AT&T syntax, which resembles a lot the syntax for standard
  m68k assemblers, and is standard in the UNIX world.  This syntax is no
  worse, no better than the Intel syntax.  It's just different.  When
  you get used to it, you find it much more regular than the Intel
  syntax, though a bit boring.

  Here are the major caveats about GAS syntax:

    Register names are prefixed with %, so that registers are %eax, %dl
     and so on, instead of just eax, dl, etc.  This makes it possible to
     include external C symbols directly in assembly source, without any
     risk of confusion, or any need for ugly underscore prefixes.

    The order of operands is source(s) first, and destination last, as
     opposed to the Intel convention of destination first and sources
     last.  Hence, what in Intel syntax is mov ax,dx (move contents of
     register dx into register ax) will be in GAS syntax mov %dx, %ax.

    The operand length is specified as a suffix to the instruction
     name.  The suffix is b for (8-bit) byte, w for (16-bit) word, and l
     for (32-bit) long.  For instance, the correct syntax for the above
     instruction would have been movw %dx,%ax.  However, gas does not
     require strict AT&T syntax, so the suffix is optional when length
     can be guessed from register operands, and else defaults to 32-bit
     (with a warning).

    Immediate operands are marked with a $ prefix, as in addl $5,%eax
     (add immediate long value 5 to register %eax).

    No prefix to an operand indicates it is a memory-address; hence
     movl $foo,%eax puts the address of variable foo in register %eax,
     but movl foo,%eax puts the contents of variable foo in register
     %eax.

    Indexing or indirection is done by enclosing the index register or
     indirection memory cell address in parentheses, as in testb
     $0x80,17(%ebp) (test the high bit of the byte value at offset 17
     from the cell pointed to by %ebp).


  A program exists to help you convert programs from TASM syntax to AT&T
  syntax. See
  <ftp://x2ftp.oulu.fi/pub/msdos/programming/convert/ta2asv08.zip>.
  (Since the original x2ftp site is closing (no more?), use a mirror
  site <ftp://ftp.lip6.fr/pub/pc/x2ftp/README.mirror_sites>).  There
  also exists a program for the reverse conversion:
  <http://www.multimania.com/placr/a2i.html>.


  GAS has comprehensive documentation in TeXinfo format, which comes at
  least with the source distribution.  Browse extracted .info pages with
  Emacs or whatever.  There used to be a file named gas.doc or as.doc
  around the GAS source package, but it was merged into the TeXinfo
  docs.  Of course, in case of doubt, the ultimate documentation is the
  sources themselves!  A section that will particularly interest you is
  Machine Dependencies::i386-Dependent::


  Again, the sources for Linux (the OS kernel) come in as excellent
  examples; see under linux/arch/i386/ the following files: kernel/*.S,
  boot/compressed/*.S, mathemu/*.S.

  If you are writing kind of a language, a thread package, etc., you
  might as well see how other languages (OCaml <http://para.inria.fr/>,
  Gforth <http://www.jwdt.com/~paysan/gforth.html>, etc.), or thread
  packages (QuickThreads, MIT pthreads, LinuxThreads, etc), or whatever,
  do it.

  Finally, just compiling a C program to assembly might show you the
  syntax for the kind of instructions you want.  See section ``Do you
  need Assembly?'' above.



  3.2.3.  16-bit mode

  The current stable release of binutils (2.9.1.0.25) now fully supports
  16-bit mode (registers and addressing) on i386 PCs.  Still with its
  peculiar AT&T syntax, of course.  Use .code16 and .code32 to switch
  between assembly modes.

  Also, a neat trick used by some (including the oskit authors) is to
  have GCC produce code for 16-bit real mode, using an inline assembly
  statement asm(".code16\n").  GCC will still emit only 32-bit
  addressing modes, but GAS will insert proper 32-bit prefixes for them.


  3.2.4.  GASP

  GASP is the GAS Preprocessor.  It adds macros and some nice syntax to
  GAS.  GASP comes together with GAS in the GNU binutils archive.  It
  works as a filter, much like cpp and the like.  I have no idea on
  details, but it comes with its own texinfo documentation, so just
  browse them (in .info), print them, grok them.  GAS with GASP looks
  like a regular macro-assembler to me.



  3.3.  NASM

  The Netwide Assembler project provides cool i386 assembler, written in
  C, that should be modular enough to eventually support all known
  syntaxes and object formats.


  3.3.1.  Where to find NASM

  <http://www.cryogen.com/Nasm/>

  Binary release on your usual metalab mirror in devel/lang/asm/.
  Should also be available as .rpm or .deb in your usual RedHat/Debian
  distributions' contrib.

  At the time of writing current version of NASM is 0.98.

  Note: there's also an extended NASM version available at
  <ftp://ftp.linuxgames.com/crystal/nasm/> know as 0.98e. It introduces
  several serious bugfixes and improvements, so you may want to use it
  instead of "official" version.


  3.3.2.  What it does

  The syntax is Intel-style.  Fairly good macroprocessing support is
  integrated.

  Supported object file formats are bin, aout, coff, elf, as86, (DOS)
  obj, win32, (their own format) rdf.

  NASM can be used as a backend for the free LCC compiler (support files
  included).

  Unless you're using BCC as a 16-bit compiler (which is out of scope of
  this 32-bit HOWTO), you should definitely use NASM instead of say AS86
  or MASM, because it is actively supported online, and runs on all
  platforms.

  Note: NASM also comes with a disassembler, NDISASM.

  Its hand-written parser makes it much faster than GAS, though of
  course, it doesn't support three bazillion different architectures.
  If you like Intel-style syntax, as opposed to GAS syntax, then it
  should be the assembler of choice...

  Note: There are ``converters between GAS AT&T and Intel assembler
  syntax'', which perform conversion in both directions.


  3.4.  AS86

  AS86 is a 80x86 assembler, both 16-bit and 32-bit, part of Bruce
  Evans' C Compiler (BCC).  It has mostly Intel-syntax, though it
  differs slightly as for addressing modes.
  3.4.1.  Where to get AS86

  A completely outdated version of AS86 is distributed by HJLu just to
  compile the Linux kernel, in a package named bin86 (current version
  0.4), available in any Linux GCC repository.  But I advise no one to
  use it for anything else but compiling Linux.  This version supports
  only a hacked minix object file format, which is not supported by the
  GNU binutils or anything, and it has a few bugs in 32-bit mode, so you
  really should better keep it only for compiling Linux.

  The most recent versions by Bruce Evans (bde@zeta.org.au) are
  published together with the FreeBSD distribution.  Well, they were: I
  could not find the sources from distribution 2.1 on :( Hence, I put
  the sources at my place:
  <http://www.tunes.org/~fare/files/asm/bcc-95.3.12.src.tgz>

  The Linux/8086 (aka ELKS) project is somehow maintaining bcc (though I
  don't think they included the 32-bit patches).  See around
  <http://www.linux.org.uk/ELKS-Home/> (or
  <http://www.elks.ecs.soton.ac.uk>) and
  <ftp://linux.mit.edu/pub/linux/ELKS/>.  I haven't followed these
  developments, and would appreciate a reader contributing on this
  topic.

  Among other things, these more recent versions, unlike HJLu's,
  supports Linux GNU a.out format, so you can link you code to Linux
  programs, and/or use the usual tools from the GNU binutils package to
  manipulate your data.  This version can co-exist without any harm with
  the previous one (see according question below).

  BCC from 12 march 1995 and earlier version has a misfeature that makes
  all segment pushing/popping 16-bit, which is quite annoying when
  programming in 32-bit mode.  I wrote a patch at a time when the TUNES
  Project used as86:
  <http://www.tunes.org/~fare/files/asm/as86.bcc.patch.gz>.  Bruce Evans
  accepted this patch, but since as far as I know he hasn't published a
  new release of bcc, the ones to ask about integrating it (if not done
  yet) are the ELKS developers.



  3.4.2.  How to invoke the assembler?

  Here's the GNU Makefile entry for using bcc to transform .s asm into
  both GNU a.out .o object and .l listing:


  ______________________________________________________________________
  %.o %.l:        %.s
          bcc -3 -G -c -A-d -A-l -A$*.l -o $*.o $<
  ______________________________________________________________________



  Remove the %.l, -A-l, and -A$*.l, if you don't want any listing.  If
  you want something else than GNU a.out, you can see the docs of bcc
  about the other supported formats, and/or use the objcopy utility from
  the GNU binutils package.



  3.4.3.  Where to find docs

  The docs are what is included in the bcc package.  I salvaged the man
  pages that used to be available from the FreeBSD site at
  <http://www.tunes.org/~fare/files/asm/bcc-95.3.12.src.tgz>.  Maybe
  ELKS developers know better.  When in doubt, the sources themselves
  are often a good docs: it's not very well commented, but the
  programming style is straightforward.  You might try to see how as86
  is used in ELKS or Tunes 0.0.0.25...



  3.4.4.  What if I can't compile Linux anymore with this new version ?

  Linus is buried alive in mail, and since HJLu (official bin86
  maintainer) chose to write hacks around an obsolete version of as86
  instead of building clean code around the latest version, I don't
  think my patch for compiling Linux with a modern as86 has any chance
  to be accepted if resubmitted.  Now, this shouldn't matter: just keep
  your as86 from the bin86 package in /usr/bin/, and let bcc install the
  good as86 as /usr/local/libexec/i386/bcc/as where it should be. You
  never need explicitly call this "good" as86, because bcc does
  everything right, including conversion to Linux a.out, when invoked
  with the right options; so assemble files exclusively with bcc as a
  frontend, not directly with as86.

  Since GAS now supports 16-bit code, and since H. Peter Anvin, well-
  known linux hacker, works on NASM, maybe Linux will get rid of AS86,
  anyway? Who knows!



  3.5.  OTHER ASSEMBLERS

  These are other non-regular options, in case the previous didn't
  satisfy you (why?), that I don't recommend in the usual (?) case, but
  that could be quite useful if the assembler must be integrated in the
  software you're designing (i.e. an OS or development environment).


  3.5.1.  Win32Forth assembler

  Win32Forth is a free 32-bit ANS FORTH system that successfully runs
  under Win32s, Win95, Win/NT.  It includes a free 32-bit assembler
  (either prefix or postfix syntax) integrated into the reflective FORTH
  language.  Macro processing is done with the full power of the
  reflective language FORTH; however, the only supported input and
  output contexts is Win32For itself (no dumping of .obj file, but you
  could add that feature yourself, of course).  Find it at
  <ftp://ftp.forth.org/pub/Forth/Compilers/native/windows/Win32For/>.


  3.5.2.  Terse

  Terse <http://www.terse.com> is a programming tool that provides THE
  most compact assembler syntax for the x86 family!  However, it is evil
  proprietary software.  It is said that there was a project for a free
  clone somewhere, that was abandoned after worthless pretenses that the
  syntax would be owned by the original author.  Thus, if you're looking
  for a nifty programming project related to assembly hacking, I invite
  you to develop a terse-syntax frontend to NASM, if you like that
  syntax.

  As an interesting historic remark, on comp.compilers
  <news:comp.compilers>, 1999/07/11 19:36:51, the moderator wrote:
  "There's no reason that assemblers have to have awful syntax.  About
  30 years ago I used Niklaus Wirth's PL360, which was basically a S/360
  assembler with Algol syntax and a a little syntactic sugar like while
  loops that turned into the obvious branches.  It really was an
  assembler, e.g., you had to write out your expressions with explicit
  assignments of values to registers, but it was nice.  Wirth used it to
  write Algol W, a small fast Algol subset, which was a predecessor to
  Pascal.  As is so often the case, Algol W was a significant
  improvement over many of its successors. -John"



  3.5.3.  HLA

  HLA <http://webster.cs.ucr.edu> is a High Level Assembly language.  It
  uses a high level language like syntax (similar to Pascal, C/C++, and
  other HLLs) for variable declarations, procedure declarations, and
  procedure calls. It uses a modified assembly language syntax for the
  standard machine instructions.  It also provides several high level
  language style control structures (if, while, repeat..until, etc.)
  that help you write much more readable code.

  HLA is free, but runs only under Win32.  You need MASM and a 32-bit
  version of MS-link, because HLA produces MASM code and uses MASM for
  final assembling and linking. However it comes with m2t (MASM to TASM)
  post-processor program that converts the HLA MASM output to a form
  that will compile under TASM. Unfortunately, NASM is not supported.



  3.5.4.  TALC

  TALC <http://www.cs.cornell.edu/talc/> is another free MASM/Win32
  based compiler (however it supports ELF output, does it?).

  TAL stands for Typed Assembly Language.  It extends traditional
  untyped assembly languages with typing annotations, memory management
  primitives, and a sound set of typing rules, to guarantee the memory
  safety, control flow safety,and type safety of TAL programs.
  Moreover, the typing constructs are expressive enough to encode most
  source language programming features including records and structures,
  arrays, higher-order and polymorphic functions, exceptions, abstract
  data types, subtyping, and modules.  Just as importantly, TAL is
  flexible enough to admit many low-level compiler optimizations.
  Consequently, TAL is an ideal target platform for type-directed
  compilers that want to produce verifiably safe code for use in secure
  mobile code applications or extensible operating system kernels.


  3.5.5.  Non-free and/or Non-32bit x86 assemblers.

  You may find more about them, together with the basics of x86 assembly
  programming, in ``Raymond Moon's x86 assembly FAQ''.

  Note that all DOS-based assemblers should work inside the Linux DOS
  Emulator, as well as other similar emulators, so that if you already
  own one, you can still use it inside a real OS.  Recent DOS-based
  assemblers also support COFF and/or other object file formats that are
  supported by the GNU BFD library, so that you can use them together
  with your free 32-bit tools, perhaps using GNU objcopy (part of the
  binutils) as a conversion filter.



  4.  METAPROGRAMMING/MACROPROCESSING

  Assembly programming is a bore, but for critical parts of programs.

  You should use the appropriate tool for the right task, so don't
  choose assembly when it's not fit; C, OCaml, perl, Scheme, might be a
  better choice for most of your programming.
  However, there are cases when these tools do not give a fine enough
  control on the machine, and assembly is useful or needed.  In those
  case, you'll appreciate a system of macroprocessing and
  metaprogramming that'll allow recurring patterns to be factored each
  into a one indefinitely reusable definition, which allows safer
  programming, automatic propagation of pattern modification, etc.
  Plain assembler often is not enough, even when one is doing only small
  routines to link with C.



  4.1.  What's integrated into the above


  Yes I know this section does not contain much useful up-to-date
  information.  Feel free to contribute what you discover the hard
  way...



  4.1.1.  GCC

  GCC allows (and requires) you to specify register constraints in your
  inline assembly code, so the optimizer always know about it; thus,
  inline assembly code is really made of patterns, not forcibly exact
  code.

  Thus, you can make put your assembly into CPP macros, and inline C
  functions, so anyone can use it in as any C function/macro.  Inline
  functions resemble macros very much, but are sometimes cleaner to use.
  Beware that in all those cases, code will be duplicated, so only local
  labels (of 1: style) should be defined in that asm code.  However, a
  macro would allow the name for a non local defined label to be passed
  as a parameter (or else, you should use additional meta-programming
  methods).  Also, note that propagating inline asm code will spread
  potential bugs in them; so watch out doubly for register constraints
  in such inline asm code.

  Lastly, the C language itself may be considered as a good abstraction
  to assembly programming, which relieves you from most of the trouble
  of assembling.



  4.1.2.  GAS

  GAS has some macro capability included, as detailed in the texinfo
  docs.  Moreover, while GCC recognizes .s files as raw assembly to send
  to GAS, it also recognizes .S files as files to pipe through CPP
  before to feed them to GAS.  Again and again, see Linux sources for
  examples.



  4.1.3.  GASP

  It adds all the usual macroassembly tricks to GAS.  See its texinfo
  docs.



  4.1.4.  NASM

  NASM has comprehensive macro support, too.  See according docs.  If
  you have some bright idea, you might wanna contact the authors, as
  they are actively developing it.  Meanwhile, see about external
  filters below.



  4.1.5.  AS86

  It has some simple macro support, but I couldn't find docs.  Now the
  sources are very straightforward, so if you're interested, you should
  understand them easily.  If you need more than the basics, you should
  use an external filter (see below).



  4.1.6.  OTHER ASSEMBLERS


    Win32FORTH: CODE and END-CODE are normal that do not switch from
     interpretation mode to compilation mode, so you have access to the
     full power of FORTH while assembling.

    TUNES: it doesn't work yet, but the Scheme language is a real high-
     level language that allows arbitrary meta-programming.



  4.2.  External Filters

  Whatever is the macro support from your assembler, or whatever
  language you use (even C !), if the language is not expressive enough
  to you, you can have files passed through an external filter with a
  Makefile rule like that:


  ______________________________________________________________________
  %.s:    %.S other_dependencies
          $(FILTER) $(FILTER_OPTIONS) < $< > $@
  ______________________________________________________________________



  4.2.1.  CPP

  CPP is truly not very expressive, but it's enough for easy things,
  it's standard, and called transparently by GCC.

  As an example of its limitations, you can't declare objects so that
  destructors are automatically called at the end of the declaring
  block; you don't have diversions or scoping, etc.

  CPP comes with any C compiler.  However, considering how mediocre it
  is, stay away from it if by chance you can make it without C,



  4.2.2.  M4

  M4 gives you the full power of macroprocessing, with a Turing
  equivalent language, recursion, regular expressions, etc.  You can do
  with it everything that CPP cannot.

  See macro4th (this4th)
  <ftp://ftp.forth.org/pub/Forth/Compilers/native/unix/this4th.tar.gz>
  or the Tunes 0.0.0.25 sources
  <ftp://ftp.tunes.org/pub/tunes/obsolete/dist/tunes.0.0.0/tunes.0.0.0.25.src.zip>
  as examples of advanced macroprogramming using m4.

  However, its disfunctional quoting and unquoting semantics force you
  to use explicit continuation-passing tail-recursive macro style if you
  want to do advanced macro programming (which is remindful of TeX --
  BTW, has anyone tried to use TeX as a macroprocessor for anything else
  than typesetting ?).  This is NOT worse than CPP that does not allow
  quoting and recursion anyway.

  The right version of m4 to get is GNU m4 1.4 (or later if exists),
  which has the most features and the least bugs or limitations of all.
  m4 is designed to be slow for anything but the simplest uses, which
  might still be ok for most assembly programming (you're not writing
  million-lines assembly programs, are you?).



  4.2.3.  Macroprocessing with your own filter

  You can write your own simple macro-expansion filter with the usual
  tools: perl, awk, sed, etc.  That's quick to do, and you control
  everything.  But of course, any power in macroprocessing must be
  earned the hard way.



  4.2.4.  Metaprogramming

  Instead of using an external filter that expands macros, one way to do
  things is to write programs that write part or all of other programs.

  For instance, you could use a program outputting source code

    to generate sine/cosine/whatever lookup tables,

    to extract a source-form representation of a binary file,

    to compile your bitmaps into fast display routines,

    to extract documentation, initialization/finalization code,
     description tables, as well as normal code from the same source
     files,

    to have customized assembly code, generated from a
     perl/shell/scheme script that does arbitrary processing,

    to propagate data defined at one point only into several cross-
     referencing tables and code chunks.

    etc.

  Think about it!



  4.2.4.1.  Backends from compilers

  Compilers like GCC, SML/NJ, Objective CAML, MIT-Scheme, CMUCL, etc, do
  have their own generic assembler backend, which you might choose to
  use, if you intend to generate code semi-automatically from the
  according languages, or from a language you hack: rather than write
  great assembly code, you may instead modify a compiler so that it
  dumps great assembly code!



  4.2.4.2.  The New-Jersey Machine-Code Toolkit

  There is a project, using the programming language Icon (with an
  experimental ML version), to build a basis for producing assembly-
  manipulating code.  See around
  <http://www.eecs.harvard.edu/~nr/toolkit/>



  4.2.4.3.  TUNES


  The TUNES Project <http://www.tunes.org> for a Free Reflective
  Computing System is developing its own assembler as an extension to
  the Scheme language, as part of its development process.  It doesn't
  run at all yet, though help is welcome.

  The assembler manipulates abstract syntax trees, so it could equally
  serve as the basis for a assembly syntax translator, a disassembler, a
  common assembler/compiler back-end, etc.  Also, the full power of a
  real language, Scheme, make it unchallenged as for
  macroprocessing/metaprogramming.



  5.  CALLING CONVENTIONS



  5.1.  Linux



  5.1.1.  Linking to GCC

  This is the preferred way if you are developing mixed C-asm project.
  Check GCC docs and examples from Linux kernel .S files that go through
  gas (not those that go through as86).

  32-bit arguments are pushed down stack in reverse syntactic order
  (hence accessed/popped in the right order), above the 32-bit near
  return address.  %ebp, %esi, %edi, %ebx are callee-saved, other
  registers are caller-saved; %eax is to hold the result, or %edx:%eax
  for 64-bit results.

  FP stack: I'm not sure, but I think it's result in st(0), whole stack
  caller-saved.

  Note that GCC has options to modify the calling conventions by
  reserving registers, having arguments in registers, not assuming the
  FPU, etc. Check the i386 .info pages.

  Beware that you must then declare the cdecl or regparm(0) attribute
  for a function that will follow standard GCC calling conventions.  See
  in the GCC info pages the section: C Extensions::Extended Asm::.  See
  also how Linux defines its asmlinkage macro...



  5.1.2.  ELF vs a.out problems

  Some C compilers prepend an underscore before every symbol, while
  others do not.

  Particularly, Linux a.out GCC does such prepending, while Linux ELF
  GCC does not.

  If you need cope with both behaviors at once, see how existing
  packages do.  For instance, get an old Linux source tree, the Elk,
  qthreads, or OCaml...

  You can also override the implicit C->asm renaming by inserting
  statements like

  ______________________________________________________________________
          void foo asm("bar") (void);
  ______________________________________________________________________


  to be sure that the C function foo will be called really bar in assem
  bly.

  Note that the utility objcopy, from the binutils package, should allow
  you to transform your a.out objects into ELF objects, and perhaps the
  contrary too, in some cases.  More generally, it will do lots of file
  format conversions.



  5.1.3.  Direct Linux syscalls

  Often you will be told that using libc is the only way, and direct
  system calls are bad. This is true. To some extent.  So, you must know
  that libc is not sacred, and in most cases libc only does some checks,
  then calls kernel, and then sets errno.  You can easily do this in
  your program as well (if you need to), and your program will be dozen
  times smaller, and this will also result in improved performance, just
  because you're not using shared libraries (static binaries are
  faster).  Using or not using libc in assembly programming is more a
  question of taste/belief than something practical.  Remember, Linux is
  aiming to be POSIX compliant, so does libc. This means that syntax of
  almost all libc "system calls" exactly matches syntax of real kernel
  system calls (and vice versa). Besides, modern libc becomes slower and
  slower, and eats more and more memory, and so, cases of using direct
  system calls become quite usual.  But.. main drawback of throwing libc
  away is that possibly you will need to implement several libc specific
  functions (that are not just syscall wrappers) on your own (printf and
  Co.).. and you are ready for that, aren't you? :)


  Here is summary of direct system calls pros and cons.

  Pros:

    smallest possible size; squeezing the last byte out of the system.

    highest possible speed; squeezing cycles out of your favorite
     benchmark.

    full control: you can adapt your program/library to your specific
     language or memory requirements or whatever

    no pollution by libc cruft.

    no pollution by C calling conventions (if you're developing your
     own language or environment).

    static binaries make you independent from libc upgrades or crashes,
     or from dangling #! path to a interpreter (and are faster).

    just for the fun out of it (don't you get a kick out of assembly
     programming?)

  Cons:

    If any other program on your computer uses the libc, then
     duplicating the libc code will actually waste memory, not save it.

    Services redundantly implemented in many static binaries are a
     waste of memory.  But you can make your libc replacement a shared
     library.

    Size is much better saved by having some kind of bytecode,
     wordcode, or structure interpreter than by writing everything in
     assembly.  (the interpreter itself could be written either in C or
     assembly.)  The best way to keep multiple binaries small is to not
     have multiple binaries, but instead to have an interpreter process
     files with #! prefix.  This is how OCaml works when used in
     wordcode mode (as opposed to optimized native code mode), and it is
     compatible with using the libc.  This is also how Tom
     Christiansen's Perl PowerTools <http://language.perl.com/ppt/>
     reimplementation of unix utilities works.  Finally, one last way to
     keep things small, that doesn't depend on an external file with a
     hardcoded path, be it library or interpreter, is to have only one
     binary, and have multiply-named hard or soft links to it: the same
     binary will provide everything you need in an optimal space, with
     no redundancy of subroutines or useless binary headers; it will
     dispatch its specific behavior according to its argv[0]; in case it
     isn't called with a recognized name, it might default to a shell,
     and be possibly thus also usable as an interpreter!

    You cannot benefit from the many functionalities that libc provides
     besides mere linux syscalls: that is, functionality described in
     section 3 of the manual pages, as opposed to section 2, such as
     malloc, threads, locale, password, high-level network management,
     etc.

    Consequently, you might have to reimplement large parts of libc,
     from printf to malloc and gethostbyname.  It's redundant with the
     libc effort, and can be quite boring sometimes.  Note that some
     people have already reimplemented "light" replacements for parts of
     the libc -- check them out!  (Redhat's minilibc, Rick Hohensee's
     libsys <ftp://linux01.gwdg.de/pub/cLIeNUX/interim/libsys.tgz>,
     Felix von Leitner's dietlibc <http://www.fefe.de/dietlibc/>,
     Christian Fowelin's ``libASM'', ``asmutils'' project is working on
     pure assembly libc)

    Static libraries prevent your benefitting from libc upgrades as
     well as from libc add-ons such as the zlibc package, that does on-
     the-fly transparent decompression of gzip-compressed files.

    The few instructions added by the libc are a ridiculously small
     speed overhead as compared to the cost of a system call.  If speed
     is a concern, your main problem is in your usage of system calls,
     not in their wrapper's implementation.

    Using the standard assembly API for system calls is much slower
     than using the libc API when running in micro-kernel versions of
     Linux such as L4Linux, that have their own faster calling
     convention, and pay high convention-translation overhead when using
     the standard one (L4Linux comes with libc recompiled with their
     syscall API; of course, you could recompile your code with their
     API, too).

    See previous discussion for general speed optimization issue.

    If syscalls are too slow to you, you might want to hack the kernel
     sources (in C) instead of staying in userland.

  If you've pondered the above pros and cons, and still want to use
  direct syscalls (as documented in section 2 of the manual pages), then
  here is some advice.


    You can easily define your system calling functions in a portable
     way in C (as opposed to unportable using assembly), by including
     <asm/unistd.h>, and using provided macros.

    Since you're trying to replace it, go get the sources for the libc,
     and grok them.  (And if you think you can do better, then send
     feedback to the authors!)

    As an example of pure assembly code that does everything you want,
     examine ``Linux Assembly resources''.

  Basically, you issue an int 0x80, with the __NR_syscallname number
  (from asm/unistd.h) in eax, and parameters (up to five) in ebx, ecx,
  edx, esi, edi respectively.  Result is returned in eax, with a
  negative result being an error, whose opposite is what libc would put
  in errno.  The user-stack is not touched, so you needn't have a valid
  one when doing a syscall.

  As for the invocation arguments passed to a process upon startup, the
  general principle is that the stack originally contains the number of
  arguments argc, then the list of pointers that constitute *argv, then
  a null-terminated sequence of null-terminated variable=value strings
  for the environment.  For more details, do examine ``Linux assembly
  resources'', read the sources of C startup code from your libc (crt0.S
  or crt1.S), or those from the Linux kernel (exec.c and binfmt_*.c in
  linux/fs/).



  5.1.4.  Hardware I/O under Linux

  If you want to do direct I/O under Linux, either it's something very
  simple that needn't OS arbitration, and you should see the IO-Port-
  Programming mini-HOWTO; or it needs a kernel device driver, and you
  should try to learn more about kernel hacking, device driver
  development, kernel modules, etc, for which there are other excellent
  HOWTOs and documents from the LDP.

  Particularly, if what you want is Graphics programming, then do join
  one of the GGI <http://www.ggi-project.org/> or      XFree86
  <http://www.XFree86.org/> projects.

  Some people have even done better, writing small and robust XFree86
  drivers in an interpreted domain-specific language, GAL
  <http://www.irisa.fr/compose/gal/>, and achieving the efficiency of
  hand C-written drivers through partial evaluation (drivers not only
  not in asm, but not even in C!).  The problem is that the partial
  evaluator they used to achieve efficiency is not free software.  Any
  taker for a replacement?

  Anyway, in all these cases, you'll be better when using GCC inline
  assembly with the macros from linux/asm/*.h than writing full assembly
  source files.



  5.1.5.  Accessing 16-bit drivers from Linux/i386

  Such thing is theoretically possible (proof: see how DOSEMU
  <http://www.dosemu.org> can selectively grant hardware port access to
  programs), and I've heard rumors that someone somewhere did actually
  do it (in the PCI driver? Some VESA access stuff? ISA PnP? dunno).  If
  you have some more precise information on that, you'll be most
  welcome.  Anyway, good places to look for more information are the
  Linux kernel sources, DOSEMU sources (and other programs in the DOSEMU
  repository <ftp://tsx-11.mit.edu/pub/linux/ALPHA/dosemu/>), and
  sources for various low-level programs under Linux...  (perhaps GGI if
  it supports VESA).

  Basically, you must either use 16-bit protected mode or vm86 mode.

  The first is simpler to setup, but only works with well-behaved code
  that won't do any kind of segment arithmetics or absolute segment
  addressing (particularly addressing segment 0), unless by chance it
  happens that all segments used can be setup in advance in the LDT.

  The later allows for more "compatibility" with vanilla 16-bit
  environments, but requires more complicated handling.

  In both cases, before you can jump to 16-bit code, you must

    mmap any absolute address used in the 16-bit code (such as ROM,
     video buffers, DMA targets, and memory-mapped I/O) from /dev/mem to
     your process' address space,

    setup the LDT and/or vm86 mode monitor.

    grab proper I/O permissions from the kernel (see the above section)

  Again, carefully read the source for the stuff contributed to the
  DOSEMU project, particularly these mini-emulators for running ELKS
  and/or simple .COM programs under Linux/i386.



  5.2.  DOS

  Most DOS extenders come with some interface to DOS services.  Read
  their docs about that, but often, they just simulate int 0x21 and
  such, so you do "as if" you are in real mode (I doubt they have more
  than stubs and extend things to work with 32-bit operands; they most
  likely will just reflect the interrupt into the real-mode or vm86
  handler).

  Docs about DPMI (and much more) can be found on
  <ftp://x2ftp.oulu.fi/pub/msdos/programming/> (again, the original
  x2ftp site is closing (no more?), so use a mirror site
  <ftp://ftp.lip6.fr/pub/pc/x2ftp/README.mirror_sites>).

  DJGPP comes with its own (limited) glibc
  derivative/subset/replacement, too.

  It is possible to cross-compile from Linux to DOS, see the
  devel/msdos/ directory of your local FTP mirror for metalab.unc.edu
  Also see the MOSS dos-extender from the Flux project
  <http://www.cs.utah.edu/projects/flux/> from university of Utah.


  Other documents and FAQs are more DOS-centered.  We do not recommend
  DOS development.



  5.3.  Windows and Co.

  This HOWTO is not about Windows programming, you can find lots of
  documents about it everywhere..  The thing you should know is that
  Cygnus Solutions <http://www.cygnus.com> developed the cygwin32.dll
  library <http://sourceware.cygnus.com/cygwin/>, for GNU programs to
  run on Win32 platform; thus, you can use GCC, GAS, all the GNU tools,
  and many other Unix applications.


  5.4.  Your own OS

  Control is what attracts many OS developers to assembly, often is what
  leads to or stems from assembly hacking.  Note that any system that
  allows self-development could be qualified an "OS", though it can run
  "on the top" of an underlying system (much like Linux over Mach or
  OpenGenera over Unix).

  Hence, for easier debugging purpose, you might like to develop your
  "OS" first as a process running on top of Linux (despite the
  slowness), then use the Flux OS kit
  <http://www.cs.utah.edu/projects/flux/oskit/> (which grants use of
  Linux and BSD drivers in your own OS) to make it standalone.  When
  your OS is stable, it is time to write your own hardware drivers if
  you really love that.

  This HOWTO will not cover topics such as Boot loader code & getting
  into 32-bit mode, Handling Interrupts, The basics about Intel
  protected mode or V86/R86 braindeadness, defining your object format
  and calling conventions.

  The main place where to find reliable information about that all, is
  source code of existing OSes and bootloaders.  Lots of pointers are on
  the following webpage: <http://www.tunes.org/Review/OSes.html>



  6.  QUICK START

  Finally, if you still want to try this crazy idea and write something
  in assembly (if you've reached this section -- you're real assembly
  fan), I'll herein provide what you will need to get started.

  As you've read before, you can write for Linux in different ways; I'll
  show example of using pure system calls.  This means that we will not
  use libc at all, the only thing required for our program to run is
  kernel.  Our code will not be linked to any library, will not use ELF
  interpreter -- it will communicate directly with kernel.

  I will show the same sample program in two assemblers, nasm and gas,
  thus showing Intel and AT&T syntax.

  You may also want to read Introduction to UNIX assembly programming
  <http://linuxassembly.org/intro.html> tutorial, it contains sample
  code for other UNIX-like OSes.


  6.1.  Tools you need

  First of all you need assembler (compiler): nasm or gas.  Second, you
  need linker: ld, assembler produces only object code.  Almost all
  distributions include gas and ld, in binutils package.  As for nasm,
  you may have to download and install binary packages for Linux and
  docs from the ``nasm webpage''; however, several distributions
  (Stampede, Debian, SuSe) already include it, check first.

  If you are going to dig in, you should also install kernel source.  I
  assume that you are using at least Linux 2.0 and ELF.


  6.2.  Hello, world!

  Linux is 32bit and has flat memory model.  A program can be divided
  into sections.  Main sections are .text for your code, .data for your
  data, .bss for undefined data.  Program must have at least .text
  section.

  Now we will write our first program. Here is sample code:


  6.2.1.  NASM (hello.asm)



       ______________________________________________________________________
       section .data                           ;section declaration

       msg     db      "Hello, world!",0xa ;our dear string
       len     equ     $ - msg                 ;length of our dear string

       section .text                           ;section declaration

                               ;we must export the entry point to the ELF linker or
           global _start       ;loader. They conventionally recognize _start as their
                               ;entry point. Use ld -e foo to override the default.

       _start:

       ;write our string to stdout

               mov     edx,len ;third argument: message length
               mov     ecx,msg ;second argument: pointer to message to write
               mov     ebx,1   ;first argument: file handle (stdout)
               mov     eax,4   ;system call number (sys_write)
               int     0x80    ;call kernel

       ;and exit

               mov     ebx,0   ;first syscall argument: exit code
               mov     eax,1   ;system call number (sys_exit)
               int     0x80    ;call kernel
       ______________________________________________________________________



  6.2.2.  GAS (hello.S)



  ______________________________________________________________________
  .data                                 # section declaration

  msg:
          .string       "Hello, world!\n"  # our dear string
          len = . - msg                   # length of our dear string

  .text                                 # section declaration

                          # we must export the entry point to the ELF linker or
      .global _start      # loader. They conventionally recognize _start as their
                          # entry point. Use ld -e foo to override the default.

  _start:

  # write our string to stdout

          movl    $len,%edx       # third argument: message length
          movl    $msg,%ecx       # second argument: pointer to message to write
          movl    $1,%ebx         # first argument: file handle (stdout)
          movl    $4,%eax         # system call number (sys_write)
          int     $0x80           # call kernel

  # and exit

          movl    $0,%ebx         # first argument: exit code
          movl    $1,%eax         # system call number (sys_exit)
          int     $0x80           # call kernel
  ______________________________________________________________________



  6.3.  Producing object code

  First step of building binary is producing object file from source by
  invoking assembler; we must issue the following:

  For nasm example:

  $ nasm -f elf hello.asm

  For gas example:

  $ as -o hello.o hello.S

  This will produce hello.o object file.



  6.4.  Producing executable

  Second step is producing executable file itself from object file by
  invoking linker:

  $ ld -s -o hello hello.o

  This will finally build hello executable.

  Hey, try to run it... Works? That's it. Pretty simple.



  7.  RESOURCES


  You main resource for Linux/UNIX assembly programming material is
  Linux Assembly resources page
  <http://linuxassembly.org/resources.html>.  Do visit it, and get
  plenty of pointers to assembly projects, tools, tutorials,
  documentation, guides, etc, concerning different UNIX operating
  systems and CPUs.  Because it evolves quickly, I will no longer
  duplicate it in this HOWTO.

  If you are new to assembly in general, here are few starting pointers:



    The Art Of Assembly
     <http://webster.cs.ucr.edu/Page_asm/ArtOfAsm.html>

    x86 assembly FAQ <http://www2.dgsys.com/~raymoon/faq/>

    ftp.luth.se <ftp://ftp.luth.se/pub/msdos/> mirrors the hornet and
     x2ftp former archives of msdos assembly coding stuff

    CoreWars <http://www.koth.org>, a fun way to learn assembly in
     general

    Usenet: comp.lang.asm.x86 <news://comp.lang.asm.x86>; alt.lang.asm
     <news://alt.lang.asm>



  7.1.  Mailing list

  If you're are interested in Linux/UNIX assembly programming (or have
  questions, or are just curious) I especially invite you to join Linux
  assembly programming mailing list.

  This is an open discussion of assembly programming under Linux,
  FreeBSD, BeOS, or any other UNIX/POSIX like OS; also it is not limited
  to x86 assembly (Alpha, Sparc, PPC and other hackers are welcome
  too!).

  List address is  <mailto:linux-assembly@egroups.com>.

  To subscribe send a blank message to  <mailto:linux-
  assembly@egroups.com>.

  List archives are available at  <http://www.egroups.com/list/linux-
  assembly/>.



  7.2.  Frequently asked questions (with answers)

  Here are frequently asked questions. Answers are taken from the
  ``linux-assembly mailing list''.


  7.2.1.  How do I do graphics programming in Linux?

  An answer from Paul Furber <mailto:paulf@icom.co.za>:



  Ok you have a number of options to graphics in Linux. Which one you use
  depends on what you want to do. There isn't one Web site with all the
  information but here are some tips:

  SVGALib: This is a C library for console SVGA access.
  Pros: very easy to learn, good coding examples, not all that different
  from equivalent gfx libraries for DOS, all the effects you know from DOS
  can be converted with little difficulty.
  Cons: programs need superuser rights to run since they write directly to
  the hardware, doesn't work with all chipsets, can't run under X-Windows.
  Search for svgalib-1.4.x on http://ftp.is.co.za

  Framebuffer: do it yourself graphics at SVGA res
  Pros: fast, linear mapped video access, ASM can be used if you want :)
  Cons: has to be compiled into the kernel, chipset-specific issues, must
  switch out of X to run, relies on good knowledge of linux system calls
  and kernel, tough to debug
  Examples: asmutils (http://www.linuxassembly.org) and the leaves example
  and my own site for some framebuffer code and tips in asm
  (http://ma.verick.co.za/linux4k/)

  Xlib: the application and development libraries for XFree86.
  Pros: Complete control over your X application
  Cons: Difficult to learn, horrible to work with and requires quite a bit
  of knowledge as to how X works at the low level.
  Not recommended but if you're really masochistic go for it. All the
  include and lib files are probably installed already so you have what
  you need.

  Low-level APIs: include PTC, SDL, GGI and Clanlib
  Pros: very flexible, run under X or the console, generally abstract away
  the video hardware a little so you can draw to a linear surface, lots of
  good coding examples, can link to other APIs like OpenGL and sound libs,
  Windows DirectX versions for free
  Cons: Not as fast as doing it yourself, often in development so versions
  can (and do) change frequently.
  Examples: PTC and GGI have excellent demos, SDL is used in sdlQuake,
  Myth II, Civ CTP and Clanlib has been used for games as well.

  High-level APIs: OpenGL - any others?
  Pros: clean api, tons of functionality and examples, industry standard
  so you can learn from SGI demos for example
  Cons: hardware acceleration is normally a must, some quirks between
  versions and platforms
  Examples: loads - check out www.mesa3d.org under the links section.

  To get going try looking at the svgalib examples and also install SDL
  and get it working. After that, the sky's the limit.



  7.2.2.  How do I debug pure assembly code under Linux?


  If you're using gas, you should consult Linux assembly Tutorial
  <http://www.cs.pdx.edu/~bjorn/CS200/linux_tutorial/> by Bjorn
  Chambless.

  With nasm situation is a bit different, since it doesnot support gdb
  specific debugging extensions.  Although gdb is source-level debugger,
  it can be used to debug pure assembly code, and with some trickery you
  can make gdb to do what you need.  Here's an answer from Dmitry
  Bakhvalov <mailto:dl@gazeta.ru>:


  Personally, I use gdb for debugging asmutils. Try this:

  1) Use the following stuff to compile:
     $nasm -f elf -g smth.asm
     $ld -o smth smth.o

  2) Fire up gdb:
     $gdb smth

  3) In gdb:
     (gdb) disassemble _start
     Place a breakpoint at <_start+1> (If placed at _start the breakpoint
     wouldnt work, dunno why)
     (gdb) b *0x8048075

     To step thru the code I use the following macro:
     (gdb)define n
     >ni
     >printf "eax=%x ebx=%x ...etc...",$eax,$ebx,...etc...
     >disassemble $pc $pc+15
     >end

     Then start the program with r command and debug with n.

     Hope this helps.



  An additional note from ???:


      I have such a macro in my .gdbinit for quite some time now, and it
      for sure makes life easier. A small difference : I use "x /8i $pc",
      which guarantee a fixed number of disassembled instructions. Then,
      with a well chosen size for my xterm, gdb output looks like it is
      refreshed, and not scrolling.



  If you want to set breakpoints across your code, you can just use int
  3 instruction as breakpoint (instead of entering address manually in
  gdb).


  7.2.3.  Any other useful debugging tools?

  Definitely strace can help a lot (ktrace and kdump on FreeBSD), it is
  used to trace system calls and signals.  Read its manual page (man
  strace) and strace --help output for details.



  7.2.4.  How do I access BIOS functions from Linux (BSD, BeOS, etc)?

  Noway. This is protected mode, use OS services instead.  Again, you
  can't use int 0x10, int 0x13, etc.  Fortunately almost everything can
  be implemented through system calls or library functions.  In the
  worst case you may go through direct port access, or make a kernel
  patch to implement needed functionality.

  That's all for now, folks.

  $Id: Assembly-HOWTO.sgml,v 1.16 2000/07/11 10:38:10 konst Exp $



  Linux Astronomy HOWTO
  Elwood Downey and John Huggins howto@astronomy.net
  $Revision: 1.6 $, $Date: 2000/05/03 22:01:25 $

  This document shares tips and resources to utilize Linux solutions in
  the pursuit of Astronomy.
  ______________________________________________________________________

  Table of Contents


  1. Introduction

     1.1 Knowledge Required
     1.2 Scope
     1.3 Version
     1.4 Copyright

  2. Software

     2.1 Collections
     2.2 Planetarium Programs
     2.3 Libraries
     2.4 Other

  3. Astronomical Images over the web

     3.1 List

  4. Organizations

  5. Hardware Control

     5.1 Telescope Control
     5.2 CCD Camera Control

  6. Installation Help



  ______________________________________________________________________

  1.  Introduction

  1.1.  Knowledge Required


  With all the help from major Linux distributions such as SuSE, Redhat,
  Caldera and many others, Linux based systems are becoming easier to
  use.  However, there is still some need of understanding of basic UNIX
  skills to make the most of Linux.  Thus, this HOWTO will assume that
  the reader has at least a basic knowledge of using a UNIX system
  including the ability to compile and install programs.

  A few resources we have found useful over the years include:



    "A Practical Guide to the UNIX System", Mark G. Sobel

    "Advanced Programming in the UNIX Environment", the late W. Richard
     Stevens

    "Running LINUX", Matt Welsh et al.

    "LINUX Device Drivers", Alessandro Rubini



  Similarly, this is not a tutorial or reference for astronomy
  principles or

  astronomical instrumentation. Astronomy is perhaps the grandest of all

  sciences, employing widely disparate disciplines in a bold attempt to

  understand nothing less than the universe itself. Your interests will
  lead

  in many directions. A few references we have used include:



    "Astronomy with your Personal Computer", Peter Duffett-Smith

    "Astronomy on the Personal Computer", Oliver Montenbruck et al

    "Textbook on Spherical Astronomy", W. M. Smart

    "The Astronomy and Astrophysics Encyclopedia", Stephen P. Maran,
     ed.



  1.2.  Scope


  The authors define the scope of this HOWTO as primarily an index

  to Linux tools applicable in some fashion to the pursuit of Astronomy.
  It

  is *not* our intention to list WWW astronomy references in general.
  Our

  own interests tend more towards the technology than the pure science
  and so

  we welcome contributions from others who have found Linux tools which

  contribute in other ways to Astronomy. Please contact us at the
  address

  above.


  1.3.  Version


  $Revision: 1.6 $



  $Date: 2000/05/03 22:01:25 $



  The latest version of this document is always available on the
  Astronomy Net at Astronomy HOWTO.
  We eagerly accept suggestions from you.  Send them to Astronomy HOWTO
  Editors.



  1.4.  Copyright


  Copyright 2000 by Elwood Downey and John Huggins. This document may be
  distributed only subject to the terms and conditions set forth in the
  LDP License except that this document must not be distributed in
  modified form without the author's consent.



  A verbatim copy may be reproduced or distributed in any medium
  physical or electronic without permission of the author. Translations
  are similarly permitted without express permission if it includes a
  notice on who translated it.  Commercial redistribution is allowed and
  encouraged; however please notify authors of any such distributions.



  Excerpts from the document may be used without prior consent provided
  that the derivative work contains the verbatim copy or a pointer to a
  verbatim copy.



  Permission is granted to make and distribute verbatim copies of this
  document provided the copyright notice and this permission notice are
  preserved on all copies.



  In short, we wish to promote dissemination of this information through
  as many channels as possible. However, we wish to retain copyright on
  this HOWTO document, and would like to be notified of any plans to
  redistribute this HOWTO.



  2.  Software



  2.1.  Collections


  Here are some links to collections and other indexes of Linux
  astronomy

  software.


    The Linux for Astronomy CDROM

    Scientific Applications on Linux (SAL), Physics and Astronomy

    Linux Applications and Utilities Page, Science and Math



  2.2.  Planetarium Programs


  Here is discussion of whole programs for use in finding objects,
  natural and

  man-made, in the sky which run on Linux.



    XEphem

     has been the pet project of one of us (Downey) for the past

     15-odd years. It has grown to become one of the more capable

     interactive tools for the computation of astronomical ephemerides.



    XSky

     is by Terry R. Friedrichsen, terry@venus.sunquest.com. XSky is

     essentially an interactive sky atlas.



     Skymap

     is an astronomical mapping program written in Fortran and C for
     unix workstations by Doug Mink of the Smithsonian Astrophysical
     Observatory Telescope Data Center.



     Xplns

     reproduces real starry sky on your display of X Window System.



     AstrHorloge

     is a small astronomy software that shows a sky map, give you the
     coordinates of

     stars and planets.



  2.3.  Libraries


  This section discusses bits and pieces of software that can be used to
  form the

  basis for specialized projects.



    SLALIB, part of the

     Starlink Project, is a

     complete library of subroutines for astrometric computations.



    Astrophysics Source Code Library

     is a collection of links to numerical astrophysical process models.



    Astronomy and numerical software source codes is a collection of C
     codes related to astronomy.



    How to compute planetary positions.



  2.4.  Other


  Every list needs a miscellaneous section, and this is it for Software.



    IRAF is a gigantic

     but exceptionally capable astronomical analysis system,

     shepherded over the past 20-odd years by Doug Tody of NOAO.

     It has accumulated innumerable authoritative contributions from
     leading

     astronomers in all areas of astronomical data analysis. If you have
     a serious

     interest in astronomical data reduction and significant time to
     invest, this

     system will reward you mightily.



    Nightfall Eclipsing Binary Star Program



  3.  Astronomical Images over the web


  Much effort exists to allow access to Astronomical image file type
  such as FITS from any web browser.  Here are some pointers.



  3.1.  List


  The folks at harvard have a list of Image Servers and Image Browsers.



    Astronomical Images Over the Web



  4.  Organizations



    The yearly Astronomical Data Analysis Software and Systems, ADAAS,

     Conference Series provides a forum for scientists and computer
     specialists

     concerned with algorithms, software and operating systems in the
     acquisition,

     reduction and analysis of astronomical data.  The program includes
     invited

     talks, contributed papers and poster sessions as well as user group
     meetings

     and special interest meetings ("BOFs'').  All these activities aim
     to

     encourage communication between software specialists and users, and
     also to

     stimulate further development of astronomical software and systems.



     The linuxastro mailing list, linuxastro@majordomo.cv.nrao.edu, is
     for

     people who are interested in porting astronomical software to
     linux. For

     more information, see

     linuxastro.



  5.  Hardware Control


  More folks are using Linux to control equipment.  Users range from
  amateur astronomers in the field to professional observatories.



  5.1.  Telescope Control



     OCAAS is a

     complete Observatory Control and Astronomical Analysis System for
     Linux.



    XEphem

     has the capability to communicate with a telescope control daemon
     process.



  5.2.  CCD Camera Control



     Apogee Instruments Inc supports their line of professional CCD
     cameras under Linux.



     SBIG offers some assistance with operating their ST7 and ST8 CCD
     cameras under Linux.



  6.  Installation Help


  You need to know what you're doing with Linux and installing programs,
  but help is available for some programs.  Here are some ways to make
  life easier.



     AstroMake

     is is a utility intended to make installations of some common
     astronomical packages (in binary form) easy.



     XEphem requires several elements to exist on your machine.  Life
     is much

     simpler with the CDROM version of the program as it contains an
     installation

     script which loads the appropriate precompiled binary for most
     systems and

     places all auxiliary files to the correct spots. See

     XEphem CDROM




Linux BRIDGE-STP-HOWTO

Uwe Bhme

             Johann-Heinrich-Abt-Strae 7
             95213
             Mnchberg
             Germany
             +49/9251 960877
             +49/9251 960878
             uwe@bnhof.de
   
Lennert Buytenhenk

   bridge code maintainer and developer
   gnu.org
   
             buytenh@gnu.org
   
   Still draft
   
   Copyright  2000 by Uwe Bhme
   Revision History
   Revision v0.00 01 June 2000 Revised by: U.B.
   Initial Release.
   Revision v1.01 07 June 2000 Revised by: U.B.
   Applied patch from Lennert. Corrected some syntactical errors.
   Completed some brctl commands. Added test output and description.
   Revision v1.02 08 June 2000 Revised by: U.B.
   More typo and grammar corrections.
   Revision v1.03 09 June 2000 Revised by: U.B.
   The usual typo. Applied Lennert's explanations about the message logs
   of the pull-the-plug-test.
   Revision v1.04 11 June 2000 Revised by: U.B.
   The usual typo. Applied ultimate test dumps.
   Revision v1.05 17 June 2000 Revised by: U.B.
   System freeze remark. Modified style sheet.
   Revision v0.01 25 June 2000 Revised by: U.B.
   Changes name from BRIDGE-HOWTO to BRIDGE-STP-HOWTO (avoid interference
   with BRIDGE-HOWTO by Christopher Cole) and restart Version numbering
   (we where already too far). Lennert Buytenhenk announced as coauthor.
     _________________________________________________________________
   
   Table of Contents
   1. [1]License
   2. [2]What Is A Bridge?
   3. [3]Rules On Bridging
   4. [4]Preparing The Bridge
          
        4.1. [5]Get The Files
        4.2. [6]Apply The Patches
        4.3. [7]Configure The Kernel
        4.4. [8]Compile The Kernel
        4.5. [9]Compile The Bridge Utilities
                
   5. [10]Set Up The Bridge
          
        5.1. [11]brctl Command Synopsis
        5.2. [12]Basic Setup
                
   6. [13]Advanced Bridge Features
          
        6.1. [14]Spanning Tree Protocol
        6.2. [15]Bridge And The IP-Chains
                
   7. [16]A Practical Setup Example
          
        7.1. [17]Hardware-setup
        7.2. [18]Software-setup
        7.3. [19]See It Work
        7.4. [20]Bridge Tests
                
   Appendix A. [21]Network Interface Cards
   Appendix B. [22]Recommended Reading
   Appendix C. [23]FAQ
          
   About The Linux Modular Bridge And STP
   
   This document describes how to setup a bridge with the recent kernel
   patches and brctl utility by Lennert Buytenhek. With developer kernel
   2.3.47 the new bridging code is part of the mainstream. On 20.06.2000
   there are patches for stable kernels 2.2.14 and 2.2.15. What happend
   if a penguin crosses a bridge?
     _________________________________________________________________
   
1. License

   Copyright (c) 2000 by Uwe Bhme. This document may be distributed only
   subject to the terms and conditions set forth in the [24]LDP License
   available at [25]http://sunsite.unc.edu/LDP/LICENSE.html
     _________________________________________________________________
   
2. What Is A Bridge?

   A bridge is a device that separates two or more network segments
   within one logical network (e.g. IP-subnet).
   
   A bridge is usually placed between two separate groups of computers
   that talk with each other, but not that much with the computers in the
   other group. A good example of this is to consider a cluster of
   Macintoshes and a cluster of Unix machines. Both of these groups of
   machines tend to be quite chatty amongst themselves, and the traffic
   they produce on the network causes collisions for the other machines
   who are trying to speak to one another.
   
   The job of the bridge is to examine the destination of the data
   packets one at a time and decide whether or not to pass the packets to
   the other side of the Ethernet segment. The result is a faster,
   quieter network with less collisions.
   
   The bridging code decides whether to bridge data or to drop it not by
   looking at the protocol type (IP, IPX, NetBEUI), but by looking at the
   MAC-address unique to each NIC.
   
   Important
   
   It's vital to understand that a bridge is neither a router nor a
   fire-wall. Spoken in simple term a bridge behaves like a network
   switch (i.e. Layer 2 Switch), making it a transparent network
   component (which is not absolutely true, bat nearly). Read more about
   this at [26]Section 3.
   
   In addition, you can overcome hardware incompatibilities with a
   bridge, without leaving the address-range of your IP-net or subnet.
   E.g. it's possible to bridge between different physical media like 10
   Base T and 100 Base TX.
   
   My personal reason for starting to set up a bridge was that in my work
   I had to connect Fast Ethernet components to a existing HP Voice Grade
   network, which is a proprietary networking standard.
   
   Features Above Pure Bridging
   
   STP
          The Spanning Tree Protocol is a nifty method of keeping
          Ethernet devices connected in multiple paths working. The
          participating switches negotiate the shortest available path by
          STP. This feature will be discussed in [27]Section 6.1.
          
   Multiple Bridge Instances
          Multiple bridge instances allow you to have more than one
          bridge on your box up and running, and to control each instance
          separately.
          
   Fire-walling
          There is a patch to the bridging code which allows you to use
          IP chains on the interface inside a bridge. More info about
          this you'll find at [28]Section 6.2.
     _________________________________________________________________
   
3. Rules On Bridging

   There is a number of rules you are not allowed to break (otherwise
   your bridge does).
   
     * A port can only be a member of one bridge.
     * A bridge knows nothing about routes.
     * A bridge knows nothing about higher protocols than ARP. That's the
       reason why it can bridge any possible protocol possibly running on
       your Ethernet.
     * No matter how many ports you have in your logical bridge, it's
       covered by only one logical interface
     * As soon as a port (e.g. a NIC) is added to a bridge you have no
       more direct control about it.
       
   Warning
   
   If one of the points mentioned above is not clear to you now, don't
   continue reading. Read the documents listed in [29]Appendix Appendix B
   first.
   
   If you ever tried to ping an unmanaged switch, you will know that it
   doesn't work, because you don't have a IP-address for it. To switch
   datagrams it doesn't need one. The other thing is if you want to
   manage the switch. It's too much strain, to take a dumb terminal, walk
   to the place you installed it (normally a dark, dusty and warm room,
   with a lot of green and red Christmas lights), to connect the terminal
   and to change the settings.
   
   What you want is remote management, usually by SNMP, telnet, rlogin or
   (best) ssh. For all this services you will need a IP. That's the
   exception to the transparency. The new code allows you without any
   problem to assign a IP address to the virtual interface formed by the
   bridge-instance you will create in [30]Section 5.2. All NIC's (or
   other interfaces) in your bridge will happily listen and respond to
   datagrams destined to this IP.
   
   All other data will not interfere with the bridge. The bridge just
   acts like a switch.
     _________________________________________________________________
   
4. Preparing The Bridge

   This section describes what you need and how you do to prepare your
   bridge.
     _________________________________________________________________
   
4.1. Get The Files

   Here you can find a list of the files and down-loads you will need for
   the setup of the bridge. If you have one of the mentioned files or
   packages on your distribution, of course there is no need to create
   network load.
   
   I'll only mention the files for the 2.2.14 kernel. If you want to try
   a different one (e.g. 2.2.15 or the recent development kernel) just
   replace the kernel version number and look whether you find it.
   
   File and package list
   
   Unpatched kernel-sources
          E.g. linux-2.2.14.tar.bz2 available from your local kernel.org
          mirror. Please check first if you find it in your distribution
          (take unpatched kernel-sources). If you don't, please check
          [31]The Linux Kernel Archive Mirror System for a close by
          mirror and down-load it from there.
          
   Bridge patches
          
   Note
   
   If your kernel is later than 2.3.47 you don't need this. The bridging
   is part of the mainstream from that version.
   
          Get the bridge kernel patches for your kernel version from
          [32]http://www.openrock.net/bridge/. Identify the file by the
          kernel number.
          
   Note
   
   There are also patches allowing to work with IP chains. I never tried
   it, for I don't see the need to fire-wall inside my LAN, and
   absolutely no need to bridge against the outer world. Feel free to
   contribute about that issue.
   
   Kernel patches for the stable 2.2 kernel. 
          + [33]bridge-0.0.5-against-2.2.14.diff
          + [34]bridge-0.0.5-against-2.2.15.diff
            
   Bridge configuration utilities
          You also will need the bridge configuration utilities to set up
          the bridge [35]Section 5. You can also download them from
          [36]http://www.openrock.net/bridge/. The current one (as of
          this writing) is bridge-utils-0.9.1.tar.gz.
          [37]bridge-utils-0.9.1.tar.gz.
     _________________________________________________________________
   
4.2. Apply The Patches

   Note
   
   If your kernel is later than 2.3.47 you don't need this. The bridging
   is part of the mainstream from that version.
   
   Apply the bridging patch your kernel. If you don`t know how to do that
   read the Kernel-HOWTO which can be found in your distribution or at
   [38]http://sunsite.unc.edu/LDP/HOWTO/HOWTO-INDEX.html 
   
   Example 1. Applying a kernel patch
root@mbb-1:~ # cd /usr/src/linux-2.2.14
root@mbb-1:/usr/src/linux-2.2.14 # patch -p1 < \
    bridge-0.0.5-against-2.2.14.diff
.
.
     _________________________________________________________________
   
4.3. Configure The Kernel

   Now it's time we configure our freshly patched kernel to create the
   ability to bridge.
   
   Run make config, make menuconfig or the click-o-rama make xconfig.
   Select bridging in the networking option section to be compiled as a
   module. AFAIK there is no strong reason why not to compile it as a
   kernel module, whereas I heard rumors about problems with compiling
   the bridging code directly into the kernel.
   
root@mbb-1:~ # cd /usr/src/linux-2.2.14
root@mbb-1:/usr/src/linux-2.2.14 # make menuconfig
.
     _________________________________________________________________
   
4.4. Compile The Kernel

   Compile your kernel [39]Example 2. Make the new compiled kernel-image
   to be loaded. I don't know if the kernel patches only apply to the
   bridging-module or also modify some interfaces inside vmlinuz. So it
   might not be a error to give a reboot after you updated the
   kernel-image.
   
   Example 2. Commands To Compile Your Kernel
root@mbb-1:/usr/src/linux-2.2.14 # make dep clean zImage modules modules_instal
l zlilo
...
     _________________________________________________________________
   
4.5. Compile The Bridge Utilities

   There is no magic about it. Just unzip the utilities-tarball, cd into
   the newly created directory and give a make.
   
   Example 3. Commands To Compile Your Bridge-Utilities
root@mbb-1:/usr/src/linux-2.2.14 # cd /usr/local/src
root@mbb-1:/usr/local/src/ # tar xzvf bridge-utils-0.9.1.tar.gz
.....
....
root@mbb-1:/usr/local/src # cd bridge
root@mbb-1:/usr/local/src/bridge # make
.....
....

   After the compilation shown in [40]Example 3 have worked properly, you
   can copy the executables to let's say /usr/sbin/ (at least I did). So
   the commands you have to give should be clear, but to be complete see
   [41]Example 4
   
   Example 4. Copy The Binaries Of The Utilities
root@mbb-1:/usr/local/src/bridge # cd brctl
root@mbb-1:/usr/local/src/bridge/brctl # cp brctl /usr/bin/local
root@mbb-1:/usr/local/src/bridge/brctl # chmod 700 /usr/bin/local/brctl
root@mbb-1:/usr/local/src/bridge/brctl # cp brctld /usr/bin/local
root@mbb-1:/usr/local/src/bridge/brctl # chmod 700 /usr/bin/local/brctld

   Also now you can copy the new man-page to a decent place, as shown in
   [42]Example 5.
   
   Example 5. Copy The Man-page Of brctl
root@mbb-1:/usr/local/src/bridge # cd doc
root@mbb-1:/usr/local/src/bridge/doc #  gzip -c brctl.8 > /usr/local/man/man8/b
rctl.8.gz
     _________________________________________________________________
   
5. Set Up The Bridge

   Make sure all your network cards are working nicely and are
   accessible. If so, ifconfig will show you the hardware layout of the
   network-interface. If you have problems making your cards work please
   read the Ethernet-HOWTO at
   [43]http://sunsite.unc.edu/LDP/HOWTO/HOWTO-INDEX.html . Don't mess
   around with IP-addresses or net-masks. You will not need it, until you
   bridge fully operational an up.
   
   After you did the steps mentioned above a modprobe -v bridge should
   show no errors. Also for each of the network cards you want to use in
   the bridge the ifconfig whateverNameYourInterfaceHas should give you
   some information about the interface.
   
   If your bridge-utilities have been correctly built and your kernel and
   bridge-module are OK, then issuing a brctl should show a small command
   synopsis.
     _________________________________________________________________
   
5.1. brctl Command Synopsis

root@mbb-1:~ # brctl
commands:
    addbr           <bridge>                add bridge                       (1
)
    addif           <bridge> <device>       add interface to bridge          (2
)
    delbr           <bridge>                delete bridge                    (3
)
    delif           <bridge> <device>       delete interface from bridge     (4
)
    show                                    show a list of bridges           (5
)
    showbr          <bridge>                show bridge info                 (6
)
    showmacs        <bridge>                show a list of mac addrs         (7
)

    setageing       <bridge> <time>         set ageing time                  (8
)
    setbridgeprio   <bridge> <prio>         set bridge priority              (9
)
    setfd           <bridge> <time>         set bridge forward delay         (1
0)
    setgcint        <bridge> <time>         set garbage collection interval  (1
1)
    sethello        <bridge> <time>         set hello time                   (1
2)
    setmaxage       <bridge> <time>         set max message age              (1
3)
    setpathcost     <bridge> <port> <cost>  set path cost                    (1
4)
    setportprio     <bridge> <port> <prio>  set port priority                (1
5)
    stp             <bridge> <state>        {dis,en}able stp                 (1
6)

   [44](1) [45](3) 
          The brctl addbr bridgename command creates a logical bridge
          instance with the name bridgename. You will need at least one
          logical instance to do any bridging at all. You can interpret
          the logical bridge being a container for the interfaces taking
          part in the bridging. Each bridging instance is represented by
          a new network interface.
          
   Example 6. Creating A Instance
root@mbb-1:~ # brctl addbr mybridge1


          The corresponding "shutdown" command is brctl delbr bridgename.
          
   Caution
   
   brctl delbr bridgename will only work, if there are no more interfaces
   added to the instance you want to delete.
   [46](2) [47](4) 
          The brctl addif bridgename device command enslaves the network
          device device to take part in the bridging of bridgename. As a
          simple explanation, each interface enslaved into a instance is
          bridged to the other interfaces of the instance.
          The corresponding command to tale a interface out of the bridge
          would be brctl delif bridgename device
   [48](5) 
          The brctl show command gives you a summary about the overall
          bridge status, and the instances running as shown in
          [49]Example 7. If you are interested in detailed information
          about a instance and it's interfaces you will have to check
          [50](6) .
          
   Example 7. Output Of brctl show
root@mbb-1:~ # brctl show
bridge name     bridge id               stp enabled
mybridge1       0000.0800062815f6       yes


   [51](6) 
          The brctl showbr bridgename command gives you a summary about a
          bridge instance and it's enslaved interfaces.
          
   Example 8. Output Of brctl showbr bridgename
root@mbb-1:~ # brctl showbr mybridge1
mybridge1
 bridge id              0000.0800062815f6
 designated root        0000.0800062815f6
 root port                 0                    path cost                  0
 max age                   4.00                 bridge max age             4.00
 hello time                1.00                 bridge hello time          1.00
 forward delay             4.00                 bridge forward delay       4.00
 ageing time             300.00                 gc interval                4.00
 hello timer               0.84                 tcn timer                  0.00
 topology change timer     0.00                 gc timer                   1.84
 flags


eth0 (1)
 port id                8001                    state                   forward
ing
 designated root        0000.0800062815f6       path cost                100
 designated bridge      0000.0800062815f6       message age timer          0.00
 designated port        8001                    forward delay timer        0.00
 designated cost           0                    hold timer                 0.84
 flags

eth1 (2)
 port id                8002                    state                   forward
ing
 designated root        0000.0800062815f6       path cost                100
 designated bridge      0000.0800062815f6       message age timer          0.00
 designated port        8002                    forward delay timer        0.00
 designated cost           0                    hold timer                 0.84
 flags


   [52](7) 
          The brctl showmacs bridgename command gives you a list of
          MAC-addresses of the interfaces which are enslaved in
          bridgename.
          
   Example 9. Output Of brctl showmacs bridgename
root@mbb-1:~ # brctl showmacs mybridge1
port no mac addr                is local?       ageing timer
  1     00:10:4b:b6:c6:e4       no               119.25
  1     00:50:04:43:82:85       no                 0.00
  1     00:50:da:45:45:b1       no                76.75
  1     00:a0:24:d0:4c:d6       yes                0.00
  1     00:a0:24:f0:22:71       no                 5.81
  1     08:00:09:b5:dc:41       no                22.22
  1     08:00:09:fb:39:a1       no                27.24
  1     08:00:09:fc:92:2c       no                53.13
  4     08:00:09:fc:d2:11       yes                0.00
  1     08:00:09:fd:23:88       no               230.42
  1     08:00:09:fe:0d:6f       no               144.55


   [53](8) 
          Sets the aging time. The aging time is the number of seconds a
          MAC-address will be kept in the forwarding database after
          having received a packet from this MAC address. The entries in
          the forwarding database are periodically timed out to ensure
          they won't stay around forever. Normally there should be no
          need to modify this parameter.
   [54](9) 
          Sets the bridge's relative priority. The bridge with the lowest
          priority will be elected as the root bridge. The root bridge is
          the "central" bridge in the spanning tree. More information
          about STP you find at [55]Section 6.1.
   [56](10) 
          Sets the forwarding delay time. The forwarding delay time is
          the time spent in each of the Listening and Learning states
          before the Forwarding state is entered.
   [57](11) 
          Sets the garbage collection interval. Every (this number)
          seconds, the entire forwarding database is checked for
          timed-out entries. The timed-out entries are removed.
   [58](12) 
          Sets the hello time. Every (this number) seconds, a hello
          packet is sent out by the Root Bridge and the Designated
          Bridges. Hello packets are used to communicate information
          about the topology throughout the entire Bridged Local Area
          Network. More information about STP you find at [59]Section
          6.1.
   [60](13) 
          Sets the maximum message age. If the last seen (received) hello
          packet is more than this number of seconds old, the bridge in
          question will start the takeover procedure in attempt to become
          the Root Bridge itself. More information about STP you find at
          [61]Section 6.1.
   [62](14) 
          Sets the cost of receiving (or sending, I'm not sure) a packet
          on this interface. Faster interfaces should have lower path
          costs. These values are used in the computation of the minimal
          spanning tree. More information about STP you find at
          [63]Section 6.1. Paths with lower costs are likelier to be used
          in the spanning tree than high-cost paths (As an example, think
          of a gigabit line with a 100Mbit or 10Mbit line as a backup
          line. You don't want the 10/100Mbit line to become the primary
          line there.)
          The Linux implementation currently sets the path cost of all
          eth* interfaces to 100, the nominal cost for a 10Mbit
          connection. There is unfortunately no easy way to discern
          10Mbit from 100Mbit from 1Gbit Ethernet cards, so the bridge
          cannot use the real interface speed.
   [64](16) 
          With this parameter you can enable or disable the Spanning Tree
          Protocol.
   [65](9) [66](12) [67](14) [68](16) 
          This parameters are only of interest, if you have more than one
          bridge in your LAN and stp enabled. Before modifying them you
          should read [69]Section 6.1.
     _________________________________________________________________
   
5.2. Basic Setup

   The standard configuration should consist of:
   
    1. Create the bridge interface.
       
root@mbb-1:~ # brctl addbr mybridge


    2. Add interfaces to the bridge.
       
root@mbb-1:~ # brctl addif mybridge eth0
root@mbb-1:~ # brctl addif mybridge eth1


    3. Zero IP the interfaces.
       
root@mbb-1:~ # ifconfig eth0 0.0.0.0
root@mbb-1:~ # ifconfig eth1 0.0.0.0


    4. Optionally you can configure the virtual interface mybridge to
       take part in your network. It behaves like one interface (like a
       normal network card). Exactly that way you configure it.
       
root@mbb-1:~ # ifconfig mybridge 192.168.100.5 netmask 255.255.255.0 up


   A more sophisticated setup script you will find at [70]Example 16.
   
   Important
   
   If you get the terrible experience of a frozen system or some nasty
   behavior of your nicely shaped linux box at
root@mbb-1:~ # ifconfig ethn 0 0.0.0.0

   please try (after the reboot of the system if necessary) before
   starting any bridge stuff at all a
root@mbb-1:~ # ifconfig ethn promisc up

   If again your system is frozen it's you NIC's driver you have to
   blame, not the bridging code.
     _________________________________________________________________
   
6. Advanced Bridge Features

   Here we will cover some advanced features of the new bridge code.
     _________________________________________________________________
   
6.1. Spanning Tree Protocol

   Tell me... 
   
     * You are a networkadmin...?
     * You have a switch on top of your ethernet tree...?
     * You have nightmares of a switch emmiting smoke...?
     * Your company is not extremely rich and con provide another
       redundant switch just waiting for you to plug the patchwires..?
     * You don't feel like placing your bed close to your main network
       node to plug the wires...?
       
   Don't wait until you're just another nervous wreck. Join linux bridge
   community and enjoy the relaxment a stp-enabled inhouse scenario is
   offering to you.
   
   Ok, let's leave that commercial and get back linux and the bridge.
   Take a look on this small thread from the linux-bridge mailing list.
   
STP Thread from bridge@openrock.net

   [71]Could you give me some hints about multi-bridge scenarios. 
   [72]Does the STP "heartbeat" mechanism also work with bridges with
          more than two cards? 
          
   [73]How fast does it get up, and what can I do about it? 
          
   Could you give me some hints about multi-bridge scenarios.
   
   You can just set up two "mirrored" bridges. You have two network
   interfaces in your bridge? Set up the mirror bridge so that it has two
   network interfaces as well, and connect each of the interfaces to one
   subnet. This will work without the need of configuration.
   
   Note
   
   Be sure that you have the spanning tree protocol enabled. If you
   didn't use brctl, this should be fine, because in Linux, it is on by
   default. To check, you could check whether the bridge sends a packet
   to 0180c2000000 every 2 seconds. If it does, the STP is on. The STP is
   needed so that only one bridge will be active at any given time.
   
   The "master" bridge will send out STP packets every 2 seconds by
   default. The "slave" bridge will receive these packets, and will
   notice that the master is still up. If the slave hasn't received a
   packet in 20 seconds (max. message age parameter), it will start the
   takeover procedure. From the moment the takeover procedure starts, it
   will take about 30 seconds (twice the forward delay parameter) for the
   bridge to become fully operational.
   
   Does the STP "heartbeat" mechanism also work with bridges with more
   than two cards?
   
   Yes, it works with any number of interfaces. You can invent bizarre
   topologies to your heart's desire. You can use multiple (redundant)
   bridge-bridge connects, you can insert loops, whatever. The STP code
   will always find the minimal spanning tree. The bridge code will even
   deal with the loss of any number of interfaces. If there are two
   redundant bridges with identical connections, the loss of an interface
   on one of the bridges will cause the other bridge to take over
   forwarding to that specific interface. Now isn't that great? :)
   
   How fast does it get up, and what can I do about it?
   
   If you think 50 seconds is too much -- and I guess you should; alas,
   the IEEE specs gives us these default values -- you can tweak these
   parameters. If you set the hello time (the STP packet interval) from 2
   to 1 second, you can safely set the message age parameter to 4
   seconds. Then you can set the forward delay to 4 seconds, and this
   will in total give you a takeover time of ~12 seconds.
   
   The great thing which is made possible by STP is a redundant parallel
   bridging scenario, with automated take over features. Within a network
   basing on stp the bridges always try to send a datagram the (by path
   cost) shortest path. Only on that path the bridges are forwarding, all
   other paths between this shortest way are blocked. If there is a
   broken path, the bridges agree about the next shortest. So doubled
   paths don't break the net, but are bringing more security... For a
   example setup of a fail secured connection see [74]Section 7.
     _________________________________________________________________
   
6.2. Bridge And The IP-Chains

   The normal idea about a bridge would not allow anything like
   firewalling, but since several people have asked Lennert for ipchains
   firewalling on bridge forwarding he implemented it.
   
   Important
   
   If you want to do this, you will need to apply the special
   ip-chain-bridge-patch (also available at [75]the bridge homepage).
   
   As soon you have everything up correctly, the bridging code will check
   each to-be-forwarded packet against the ipchains chain which has the
   same name as the bridge.
   
   So.. if a packet on eth0 is to be forwarded to eth1, and those
   interfaces are both part of the bridge group br0, the bridging code
   will check the packet against the chain called 'br0'.
   
   Warning
   
   If the chain does not exist, the packet will be forwarded. So if you
   want to do firewalling, you'll have to create the chain yourself.
   
   Example 10. A Simple Bridge Firewall Setup
Example:
# brctl addbr br0                                    (1)
# brctl addif br0 eth0                               (2)
# brctl addif br0 eth1                               (3)
# ifconfig br0 10.0.0.254                            (4)
# ipchains -N br0                                    (5)
# ipchains -A br0 -s 10.0.0.1/8 -i eth0 -j DENY      (6)

   [76](1) 
          Creating a bridge interface named br0
   [77](2) [78](3) 
          Placing eth0 and eth1 into the bridge.
   [79](4) 
          Assigning a regular IP address to the bridge. The IP is taken
          from private network 10.X.X.X (Class A).
   [80](5) 
          Creating a ip-chain named br0
   [81](1) [82](5) 
          
   Caution
   
   It's vital to have the same name here (br0 or whatever you have
   selected, as long as you have the same in all places).
   [83](6) 
          Denying all trafic with source 10.X.X.X on eth0.
     _________________________________________________________________
   
7. A Practical Setup Example

   This is a real-world example which is currently working in our
   network. Even if it's for sure not a very common situation it might be
   useful.
   
   I had to solve a small hardware incompatibility. HP-VG (Voice Grade)
   100Mbit network is not fast Ethernet compatible. Having neither the
   money nor the will to replace the stuff and having the need to expand
   the system I had to find a solution which was a) stable and b) cheap.
   
   For sure buying a HP modular switch was not meeting condition b). So I
   remembered I heard about Linux-bridging which automatically fulfilled
   condition a) and b).
   
   So quite some time ago I successfully set up a bridge between the two
   incompatible networks. Its first hardware-layout is shown in
   [84]Figure 1.
   
   Figure 1. Hardware setup Of The Old Bridge Scenario
   
   [old-hardware-setup.png]
   
   The old setup of my previous linux bridge
   
   It was configured as a transparent network component, meaning it
   didn't take a part in the network, but only bridged it. Originally it
   was set up on kernel 2.0.35 from a SuSE 5.3 distribution.
   
   The next problem showed up at once. A single bridge connecting the big
   segments might be c) a bottleneck and d) a reason to kill the
   netadmin, if it blows up. So I tried to find some solution for that
   problem.
   
   What happened next was that I discovered some hints that a new
   maintainer took over the bridging code. A few mails on the
   bridge-mailing list later as shown in [85]Section 6.1 I was more
   clever. The new modular bridging code fulfilled exactly what I was
   looking for.
   
   The new maintainer: Lennert Buytenhek . His project page can be found
   at [86]http://openrock.net/bridge IMHO he's doing a great job. Thanks
   a lot.
     _________________________________________________________________
   
7.1. Hardware-setup

   The ideas and hints I got from the mailing list discussion shown in
   [87]Section 6.1 lead to a new hardware-setup shown in [88]Figure 2.
   The setup is intended to provide a default machine (guess which one).
   The bridge has 3 HP cards of which each is connected to a HP VG15 hub.
   The 3com card is connected to a 3com Superstack Fast Ethernet switch.
   
   Figure 2. Hardware Setup Of The Multi bridge Scenario
   
   [hardware-setup.png]
   
   The practically working setup of my local linux Ethernet multi bridge
   
   This setup is not only fail proof to any one of the bridge's
   interfaces being down, but also to complete blackout of one of the
   bridges. Additional advantage to the old-setup [89]Figure 1 that the
   single HUBS are switched. This means that a datagram being sent from
   one port on the VG15 HUB blocks 30 ports by maximum and 15 ports by
   minimum, instead of blocking all 45 ports. Also, the breakdown of the
   HUB, to the old bridge was connected, would have caused the whole
   HP-segment to break down. With the new code only the machines
   connected to the broken HUB will get no more data.
     _________________________________________________________________
   
7.2. Software-setup

   For both bridges the setup is exactly the same (with the exception of
   bridge priority which will be discussed later on). The machine was
   setup by the SuSE 6.4 distribution with the original unpatched kernel
   sources installed. At this point only the minimal configuration and no
   additional hardware or network setup.
   
   The basic setup is according the descriptions in the beginning of this
   document. The thing I did in addition was bringing up the unpatched
   2.2.14 sources of the SuSE 6.4 distribution to version 2.2.15 as in
   [90]Example 11.
   
   Example 11. Upgrading The Kernel From 2.2.14 To 2.2.15
root@mbb-1:~ # cd /usr/src/linux-2.2.14
root@mbb-1:/usr/src/linux-2.2.14 # patch -p1 \
    /usr/local/download/kernel/patch-2.2.15
patching file ........................
patching file ...................
...
..
root@mbb-1:/usr/src/linux-2.2.14 # cd ..
root@mbb-1:/usr/src # mv linux-2.2.14 linux-2.2.15
root@mbb-1:/usr/src # rm linux
root@mbb-1:/usr/src # ln -s linux-2.2.15 linux

   Next step was to apply the bridge-patch as shown in [91]Example 12.
   
   Example 12. Applying The Kernel Patch
root@mbb-1:/usr/src # cd /usr/src/linux-2.2.15
root@mbb-1:/usr/src/linux-2.2.15 # patch -p1 < \
    bridge-0.0.5-against-2.2.15.diff
patching file ........................
patching file ...................
...
..

   After that I selected the bridging code to be compiled as a module as
   shown in [92]Example 13.
   
   Example 13. Configuring The Kernel
root@mbb-1:/usr/src/linux-2.2.15 # make config

..

*
* Code maturity level options
*
Prompt for development and/or incomplete code/drivers (CONFIG_EXPERIMENTAL)
[N/y/?] Y

..


802.1d Ethernet Bridging (CONFIG_BRIDGE) [N/y/m/?] (NEW) m

..

   By the way I also selected the drivers of my NIC's to be compiled as
   modules which resulted to 3c95x.o and hp100.o.
   
root@mbb-1:/usr/src/linux-2.2.15 # make dep clean zImage \
  modules modules_install zlilo

..

root@mbb-1:/usr/src/linux-2.2.15 # init 6

   After the reboot happening I started at runlevel 1 leaving all the
   networking out of the running system. That gave me the chance to check
   the setup step by step.
   
   The command modprobe -v bridge worked without any warnings, so that
   one was OK. Next I edited my /etc/modules.conf by aliasing my network
   card drivers as shown in [93]Example 14 and [94]Example 15. I didn't
   need to make use of the options, all cards where realized proper as I
   checked by cat /proc/modules, cat /proc/interrupts and cat
   /proc/ioports.
   
   Example 14. /etc/modules.conf of mbb-1
# Aliases - specify your hardware
alias eth0             3c59x
alias eth1             hp100
alias eth2             hp100
alias eth3             hp100

   Example 15. /etc/modules.conf of mbb-2
# Aliases - specify your hardware
alias eth0             3c509
alias eth1             hp100
alias eth2             hp100
alias eth3             hp100

   So next thing would have been a step by step setup of the bridge and
   it's interfaces. Because I'm lazy I just show the init script I
   prepared for the setup.
   
   Important
   
   Of course you'll have do adapt the script to your system, if you want
   to use it. Please remember I'm writing this for the setup of a SuSE
   distribution.
   
   Example 16. Bridge Init Script
   
#! /bin/bash
# Copyright (c) 2000 Uwe Bhme.  All rights reserved.
#
# Author: Uwe Bhme <uwe@bnhof.de>, 2000
#
#
# /sbin/init.d/bridge
#

. /etc/rc.config

return=$rc_done
case "$1" in
    start)
        echo "Starting service bridge mueb"
        brctl addbr mueb  ||  return=$rc_failed           (1)
        brctl setbridgeprio mueb 0 || return=$rc_failed   (2)
        brctl addif mueb eth0  ||  return=$rc_failed      (3)
        brctl addif mueb eth1  ||  return=$rc_failed      (4)
        brctl addif mueb eth2  ||  return=$rc_failed      (5)
        brctl addif mueb eth3  ||  return=$rc_failed      (6)
        ifconfig eth0 0.0.0.0  ||  return=$rc_failed      (7)
        ifconfig eth1 0.0.0.0  ||  return=$rc_failed      (8)
        ifconfig eth2 0.0.0.0  ||  return=$rc_failed      (9)
        ifconfig eth3 0.0.0.0  ||  return=$rc_failed      (10)
        brctl sethello mueb 1  ||  return=$rc_failed      (11)
        brctl setmaxage mueb 4  ||  return=$rc_failed     (12)
        brctl setfd mueb 4  ||  return=$rc_failed         (13)
        
        echo -e "$return"
        ;;
    stop)
        echo "Shutting down service bridge mueb"
        brctl delif mueb eth3  ||  return=$rc_failed      (14)
        brctl delif mueb eth2  ||  return=$rc_failed      (15)
        brctl delif mueb eth1  ||  return=$rc_failed      (16)
        brctl delif mueb eth0  ||  return=$rc_failed      (17)
        brctl delbr mueb  ||  return=$rc_failed           (18)
        rmmod bridge || return=$rc_failed                 (19)

        echo -e "$return"
        ;;
    status)
        ifconfig mueb
        brctl showbr mueb

        ;;
    restart)
        $0 stop && $0 start || return=$rc_failed
        ;;
    *)
        echo "Usage: $0 {start|stop|status|restart}"
        exit 1
esac

test "$return" = "$rc_done" || exit 1
exit 0

   [95](1) 
          This command creates a new virtual interface (bridge instance)
          with the name mueb and also brings up the bridge module.
          
   Note
   
   At least my system it does. Maybe you have to enable the kernel module
   loader.
   [96](2) 
          Here the script sets the bridge's priority (relative to other
          bridges in the net) to 0. This is indicating that this bridge
          will become the root bridge as long as there is no other bridge
          with a lower priority level available.
          
   Important
   
   In the init script of the backup bridge this line in missing, leaving
   it with the default priority of 100.
   [97](3) [98](4) [99](5) [100](6) 
          Enslaves the Ethernet interface to become a port in the bridge.
   [101](7) [102](8) [103](9) [104](10) 
          Takes away any possibly disturbing IP-address and brings the
          interface up.
   [105](11) 
          Setting the hello time of the bridge to one second makes it
          possible to reduce the maxage value of the bridges inside the
          network.
   [106](12) 
          Setting the time the a bridge is waiting before starting the
          takeover process to a shorter period.
   [107](13) 
          Forcing the bridge to forward earlier than the default time.
   [108](14) [109](15) [110](16) [111](17) 
          Take the Ethernet out of the bridging instance.
   [112](18) 
          Destroy the bridge instance.
   [113](19) 
          Remove the bridge module.
          
   To polish your setup and to be able to reach the bridge from remote
   you now can configure your bridge instance as if it would be a
   physical existing network interface. You can give it a nice IP with a
   suitable net-mask. It doesn't matter from which segment in you net,
   you will reach the bridge with this IP-address.
     _________________________________________________________________
   
7.3. See It Work

   Here I want to show and explain about how the running bridge shows up.
   The output [114]Example 17 of bridge@mbb-1 is the output of the
   primary bridge, while you see in [115]Example 18 the output of the
   backup bridge waiting to take over.
   
   Example 17. Status Output Of mbb-1 Fully Up
mueb
 bridge id              0000.0800062815f6
 designated root        0000.0800062815f6
 root port                 0                    path cost                  0
 max age                   4.00                 bridge max age             4.00
 hello time                1.00                 bridge hello time          1.00
 forward delay             4.00                 bridge forward delay       4.00
 ageing time             300.00                 gc interval                4.00
 hello timer               0.80                 tcn timer                  0.00
 topology change timer     0.00                 gc timer                   3.80
 flags


eth0 (1)
 port id                8001                    state                   forward
ing
 designated root        0000.0800062815f6       path cost                100
 designated bridge      0000.0800062815f6       message age timer          0.00
 designated port        8001                    forward delay timer        0.00
 designated cost           0                    hold timer                 0.80
 flags

eth1 (2)
 port id                8002                    state                   forward
ing
 designated root        0000.0800062815f6       path cost                100
 designated bridge      0000.0800062815f6       message age timer          0.00
 designated port        8002                    forward delay timer        0.00
 designated cost           0                    hold timer                 0.80
 flags

eth2 (3)
 port id                8003                    state                   forward
ing
 designated root        0000.0800062815f6       path cost                100
 designated bridge      0000.0800062815f6       message age timer          0.00
 designated port        8003                    forward delay timer        0.00
 designated cost           0                    hold timer                 0.80
 flags

eth3 (4)
 port id                8004                    state                   forward
ing
 designated root        0000.0800062815f6       path cost                100
 designated bridge      0000.0800062815f6       message age timer          0.00
 designated port        8004                    forward delay timer        0.00
 designated cost           0                    hold timer                 0.80
 flags

   Example 18. Status Output Of mbb-2 Fully Up
mueb
 bridge id              0064.00a024d04cd6
 designated root        0000.0800062815f6
 root port                 1                    path cost                100
 max age                   4.00                 bridge max age             4.00
 hello time                1.00                 bridge hello time          1.00
 forward delay             4.00                 bridge forward delay       4.00
 ageing time             300.00                 gc interval                4.00
 hello timer               0.00                 tcn timer                  0.00
 topology change timer     0.00                 gc timer                   2.39
 flags


eth0 (1)
 port id                8001                    state                   forward
ing
 designated root        0000.0800062815f6       path cost                100
 designated bridge      0000.0800062815f6       message age timer          0.42
 designated port        8001                    forward delay timer        0.00
 designated cost           0                    hold timer                 0.00
 flags

eth1 (2)
 port id                8002                    state                   blockin
g
 designated root        0000.0800062815f6       path cost                100
 designated bridge      0000.0800062815f6       message age timer          0.42
 designated port        8002                    forward delay timer        0.00
 designated cost           0                    hold timer                 0.00
 flags

eth2 (3)
 port id                8003                    state                   blockin
g
 designated root        0000.0800062815f6       path cost                100
 designated bridge      0000.0800062815f6       message age timer          0.42
 designated port        8003                    forward delay timer        0.00
 designated cost           0                    hold timer                 0.00
 flags

eth3 (4)
 port id                8004                    state                   blockin
g
 designated root        0000.0800062815f6       path cost                100
 designated bridge      0000.0800062815f6       message age timer          0.42
 designated port        8004                    forward delay timer        0.00
 designated cost           0                    hold timer                 0.00
 flags

   If you take a glance into /var/log/messages as shown in [116]Example
   19 and in [117]Example 20 you can see how the bridges are coming up
   and deciding how to do their duty. mbb-1 has a lower value for
   bridge-priority (see [118](9) 
   ), telling it to try to become the root bridge. As you can see mbb-1
   forwards all ports, while mbb-2 blocks all ports with the exception of
   eth0.
   
   Example 19. mbb-1 Messages From init 2
May 25 16:46:04 mbb-1 init: Switching to runlevel: 2
May 25 16:46:04 mbb-1 kernel: NET4: Ethernet Bridge 008 for NET4.0
May 25 16:46:04 mbb-1 kernel: device eth0 entered promiscuous mode
May 25 16:46:04 mbb-1 kernel: device eth1 entered promiscuous mode
May 25 16:46:04 mbb-1 kernel: device eth2 entered promiscuous mode
May 25 16:46:04 mbb-1 kernel: device eth3 entered promiscuous mode
May 25 16:46:04 mbb-1 kernel: mueb: port 4(eth3) entering listening state
May 25 16:46:04 mbb-1 kernel: mueb: port 3(eth2) entering listening state
May 25 16:46:04 mbb-1 kernel: mueb: port 2(eth1) entering listening state
May 25 16:46:04 mbb-1 kernel: mueb: port 1(eth0) entering listening state
May 25 16:46:08 mbb-1 kernel: mueb: port 4(eth3) entering learning state
May 25 16:46:08 mbb-1 kernel: mueb: port 3(eth2) entering learning state
May 25 16:46:08 mbb-1 kernel: mueb: port 2(eth1) entering learning state
May 25 16:46:08 mbb-1 kernel: mueb: port 1(eth0) entering learning state
May 25 16:46:12 mbb-1 kernel: mueb: port 4(eth3) entering forwarding state
May 25 16:46:12 mbb-1 kernel: mueb: topology change detected, propagating
May 25 16:46:12 mbb-1 kernel: mueb: port 3(eth2) entering forwarding state
May 25 16:46:12 mbb-1 kernel: mueb: topology change detected, propagating
May 25 16:46:12 mbb-1 kernel: mueb: port 2(eth1) entering forwarding state
May 25 16:46:12 mbb-1 kernel: mueb: topology change detected, propagating
May 25 16:46:12 mbb-1 kernel: mueb: port 1(eth0) entering forwarding state
May 25 16:46:12 mbb-1 kernel: mueb: topology change detected, propagating

   Example 20. mbb-2 Messages From init 2
Jun  8 06:06:16 mbb-2 init: Switching to runlevel: 2
Jun  8 06:06:17 mbb-2 kernel: NET4: Ethernet Bridge 008 for NET4.0
Jun  8 06:06:17 mbb-2 kernel: device eth0 entered promiscuous mode
Jun  8 06:06:17 mbb-2 kernel: device eth1 entered promiscuous mode
Jun  8 06:06:17 mbb-2 kernel: device eth2 entered promiscuous mode
Jun  8 06:06:17 mbb-2 kernel: device eth3 entered promiscuous mode
Jun  8 06:06:17 mbb-2 kernel: mueb: port 4(eth3) entering listening state
Jun  8 06:06:17 mbb-2 kernel: mueb: port 3(eth2) entering listening state
Jun  8 06:06:17 mbb-2 kernel: mueb: port 2(eth1) entering listening state
Jun  8 06:06:17 mbb-2 kernel: mueb: port 1(eth0) entering listening state
Jun  8 06:06:17 mbb-2 kernel: mueb: port 2(eth1) entering blocking state
Jun  8 06:06:17 mbb-2 kernel: mueb: port 3(eth2) entering blocking state
Jun  8 06:06:17 mbb-2 kernel: mueb: port 4(eth3) entering blocking state
Jun  8 06:06:21 mbb-2 kernel: mueb: port 1(eth0) entering learning state
Jun  8 06:06:25 mbb-2 kernel: mueb: port 1(eth0) entering forwarding state
     _________________________________________________________________
   
7.4. Bridge Tests

   To check if really all the promised features are working, I did some
   crude test. The message logs are shown here in.
     _________________________________________________________________
   
7.4.1. Tear The Patch Wire Test

   I think just taking a patch wire out of a bridge port is a really good
   real survival test. So I pulled the plugs one by one out of the
   sockets and looked what happened. To give you not too much tension let
   me summarize first: It's really working. All the takeovers happened
   within less then 12 seconds.
   
   The really interesting messages you can find at mbb-2. To see how
   everything comes up, I stopped network services first. In [119]Example
   21 you will see the messages caused by a init 2 followed by a "take
   out the plug, wait what happens, then place it back" in the order
   eth3, eth2, eth1, eth0 .
   
   Note
   
   The thing I did, was making the tests, and publishing the dump. The
   one writing the nice explanations was Lennert again.
   
   Example 21. mbb-2 Message Output Of Bridge Test
Jun  8 06:06:16 mbb-2 init: Switching to runlevel: 2
Jun  8 06:06:17 mbb-2 kernel: NET4: Ethernet Bridge 008 for NET4.0
Jun  8 06:06:17 mbb-2 kernel: device eth0 entered promiscuous mode
Jun  8 06:06:17 mbb-2 kernel: device eth1 entered promiscuous mode
Jun  8 06:06:17 mbb-2 kernel: device eth2 entered promiscuous mode
Jun  8 06:06:17 mbb-2 kernel: device eth3 entered promiscuous mode
Jun  8 06:06:17 mbb-2 kernel: mueb: port 4(eth3) entering listening state
Jun  8 06:06:17 mbb-2 kernel: mueb: port 3(eth2) entering listening state
Jun  8 06:06:17 mbb-2 kernel: mueb: port 2(eth1) entering listening state
Jun  8 06:06:17 mbb-2 kernel: mueb: port 1(eth0) entering listening state
            (1)
Jun  8 06:06:17 mbb-2 kernel: mueb: port 2(eth1) entering blocking state
Jun  8 06:06:17 mbb-2 kernel: mueb: port 3(eth2) entering blocking state
Jun  8 06:06:17 mbb-2 kernel: mueb: port 4(eth3) entering blocking state
Jun  8 06:06:21 mbb-2 kernel: mueb: port 1(eth0) entering learning state
Jun  8 06:06:25 mbb-2 kernel: mueb: port 1(eth0) entering forwarding state
            (2)
Jun  8 06:07:15 mbb-2 kernel: mueb: neighbour 0000.08:00:06:28:15:f6 lost on po
rt 4(eth3)  (3)
Jun  8 06:07:15 mbb-2 kernel: mueb: port 4(eth3) entering listening state
            (4)
Jun  8 06:07:19 mbb-2 kernel: mueb: port 4(eth3) entering learning state
            (5)
Jun  8 06:07:23 mbb-2 kernel: mueb: port 4(eth3) entering forwarding state
            (6)
Jun  8 06:07:23 mbb-2 kernel: mueb: topology change detected, sending tcn bpdu
            (7)
Jun  8 06:08:51 mbb-2 kernel: mueb: topology change detected, sending tcn bpdu
            (8)
Jun  8 06:08:51 mbb-2 kernel: mueb: port 4(eth3) entering blocking state
            (9)
Jun  8 06:09:22 mbb-2 kernel: mueb: neighbour 0000.08:00:06:28:15:f6 lost on po
rt 3(eth2)  (10)
Jun  8 06:09:22 mbb-2 kernel: mueb: port 3(eth2) entering listening state
Jun  8 06:09:26 mbb-2 kernel: mueb: port 3(eth2) entering learning state
Jun  8 06:09:30 mbb-2 kernel: mueb: port 3(eth2) entering forwarding state
Jun  8 06:09:30 mbb-2 kernel: mueb: topology change detected, sending tcn bpdu
Jun  8 06:10:09 mbb-2 kernel: mueb: topology change detected, sending tcn bpdu
Jun  8 06:10:09 mbb-2 kernel: mueb: port 3(eth2) entering blocking state
Jun  8 06:10:10 mbb-2 kernel: mueb: retransmitting tcn bpdu
            (11)
Jun  8 06:10:41 mbb-2 kernel: mueb: neighbour 0000.08:00:06:28:15:f6 lost on po
rt 2(eth1)  (12)
Jun  8 06:10:41 mbb-2 kernel: mueb: port 2(eth1) entering listening state
Jun  8 06:10:45 mbb-2 kernel: mueb: port 2(eth1) entering learning state
Jun  8 06:10:49 mbb-2 kernel: mueb: port 2(eth1) entering forwarding state
Jun  8 06:10:49 mbb-2 kernel: mueb: topology change detected, sending tcn bpdu
Jun  8 06:11:06 mbb-2 kernel: mueb: topology change detected, sending tcn bpdu
Jun  8 06:11:06 mbb-2 kernel: mueb: port 2(eth1) entering blocking state
Jun  8 06:11:33 mbb-2 kernel: mueb: neighbour 0000.08:00:06:28:15:f6 lost on po
rt 1(eth0)  (13)
Jun  8 06:11:33 mbb-2 kernel: mueb: port 2(eth1) entering listening state
Jun  8 06:11:37 mbb-2 kernel: mueb: port 2(eth1) entering learning state
Jun  8 06:11:41 mbb-2 kernel: mueb: port 2(eth1) entering forwarding state
Jun  8 06:11:41 mbb-2 kernel: mueb: topology change detected, sending tcn bpdu
Jun  8 06:14:18 mbb-2 kernel: mueb: topology change detected, sending tcn bpdu
Jun  8 06:14:18 mbb-2 kernel: mueb: port 2(eth1) entering blocking state
Jun  8 06:14:19 mbb-2 kernel: mueb: retransmitting tcn bpdu

   4What Lennert Told About This
   [120](1) 
          The kernel sees that there are already bridges (actually, only
          one of them, but Hello packets are coming in on all 4 of the
          ports) on eth[0123].
   [121](2) 
          To maintain connectivity with the rest of the network, the
          bridge decides to keep port 1 (eth0) active (i.e. in the
          "forwarding" state), and to temporarily disable ports 2-4.
   [122](3) 
          The plug on eth3 was pulled. Here you can see that the message
          age timer expired ([123] (13) ). The last Hello packet was seen
          more than X seconds ago. The bridge concludes that the
          connection to the bridge that was there has died. Therefore, it
          is going to try to enable this port, to provide network
          connectivity to the now-cutoff segment.
   [124](4) 
          It enters the listening state. It waits to see whether the old
          bridge might come back, or whether another bridge is going to
          claim takeover.
   [125](5) 
          Okay, no other bridge was seen. We're going to try to provide
          network connectivity to this segment ourselves. Which means:
          we're going to try and become "designated bridge" for this
          segment. We now enter the learning state. In this state, we
          only learn MAC addresses and we do not forward yet. This is
          because if we see an unknown destination address, we send the
          datagram to all ports, and this "flooding" will happen
          unnecessarily often if we have a empty MAC table. Therefore,
          we're going to fill up our MAC table with useful entries first,
          and this is what happens during the learning state.
   [126](6) 
          Okay, here we go. Pray for us.
   [127](7) 
          Because we took over for this segment, all communication
          towards this segment now goes through this bridge. This means
          that the topology has changed. If the topology changes, we must
          let all bridges now, so that they can time out stale MAC
          address location data quickly. This is why we send Topology
          Change Notification Bridge Protocol Data Units (tcn bpdus).
          Apparently the root bridge immediately acknowledges this tcn
          bpdu in the next Hello message it sends (the protocol requires
          for the root bridge to acknowledge it), because this is the
          only such message we see.
          
   Note
   
   In situations where you see loads of these messages, it means that the
   root bridge cannot acknowledge them, which probably means your root
   bridge has a twisted STP implementation.
   [128](8) 
          Hey, something happened again!
   [129](9) 
          Yup... eth3 came back online. The root bridge will provide
          connectivity for this segment again, so that we can disable
          this port.
   [130](10) [131](12) [132](13) 
          Same story for eth2, eth1 and eth0.
   [133](11) 
          This means the tcn bpdu wasn't acknowledged quick enough. That
          is why it is retransmitted.
          
   The root bridge mbb-1 was not so chatty. It only reported some
   topology changes and propagated them as you can see in [134]Example
   22. If somebody can offer a explanation why the root bridge is so
   quiet in messaging please [135]tell me.
   
   Example 22. mbb-2 Message Output Of Bridge Test
Jun  8 06:06:52 mbb-1 kernel: mueb: received tcn bpdu on port 1(eth0)
Jun  8 06:06:52 mbb-1 kernel: mueb: topology change detected, propagating
Jun  8 06:07:31 mbb-1 kernel: mueb: received tcn bpdu on port 1(eth0)
Jun  8 06:07:31 mbb-1 kernel: mueb: topology change detected, propagating
Jun  8 06:07:32 mbb-1 kernel: mueb: received tcn bpdu on port 1(eth0)
Jun  8 06:07:32 mbb-1 kernel: mueb: topology change detected, propagating
Jun  8 06:08:11 mbb-1 kernel: mueb: received tcn bpdu on port 1(eth0)
Jun  8 06:08:11 mbb-1 kernel: mueb: topology change detected, propagating
Jun  8 06:08:29 mbb-1 kernel: mueb: received tcn bpdu on port 1(eth0)
Jun  8 06:08:29 mbb-1 kernel: mueb: topology change detected, propagating
Jun  8 06:09:03 mbb-1 kernel: mueb: received tcn bpdu on port 2(eth1)
Jun  8 06:09:03 mbb-1 kernel: mueb: topology change detected, propagating
Jun  8 06:11:40 mbb-1 kernel: mueb: received tcn bpdu on port 1(eth0)
Jun  8 06:11:40 mbb-1 kernel: mueb: topology change detected, propagating
Jun  8 06:11:41 mbb-1 kernel: mueb: received tcn bpdu on port 1(eth0)
Jun  8 06:11:41 mbb-1 kernel: mueb: topology change detected, propagating

   One of the other bridges tells us that the topology of the LAN has
   changed (see [136]Example 21). Well, okay. We will set lower timeouts
   on our MACC table for a short period of time, and we will propagate
   this topology change throughout the network.
     _________________________________________________________________
   
7.4.2. Kill The Root Bridge Test

   The ultimate test is of course a total blocking, breakdown or
   something similar to the root bridge. I did this by shooting down the
   root bridge by init 1. Next I brought it up again with init 2. Last I
   pulled all plugs out of the root bridge and waited for some time
   before I placed them again. In [137]Example 23 you will see the
   messages from the master-bridge mbb-1, and in [138]Example 24 you see
   what happened the same time at the backup-bridge mbb-2.
   
   Example 23. Test Messages Of Master Bridge mbb-1
Jun 12 13:35:15 mbb-1 init: Switching to runlevel: 1
Jun 12 13:35:20 mbb-1 kernel: mueb: port 4(eth3) entering disabled state
Jun 12 13:35:20 mbb-1 kernel: mueb: port 3(eth2) entering disabled state
Jun 12 13:35:20 mbb-1 kernel: mueb: port 2(eth1) entering disabled state
Jun 12 13:35:20 mbb-1 kernel: mueb: port 1(eth0) entering disabled state
Jun 12 13:35:20 mbb-1 kernel: mueb: port 2(eth1) entering disabled state
Jun 12 13:35:20 mbb-1 kernel: device eth1 left promiscuous mode
Jun 12 13:35:20 mbb-1 kernel: mueb: port 1(eth0) entering disabled state
Jun 12 13:35:20 mbb-1 kernel: device eth0 left promiscuous mode
Jun 12 13:35:20 mbb-1 kernel: mueb: port 4(eth3) entering disabled state
Jun 12 13:35:20 mbb-1 kernel: device eth3 left promiscuous mode
Jun 12 13:35:20 mbb-1 kernel: mueb: port 3(eth2) entering disabled state
Jun 12 13:35:20 mbb-1 kernel: device eth2 left promiscuous mode
Jun 12 13:35:50 mbb-1 init: Switching to runlevel: 2
Jun 12 13:35:50 mbb-1 kernel: NET4: Ethernet Bridge 008 for NET4.0
Jun 12 13:35:51 mbb-1 kernel: device eth0 entered promiscuous mode
Jun 12 13:35:51 mbb-1 kernel: device eth1 entered promiscuous mode
Jun 12 13:35:51 mbb-1 kernel: device eth2 entered promiscuous mode
Jun 12 13:35:51 mbb-1 kernel: device eth3 entered promiscuous mode
Jun 12 13:35:51 mbb-1 kernel: mueb: port 4(eth3) entering listening state
Jun 12 13:35:51 mbb-1 kernel: mueb: port 3(eth2) entering listening state
Jun 12 13:35:51 mbb-1 kernel: mueb: port 2(eth1) entering listening state
Jun 12 13:35:51 mbb-1 kernel: mueb: port 1(eth0) entering listening state
Jun 12 13:35:51 mbb-1 kernel: mueb: received tcn bpdu on port 2(eth1)
Jun 12 13:35:51 mbb-1 kernel: mueb: topology change detected, propagating
Jun 12 13:35:52 mbb-1 kernel: mueb: received tcn bpdu on port 1(eth0)
Jun 12 13:35:52 mbb-1 kernel: mueb: topology change detected, propagating
Jun 12 13:35:55 mbb-1 kernel: mueb: port 4(eth3) entering learning state
Jun 12 13:35:55 mbb-1 kernel: mueb: port 3(eth2) entering learning state
Jun 12 13:35:55 mbb-1 kernel: mueb: port 2(eth1) entering learning state
Jun 12 13:35:55 mbb-1 kernel: mueb: port 1(eth0) entering learning state
Jun 12 13:35:59 mbb-1 kernel: mueb: port 4(eth3) entering forwarding state
Jun 12 13:35:59 mbb-1 kernel: mueb: topology change detected, propagating
Jun 12 13:35:59 mbb-1 kernel: mueb: port 3(eth2) entering forwarding state
Jun 12 13:35:59 mbb-1 kernel: mueb: topology change detected, propagating
Jun 12 13:35:59 mbb-1 kernel: mueb: port 2(eth1) entering forwarding state
Jun 12 13:35:59 mbb-1 kernel: mueb: topology change detected, propagating
Jun 12 13:35:59 mbb-1 kernel: mueb: port 1(eth0) entering forwarding state
Jun 12 13:35:59 mbb-1 kernel: mueb: topology change detected, propagating
Jun 12 13:39:03 mbb-1 kernel: mueb: received tcn bpdu on port 1(eth0)
Jun 12 13:39:03 mbb-1 kernel: mueb: topology change detected, propagating
Jun 12 13:39:05 mbb-1 kernel: mueb: received tcn bpdu on port 1(eth0)
Jun 12 13:39:05 mbb-1 kernel: mueb: topology change detected, propagating

   Example 24. Test Messages Of Backup Bridge mbb-2
Jun 12 13:35:21 mbb-2 kernel: mueb: neighbour 0000.08:00:06:28:15:f6 lost on po
rt 4(eth3)
Jun 12 13:35:21 mbb-2 kernel: mueb: port 4(eth3) entering listening state
Jun 12 13:35:21 mbb-2 kernel: mueb: neighbour 0000.08:00:06:28:15:f6 lost on po
rt 3(eth2)
Jun 12 13:35:21 mbb-2 kernel: mueb: port 3(eth2) entering listening state
Jun 12 13:35:21 mbb-2 kernel: mueb: neighbour 0000.08:00:06:28:15:f6 lost on po
rt 2(eth1)
Jun 12 13:35:21 mbb-2 kernel: mueb: port 2(eth1) entering listening state
Jun 12 13:35:21 mbb-2 kernel: mueb: neighbour 0000.08:00:06:28:15:f6 lost on po
rt 1(eth0)
Jun 12 13:35:21 mbb-2 kernel: mueb: topology change detected, propagating
Jun 12 13:35:25 mbb-2 kernel: mueb: port 4(eth3) entering learning state
Jun 12 13:35:25 mbb-2 kernel: mueb: port 3(eth2) entering learning state
Jun 12 13:35:25 mbb-2 kernel: mueb: port 2(eth1) entering learning state
Jun 12 13:35:29 mbb-2 kernel: mueb: port 4(eth3) entering forwarding state
Jun 12 13:35:29 mbb-2 kernel: mueb: topology change detected, propagating
Jun 12 13:35:29 mbb-2 kernel: mueb: port 3(eth2) entering forwarding state
Jun 12 13:35:29 mbb-2 kernel: mueb: topology change detected, propagating
Jun 12 13:35:29 mbb-2 kernel: mueb: port 2(eth1) entering forwarding state
Jun 12 13:35:29 mbb-2 kernel: mueb: topology change detected, propagating
Jun 12 13:35:49 mbb-2 kernel: mueb: topology change detected, sending tcn bpdu
Jun 12 13:35:49 mbb-2 kernel: mueb: port 3(eth2) entering blocking state
Jun 12 13:35:49 mbb-2 kernel: mueb: topology change detected, \
                              <6>mueb: port 4(eth3) entering blocking state
Jun 12 13:35:49 mbb-2 kernel: mueb: topology change detected, \
                              <6>mueb: port 2(eth1) entering blocking state
Jun 12 13:35:50 mbb-2 kernel: mueb: retransmitting tcn bpdu
Jun 12 13:38:26 mbb-2 kernel: mueb: neighbour 0000.08:00:06:28:15:f6 lost on po
rt 2(eth1)
Jun 12 13:38:26 mbb-2 kernel: mueb: port 2(eth1) entering listening state
Jun 12 13:38:27 mbb-2 kernel: mueb: neighbour 0000.08:00:06:28:15:f6 lost on po
rt 3(eth2)
Jun 12 13:38:27 mbb-2 kernel: mueb: port 3(eth2) entering listening state
Jun 12 13:38:28 mbb-2 kernel: mueb: neighbour 0000.08:00:06:28:15:f6 lost on po
rt 4(eth3)
Jun 12 13:38:28 mbb-2 kernel: mueb: port 4(eth3) entering listening state
Jun 12 13:38:30 mbb-2 kernel: mueb: port 2(eth1) entering learning state
Jun 12 13:38:30 mbb-2 kernel: mueb: neighbour 0000.08:00:06:28:15:f6 lost on po
rt 1(eth0)
Jun 12 13:38:30 mbb-2 kernel: mueb: topology change detected, propagating
Jun 12 13:38:31 mbb-2 kernel: mueb: port 3(eth2) entering learning state
Jun 12 13:38:32 mbb-2 kernel: mueb: port 4(eth3) entering learning state
Jun 12 13:38:34 mbb-2 kernel: mueb: port 2(eth1) entering forwarding state
Jun 12 13:38:34 mbb-2 kernel: mueb: topology change detected, propagating
Jun 12 13:38:35 mbb-2 kernel: mueb: port 3(eth2) entering forwarding state
Jun 12 13:38:35 mbb-2 kernel: mueb: topology change detected, propagating
Jun 12 13:38:36 mbb-2 kernel: mueb: port 4(eth3) entering forwarding state
Jun 12 13:38:36 mbb-2 kernel: mueb: topology change detected, propagating
Jun 12 13:39:01 mbb-2 kernel: mueb: topology change detected, sending tcn bpdu
Jun 12 13:39:01 mbb-2 kernel: mueb: port 3(eth2) entering blocking state
Jun 12 13:39:01 mbb-2 kernel: mueb: topology change detected, \
                              <6>mueb: port 4(eth3) entering blocking state
Jun 12 13:39:02 mbb-2 kernel: mueb: topology change detected, sending tcn bpdu
Jun 12 13:39:02 mbb-2 kernel: mueb: port 2(eth1) entering blocking state
     _________________________________________________________________
   
Appendix A. Network Interface Cards

   In this section you will find a (for now) very incomplete list of
   NIC's which are known to work or known to cause problem. For I neither
   have the money to buy a lot of different NIC's, nor I have any
   connections to hardware vendors, I depend on your feedback to keep the
   list accurate. So feel free to mail about success or failure to
   [139]Uwe Bhme.
   
   Table Appendix A-1. NIC Information
   Name Value Comment
   3c509b Etherlink III ++
   3c905b +++ Never heard about any problem
   3c905c ++ Never heard about any problem
   HP J2585A - - System hang-up after ifconfig
   HP J2585B ++
   
   Table Appendix A-2. Valuing Of NIC Information
   Value Meaning
   - - - Cards I tried and are also reported not to work by other people
   - - Cards I tried or are reported not to work by other people
   - Cards reported not to work by other people
   * Cards without any report or experience
   + Cards reported to work by other people
   ++ Cards I tried or are reported to work by other people
   +++ Cards I tried and are also reported to work by other people
     _________________________________________________________________
   
Appendix B. Recommended Reading

   Here you will some recommendations which documents you should read
   before you start to setup a bridge.
   
   [140]The bridge home-page
          Will give you recent information about the bridging code and
          the bridge utilities.
          
   [141]NET3-4-HOWTO
          Describes how to install and configure the Linux networking
          software and associated tools.
          
   [142]Ethernet-HOWTO
          Information about which Ethernet devices can be used for Linux,
          and how to set them up (focused on the hardware and low level
          driver aspect of the Ethernet cards).
     _________________________________________________________________
   
Appendix C. FAQ

   Here you will find some of the frequently asked questions connected to
   bridging.
   
FAQ

   1. [143]Hardware
          
        [144]What hardware do I need to run a bridge with 2-n NICs. 
        [145]Can you please recommend some tools to measure a 2-port
                linux bridge throughput. 
                
   2. [146]Software
          
        [147]I'm running with kernel x.x.x. Is a patch out there, to give
                me chance to use this stuff? 
                
1. Hardware

   What hardware do I need to run a bridge with 2-n NICs.
   
   I think a fat 486 or a modest Pentium should be able to keep up with
   2x100Mbit pretty well, but I have never tested this. I don't think RAM
   will matter much (8 or 16MB and all should be fine). CPU will not
   matter a whole lot either (486/Pentium and all should be fine). I
   think the primary contributor is the type of bus (ISA, PCI) and the
   type of network cards (some network cards require less "work" than
   others). Big switches usually have immensely fat internal buses (3 or
   4 gigabits is not uncommon). Standard PCI, for example, can't keep up
   with a gigabit ethernet cards.
   
   Can you please recommend some tools to measure a 2-port linux bridge
   throughput.
   
   Well, first question is: does it have 100mbit interfaces? If it hasn't
   (10mbit only), it shouldn't have problems with keeping up, almost
   regardless of the processor speed. If it does have 100mbit interfaces
   and you're not sure it will keep up, you can run a flood ping with big
   packets across it (ping -f -s 1450 ipaddress) and see whether it keeps
   up.
   
2. Software

   I'm running with kernel x.x.x. Is a patch out there, to give me chance
   to use this stuff?
   
   There are patches for and 2.2.14, 2.2.15. Since 2.3.47 it's in the
   mainstream kernel, so you don't need to patch. If you're talking about
   others, you will have to upgrade, if you need to bridge.

References

   Visible links
   1. bridge-stp-howto/BRIDGE-STP-HOWTO.html#LICENSE
   2. bridge-stp-howto/BRIDGE-STP-HOWTO.html#WHAT-IS-A-BRIDGE
   3. bridge-stp-howto/BRIDGE-STP-HOWTO.html#RULES-ON-BRIDGING
   4. bridge-stp-howto/BRIDGE-STP-HOWTO.html#PREPARING-THE-BRIDGE
   5. bridge-stp-howto/BRIDGE-STP-HOWTO.html#GET-THE-FILES
   6. bridge-stp-howto/BRIDGE-STP-HOWTO.html#APPLY-THE-PATCHES
   7. bridge-stp-howto/BRIDGE-STP-HOWTO.html#CONFIGURE-THE-KERNEL
   8. bridge-stp-howto/BRIDGE-STP-HOWTO.html#COMPILE-THE-KERNEL
   9. bridge-stp-howto/BRIDGE-STP-HOWTO.html#COMPILE-THE-UTILS
  10. bridge-stp-howto/BRIDGE-STP-HOWTO.html#SET-UP-THE-BRIDGE
  11. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SYNOPSIS
  12. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BASIC-SETUP
  13. bridge-stp-howto/BRIDGE-STP-HOWTO.html#ADVANCED-BRIDGE
  14. bridge-stp-howto/BRIDGE-STP-HOWTO.html#STP
  15. bridge-stp-howto/BRIDGE-STP-HOWTO.html#IPCHAINS
  16. bridge-stp-howto/BRIDGE-STP-HOWTO.html#PRACTICAL-EXAMPLE
  17. bridge-stp-howto/BRIDGE-STP-HOWTO.html#AEN510
  18. bridge-stp-howto/BRIDGE-STP-HOWTO.html#AEN529
  19. bridge-stp-howto/BRIDGE-STP-HOWTO.html#SEE-IT-WORK
  20. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRIDGE-TESTS
  21. bridge-stp-howto/BRIDGE-STP-HOWTO.html#NIC-INFO
  22. bridge-stp-howto/BRIDGE-STP-HOWTO.html#RECOMMENDED-READING
  23. bridge-stp-howto/BRIDGE-STP-HOWTO.html#FAX
  24. COPYRIGHT.html
  25. http://sunsite.unc.edu/LDP/LICENSE.html
  26. bridge-stp-howto/BRIDGE-STP-HOWTO.html#RULES-ON-BRIDGING
  27. bridge-stp-howto/BRIDGE-STP-HOWTO.html#STP
  28. bridge-stp-howto/BRIDGE-STP-HOWTO.html#IPCHAINS
  29. bridge-stp-howto/BRIDGE-STP-HOWTO.html#RECOMMENDED-READING
  30. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BASIC-SETUP
  31. http://www.kernel.org/mirrors/
  32. http://www.openrock.net/bridge/
  33. ftp://openrock.net/bridge/patches/bridge-0.0.5-against-2.2.14.diff
  34. ftp://openrock.net/bridge/patches/bridge-0.0.5-against-2.2.15.diff
  35. bridge-stp-howto/BRIDGE-STP-HOWTO.html#SET-UP-THE-BRIDGE
  36. http://www.openrock.net/bridge/
  37. ftp://openrock.net/bridge/bridge-utils/bridge-utils-0.9.1.tar.gz
  38. http://sunsite.unc.edu/LDP/HOWTO/HOWTO-INDEX.html
  39. bridge-stp-howto/BRIDGE-STP-HOWTO.html#KERNEL-COMPILE-COMMANDS
  40. bridge-stp-howto/BRIDGE-STP-HOWTO.html#UTILS-COMPILE-COMMANDS
  41. bridge-stp-howto/BRIDGE-STP-HOWTO.html#UTILS-COPY-BINARIES
  42. bridge-stp-howto/BRIDGE-STP-HOWTO.html#UTILS-COPY-MANPAGE
  43. http://sunsite.unc.edu/LDP/HOWTO/HOWTO-INDEX.html
  44. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-ADDBR
  45. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-DELBR
  46. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-ADDIF
  47. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-DELIF
  48. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SHOW
  49. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SHOW-OUTPUT
  50. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SHOWBR
  51. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SHOWBR
  52. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SHOWMACS
  53. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SETAGEING
  54. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SETBRIDGEPRIO
  55. bridge-stp-howto/BRIDGE-STP-HOWTO.html#STP
  56. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SETFD
  57. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SETGCINT
  58. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SETHELLO
  59. bridge-stp-howto/BRIDGE-STP-HOWTO.html#STP
  60. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SETMAXAGE
  61. bridge-stp-howto/BRIDGE-STP-HOWTO.html#STP
  62. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SETPATHCOST
  63. bridge-stp-howto/BRIDGE-STP-HOWTO.html#STP
  64. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-STP
  65. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SETBRIDGEPRIO
  66. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SETHELLO
  67. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SETPATHCOST
  68. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-STP
  69. bridge-stp-howto/BRIDGE-STP-HOWTO.html#STP
  70. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRIDGE-INIT-SCRIPT
  71. bridge-stp-howto/BRIDGE-STP-HOWTO.html#AEN409
  72. bridge-stp-howto/BRIDGE-STP-HOWTO.html#AEN426
  73. bridge-stp-howto/BRIDGE-STP-HOWTO.html#AEN435
  74. bridge-stp-howto/BRIDGE-STP-HOWTO.html#PRACTICAL-EXAMPLE
  75. http://openrock.net/bridge/
  76. bridge-stp-howto/BRIDGE-STP-HOWTO.html#IPCH-ADDBR
  77. bridge-stp-howto/BRIDGE-STP-HOWTO.html#IPCH-ADDIF-ETH0
  78. bridge-stp-howto/BRIDGE-STP-HOWTO.html#IPCH-ADDIF-ETH1
  79. bridge-stp-howto/BRIDGE-STP-HOWTO.html#IPCH-IFCONFIG
  80. bridge-stp-howto/BRIDGE-STP-HOWTO.html#IPCH-ADDCHAIN
  81. bridge-stp-howto/BRIDGE-STP-HOWTO.html#IPCH-ADDBR
  82. bridge-stp-howto/BRIDGE-STP-HOWTO.html#IPCH-ADDCHAIN
  83. bridge-stp-howto/BRIDGE-STP-HOWTO.html#IPCH-DEN-ETH0
  84. bridge-stp-howto/BRIDGE-STP-HOWTO.html#OLD-BRIDGE-HARDWARE-SETUP
  85. bridge-stp-howto/BRIDGE-STP-HOWTO.html#STP
  86. http://openrock.net/bridge
  87. bridge-stp-howto/BRIDGE-STP-HOWTO.html#STP
  88. bridge-stp-howto/BRIDGE-STP-HOWTO.html#MULTI-BRIDGE-HARDWARE-SETUP
  89. bridge-stp-howto/BRIDGE-STP-HOWTO.html#OLD-BRIDGE-HARDWARE-SETUP
  90. bridge-stp-howto/BRIDGE-STP-HOWTO.html#APPLY-KERNEL-PATCH
  91. bridge-stp-howto/BRIDGE-STP-HOWTO.html#APPLY-BRIDGE-PATCH
  92. bridge-stp-howto/BRIDGE-STP-HOWTO.html#MENUCONFIG-BRIDGE-MODULE-SELECTION
  93. bridge-stp-howto/BRIDGE-STP-HOWTO.html#MODULES-CONF-NIC-SAMPLE-MBB1
  94. bridge-stp-howto/BRIDGE-STP-HOWTO.html#MODULES-CONF-NIC-SAMPLE-MBB2
  95. bridge-stp-howto/BRIDGE-STP-HOWTO.html#CREATE-BRIDGE
  96. bridge-stp-howto/BRIDGE-STP-HOWTO.html#SET-ROOT-BRIDGE
  97. bridge-stp-howto/BRIDGE-STP-HOWTO.html#ADDIF-ETH0
  98. bridge-stp-howto/BRIDGE-STP-HOWTO.html#ADDIF-ETH1
  99. bridge-stp-howto/BRIDGE-STP-HOWTO.html#ADDIF-ETH2
 100. bridge-stp-howto/BRIDGE-STP-HOWTO.html#ADDIF-ETH3
 101. bridge-stp-howto/BRIDGE-STP-HOWTO.html#UP-ETH0
 102. bridge-stp-howto/BRIDGE-STP-HOWTO.html#UP-ETH1
 103. bridge-stp-howto/BRIDGE-STP-HOWTO.html#UP-ETH2
 104. bridge-stp-howto/BRIDGE-STP-HOWTO.html#UP-ETH3
 105. bridge-stp-howto/BRIDGE-STP-HOWTO.html#HELLO-1
 106. bridge-stp-howto/BRIDGE-STP-HOWTO.html#MAXAGE-4
 107. bridge-stp-howto/BRIDGE-STP-HOWTO.html#FORWARDDELAY-4
 108. bridge-stp-howto/BRIDGE-STP-HOWTO.html#DELIF-ETH3
 109. bridge-stp-howto/BRIDGE-STP-HOWTO.html#DELIF-ETH2
 110. bridge-stp-howto/BRIDGE-STP-HOWTO.html#DELIF-ETH1
 111. bridge-stp-howto/BRIDGE-STP-HOWTO.html#DELIF-ETH0
 112. bridge-stp-howto/BRIDGE-STP-HOWTO.html#DESTROY-BRIDGE
 113. bridge-stp-howto/BRIDGE-STP-HOWTO.html#REMOVE-MODULE
 114. bridge-stp-howto/BRIDGE-STP-HOWTO.html#SAMPLE-BRIDGE-STATUS-MBB1
 115. bridge-stp-howto/BRIDGE-STP-HOWTO.html#SAMPLE-BRIDGE-STATUS-MBB2
 116. bridge-stp-howto/BRIDGE-STP-HOWTO.html#MESSAGES-FROM-INIT-2-AT-MBB-1
 117. bridge-stp-howto/BRIDGE-STP-HOWTO.html#MESSAGES-FROM-INIT-2-AT-MBB-2
 118. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SETBRIDGEPRIO
 119. bridge-stp-howto/BRIDGE-STP-HOWTO.html#MBB-2-MESSAGES-OF-BRIDGE-TEST
 120. bridge-stp-howto/BRIDGE-STP-HOWTO.html#SEE-OTHER-BRIDGE
 121. bridge-stp-howto/BRIDGE-STP-HOWTO.html#KEEP-ONE-INTERFACE
 122. bridge-stp-howto/BRIDGE-STP-HOWTO.html#PULL-PLUG-ETH3
 123. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SETMAXAGE
 124. bridge-stp-howto/BRIDGE-STP-HOWTO.html#ENTER-LISTEN-STATE
 125. bridge-stp-howto/BRIDGE-STP-HOWTO.html#ENTER-LEARN-STATE
 126. bridge-stp-howto/BRIDGE-STP-HOWTO.html#ENTER-FORWARD-STATE
 127. bridge-stp-howto/BRIDGE-STP-HOWTO.html#TOPOLOGY-CHANGE-DETECT
 128. bridge-stp-howto/BRIDGE-STP-HOWTO.html#TOPOLOGY-CHANGED-AGAIN
 129. bridge-stp-howto/BRIDGE-STP-HOWTO.html#ROOT-IS-BACK
 130. bridge-stp-howto/BRIDGE-STP-HOWTO.html#FROM-PULL-TO-BACK-ETH2
 131. bridge-stp-howto/BRIDGE-STP-HOWTO.html#FROM-PULL-TO-BACK-ETH1
 132. bridge-stp-howto/BRIDGE-STP-HOWTO.html#FROM-PULL-TO-BACK-ETH0
 133. bridge-stp-howto/BRIDGE-STP-HOWTO.html#RETRANSMIT-TCN-BPDU
 134. bridge-stp-howto/BRIDGE-STP-HOWTO.html#MBB-1-MESSAGES-OF-BRIDGE-TEST
 135. mailto:uwe@bnhof
 136. bridge-stp-howto/BRIDGE-STP-HOWTO.html#MBB-2-MESSAGES-OF-BRIDGE-TEST
 137. bridge-stp-howto/BRIDGE-STP-HOWTO.html#TEST-MESSAGES-OF-MASTER-BRIDGE
 138. bridge-stp-howto/BRIDGE-STP-HOWTO.html#TEST-MESSAGES-OF-BACKUP-BRIDGE
 139. mailto:uwe@bnhof.de
 140. http://openrock.net/bridge/
 141. http://www.linuxdoc.org/HOWTO/NET3-4-HOWTO.html
 142. http://www.linuxdoc.org/HOWTO/Ethernet-HOWTO.html
 143. bridge-stp-howto/BRIDGE-STP-HOWTO.html#AEN842
 144. bridge-stp-howto/BRIDGE-STP-HOWTO.html#AEN845
 145. bridge-stp-howto/BRIDGE-STP-HOWTO.html#AEN852
 146. bridge-stp-howto/BRIDGE-STP-HOWTO.html#AEN858
 147. bridge-stp-howto/BRIDGE-STP-HOWTO.html#AEN861

   Hidden links:
 148. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SHOWBR
 149. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SETBRIDGEPRIO
 150. bridge-stp-howto/BRIDGE-STP-HOWTO.html#BRCTL-SETMAXAGE
  BASH Programming - Introduction HOW-TO
  by Mike G mikkey@dynamo.com.ar
  27 June 2000

  This article intends to help you to start programming     basic-inter
  mediate shell scripts. It does not intend to be an     advanced docu
  ment (see the title). I am NOT an expert nor guru     shell program
  mer. I decided to write this because I'll learn a     lot and it might
  be useful to other people. Any feedback will be apreciated,     spe
  cially in the patch form :)
  ______________________________________________________________________

  Table of Contents



  1. Introduction

     1.1 Getting the latest version
     1.2 Requisites
     1.3 Uses of this document

  2. Very simple Scripts

     2.1 Traditional hello world script
     2.2 A very simple backup script

  3. All about redirection

     3.1 Theory and quick reference
     3.2 Sample: stdout 2 file
     3.3 Sample: stderr 2 file
     3.4 Sample: stdout 2 stderr
     3.5 Sample: stderr 2 stdout
     3.6 Sample: stderr and stdout 2 file

  4. Pipes

     4.1 What they are and why you'll want to use them
     4.2 Sample: simple pipe with sed
     4.3 Sample: an alternative to ls -l *.txt

  5. Variables

     5.1 Sample: Hello World! using variables
     5.2 Sample: A very simple backup script (little bit better)
     5.3 Local variables

  6. Conditionals

     6.1 Dry Theory
     6.2 Sample: Basic conditional example if .. then
     6.3 Sample: Basic conditional example if .. then ... else
     6.4 Sample: Conditionals with variables

  7. Loops for, while and until

     7.1 For sample
     7.2 C-like for
     7.3 While sample
     7.4 Until sample

  8. Functions

     8.1 Functions sample
     8.2 Functions with parameters sample

  9. User interfaces

     9.1 Using select to make simple menus
     9.2 Using the command line

  10. Misc

     10.1 Reading user input with read
     10.2 Arithmetic evaluation
     10.3 Finding bash
     10.4 Getting the return value of a program
     10.5 Capturing a commands output
     10.6 Multiple source files

  11. Tables
     11.1 String comparison operators
     11.2 String comparison examples
     11.3 Arithmetic operators
     11.4 Arithmetic relational operators
     11.5 Useful commands

  12. More Scripts

     12.1 Applying a command to all files in a directory.
     12.2 Sample: A very simple backup script (little bit better)
     12.3 File re-namer
     12.4 File renamer (simple)

  13. When something goes wrong (debugging)

     13.1 Ways Calling BASH

  14. About the document

     14.1 (no) warranty
     14.2 Thanks to
     14.3 History
     14.4 More resources


  ______________________________________________________________________

  1.  Introduction

  1.1.  Getting the latest version

  http://www.linuxdoc.org/HOWTO/Bash-Prog-Intro-HOWTO.html



  1.2.

  Requisites

  Familiarity with GNU/Linux command lines, and familiarity with basic
  programming concepts is helpful. While this is not a programming
  introduction, it explains (or at least tries) many basic concepts.



  1.3.

  Uses of this document

  This document tries to be useful in the following situations

    You have an idea about programming and you want to start coding
     some shell scripts.

    You have a vague idea about shell programming and want some sort of
     reference.

    You want to see some shell scripts and some comments to start
     writing your own

    You are migrating from DOS/Windows (or already did) and want to
     make "batch" processes.

    You are a complete nerd and read every how-to available


  2.

  Very simple Scripts

  This HOW-TO will try to give you some hints about shell script
  programming strongly based on examples.

  In this section you'll find some little scripts which will hopefully
  help you to understand some techniques.


  2.1.


  Traditional hello world script



                 #!/bin/bash
                 echo Hello World



  This script has only two lines.  The first indicates the system which
  program to use to run the file.

  The second line is the only action performed by this script, which
  prints 'Hello World' on the terminal.

  If you get something like ./hello.sh: Command not found.  Probably the
  first line '#!/bin/bash' is wrong, issue whereis bash or see

  2.2.

  A very simple backup script



               #!/bin/bash
               tar -cZf /var/my-backup.tgz /home/me/



  In this script, instead of printing a message on the terminal, we
  create a tar-ball of a user's home directory. This is NOT intended to
  be used, a more useful backup script is presented later in this
  document.

  3.

  All about redirection

  3.1.

  Theory and quick reference

  There are 3 file descriptors, stdin, stdout and stderr (std=standard).



  Basically you can:

  1. redirect stdout to a file

  2. redirect stderr to a file

  3. redirect stdout to a stderr

  4. redirect stderr to a stdout

  5. redirect stderr and stdout to a file

  6. redirect stderr and stdout to stdout

  7. redirect stderr and stdout to stderr

     1 'represents' stdout and 2 stderr.

  A little note for seeing this things: with the less command you can
  view both stdout (which will remain on the buffer) and the stderr that
  will be printed on the screen, but erased as you try to 'browse' the
  buffer.

  3.2.  Sample: stdout 2 file

  This will cause the ouput of a program to be written to a file.


               ls -l > ls-l.txt



  Here, a file called 'ls-l.txt' will be created and it will contain
  what you would see on the screen if you type the command 'ls -l' and
  execute it.

  3.3.  Sample: stderr 2 file

  This will cause the stderr ouput of a program to be written to a file.


               grep da * 2> grep-errors.txt



  Here, a file called 'grep-errors.txt' will be created and it will con
  tain what you would see the stderr portion of the output of the 'grep
  da *' command.

  3.4.

  Sample: stdout 2 stderr

  This will cause the stderr ouput of a program to be written to the
  same filedescriptor than stdout.


               grep da * 1>&2



  Here, the stdout portion of the command is sent to stderr, you may
  notice that in differen ways.

  3.5.  Sample: stderr 2 stdout

  This will cause the stderr ouput of a program to be written to the
  same filedescriptor than stdout.


               grep * 2>&1



  Here, the stderr portion of the command is sent to stdout, if you pipe
  to less, you'll see that lines that normally 'dissapear' (as they are
  written to stderr) are being kept now (because they're on stdout).

  3.6.  Sample: stderr and stdout 2 file

  This will place every output of a program to a file. This is suitable
  sometimes for cron entries, if you want a command to pass in absolute
  silence.


               rm -f $(find / -name core) &> /dev/null



  This (thinking on the cron entry) will delete every file called 'core'
  in any directory. Notice that you should be pretty sure of what a com
  mand is doing if you are going to wipe it's output.

  4.

  Pipes

  This section explains in a very simple and practical way how to use
  pipes, nd why you may want it.


  4.1.

  What they are and why you'll want to use them

  Pipes let you use (very simple, I insist) the output of a program as
  the input of another one

  4.2.  Sample: simple pipe with sed

  This is very simple way to use pipes.


               ls -l | sed -e "s/[aeio]/u/g"



  Here, the following happens: first the command ls -l is executed, and
  it's output, instead of being printed, is sent (piped) to the sed pro
  gram, which in turn, prints what it has to.

  4.3.  Sample: an alternative to ls -l *.txt

  Probably, this is a more difficult way to do ls -l *.txt, but it is
  here for illustrating pipes, not for solving such listing dilema.


               ls -l | grep "\.txt$"



  Here, the output of the program ls -l is sent to the grep program,
  which, in turn, will print lines which match the regex "\.txt$".

  5.


  Variables

  You can use variables as in any programming languages.  There are no
  data types. A variable in bash can contain a number, a character, a
  string of characters.

  You have no need to declare a variable, just assigning a value to its
  reference will create it.



  5.1.

  Sample: Hello World! using variables



                   #!/bin/bash
                   STR="Hello World!"
                   echo $STR



  Line 2 creates a variable called STR and assigns the string "Hello
  World!" to it. Then the VALUE of this variable is retrieved by putting
  the '$' in at the beginning. Please notice (try it!)  that if you
  don't use the '$' sign, the output of the program will be different,
  and probably not what you want it to be.

  5.2.

  Sample: A very simple backup script (little bit better)



                  #!/bin/bash
                  OF=/var/my-backup-$(date +%Y%m%d).tgz
                  tar -cZf $OF /home/me/



  This script introduces another thing. First of all, you should be
  familiarized with the variable creation and assignation on line 2.
  Notice the expression If you run the script you'll notice that it runs
  the command inside the parenthesis, capturing its output.


  Notice that in this script, the output filename will be different
  every day, due to the format switch to the date command(+%Y%m%d).  You
  can change this by specifying a different format.

  Some more examples:

  echo ls

  echo $(ls)

  5.3.

  Local variables

  Local variables can be created by using the keyword local.



                       #!/bin/bash
                       HELLO=Hello
                       function hello {
                               local HELLO=World
                               echo $HELLO
                       }
                       echo $HELLO
                       hello
                       echo $HELLO



  This example should be enought to show how to use a local variable.

  6.

  Conditionals

  Conditionals let you decide whether to perform an action or not, this
  decision is taken by evaluating an expression.


  6.1.

  Dry Theory

  Conditionals have many forms. The most basic form is: if expression
  then statement where 'statement' is only executed if 'expression'
  evaluates to true.  evaluates to true.xs

  Conditionals have other forms such as: if expression then statement1
  else statement2.  Here 'statement1' is executed  if 'expression' is
  true,otherwise

  Yet another form of conditionals is: if expression1 then statement1
  else if expression2 then statement2 else statement3.  In this form
  there's added only the "ELSE IF 'expression2' THEN 'statement2'" which
  makes statement2 being executed if expression2 evaluates to true. The
  rest is as you may imagine (see previous forms).

  A word about syntax:

  The base for the 'if' constructions in bash is this:

  if [expression];

  then

  code if 'expression' is true.

  fi

  6.2.

  Sample: Basic conditional example if .. then



                   #!/bin/bash
                   if [ "foo" = "foo" ]; then
                      echo expression evaluated as true
                   fi



  The code to be executed if the expression within braces is true can be
  found after the 'then' word and before 'fi' which indicates the end of
  the conditionally executed code.

  6.3.

  Sample: Basic conditional example if .. then ... else



                   #!/bin/bash
                   if [ "foo" = "foo" ]; then
                      echo expression evaluated as true
                   else
                      echo expression evaluated as false
                   fi



  6.4.

  Sample: Conditionals with variables



                   #!/bin/bash
                   T1="foo"
                   T2="bar"
                   if [ "$T1" = "$T2" ]; then
                       echo expression evaluated as true
                   else
                       echo expression evaluated as false
                   fi


  7.


  Loops for, while and until

  In this section you'll find for, while and until loops.

  The for loop is a little bit different from other programming
  languages. Basically, it let's you iterate over a series of

  The while executes a piece of code if the control expression is true,
  and only stops when it is false (or a explicit break is found within
  the executed code.

  The until loop is almost equal to the while loop, except that the code
  is executed while the control expression evaluates to false.

  If you suspect that while and until are very similar you are right.


  7.1.

  For sample



               #!/bin/bash
               for i in $( ls ); do
                   echo item: $i
               done



  On the second line, we declare i to be the variable that will take the
  different values contained in $( ls ).

  The third line could be longer if needed, or there could be more lines
  before the done (4).

  finished and $i can take a new value.

  This script has very little sense, but a more useful way to use the
  for loop would be to use it to match only certain files on the
  previous example


  7.2.

  C-like for

  fiesh suggested adding this form of looping. It's a for loop more
  similar to C/perl... for.


               #!/bin/bash
               for i in `seq 1 10`;
               do
                       echo $i
               done


  7.3.

  While sample



                #!/bin/bash
                COUNTER=0
                while [  $COUNTER -lt 10 ]; do
                    echo The counter is $COUNTER
                    let COUNTER=COUNTER+1
                done



  This script 'emulates' the well known (C, Pascal, perl, etc) 'for'
  structure

  7.4.

  Until sample



                #!/bin/bash
                COUNTER=20
                until [  $COUNTER -lt 10 ]; do
                    echo COUNTER $COUNTER
                    let COUNTER-=1
                done



  8.

  Functions

  As in almost any programming language, you can use functions to group
  pieces of code in a more logical way or practice the divine art of
  recursion.

  Declaring a function is just a matter of writing function my_func {
  my_code }.

  Calling a function is just like calling another program, you just
  write its name.


  8.1.

  Functions sample



             #!/bin/bash
             function quit {
                 exit
             }
             function hello {
                 echo Hello!
             }
             hello
             quit
             echo foo



  Lines 2-4 contain the 'quit' function. Lines 5-7 contain the 'hello'
  function If you are not absolutely sure about what this script does,
  please try it!.

  Notice that a functions don't need to be declared in any specific
  order.

  When running the script you'll notice that first: the function 'hello'
  is called, second the 'quit' function, and the program never reaches
  line 10.

  8.2.

  Functions with parameters sample



                       #!/bin/bash
                       function quit {
                          exit
                       }
                       function e {
                           echo $1
                       }
                       e Hello
                       e World
                       quit
                       echo foo



  This script is almos identically to the previous one. The main
  difference is the funcion 'e'. This function, prints the first
  argument it receives.  Arguments, within funtions, are treated in the
  same manner as arguments given to the script.

  9.


  User interfaces

  9.1.

  Using select to make simple menus



             #!/bin/bash
             OPTIONS="Hello Quit"
             select opt in $OPTIONS; do
                 if [ "$opt" = "Quit" ]; then
                  echo done
                  exit
                 elif [ "$opt" = "Hello" ]; then
                  echo Hello World
                 else
                  clear
                  echo bad option
                 fi
             done



  If you run this script you'll see that it is a programmer's dream for
  text based menus. You'll probably notice that it's very similar to the
  'for' construction, only rather than looping for each 'word' in
  $OPTIONS, it prompts the user.


  9.2.  Using the command line



                 #!/bin/bash
                 if [ -z "$1" ]; then
                     echo usage: $0 directory
                     exit
                 fi
                 SRCD=$1
                 TGTD="/var/backups/"
                 OF=home-$(date +%Y%m%d).tgz
                 tar -cZf $TGTD$OF $SRCD



  What this script does should be clear to you. The expression in the
  first conditional tests if the program has received an argument ($1)
  and quits if it didn't, showing the user a little usage message.  The
  rest of the script should be clear at this point.

  10.

  Misc

  10.1.

  Reading user input with read

  In many ocations you may want to prompt the user for some input, and
  there are several ways to achive this. This is one of those ways:



                  #!/bin/bash
                  echo Please, enter your name
                  read NAME
                  echo "Hi $NAME!"



  As a variant, you can get multiple values with read, this example may
  clarify this.


                       #!/bin/bash
                       echo Please, enter your firstname and lastname
                       read FN LN
                       echo "Hi! $LN, $FN !"



  10.2.

  Arithmetic evaluation

  On the command line (or a shell) try this:

  echo 1 + 1

  If you expected to see '2' you'll be disappointed. What if you want
  BASH to evaluate some numbers you have? The solution is this:

  echo $((1+1))

  This will produce a more 'logical' output. This is to evaluate an
  arithmetic expression. You can achieve this also like this:

  echo $[1+1]


  If you need to use fractions, or more math or you just want it, you
  can use bc to evaluate arithmetic expressions.

  if i ran "echo $[3/4]" at the command prompt, it would return 0
  because bash  only uses integers when answering. If you  ran "echo
  3/4|bc -l", it would properly return 0.75.

  10.3.  Finding bash

  From a message from mike (see Thanks to)

  you always use #!/bin/bash .. you might was to give an example of

  how to find where bash is located.



  Suggested locations to check:

  ls -l /bin/bash

  ls -l /sbin/bash


  ls -l /usr/local/bin/bash

  ls -l /usr/bin/bash

  ls -l /usr/sbin/bash

  ls -l /usr/local/sbin/bash

  (can't think of any other dirs offhand...  i've found it in

  most of these places before on different system).

  You may try also 'wich bash'.

  10.4.

  Getting the return value of a program

  In bash, the return value of a program is stored in a special variable
  called $?.

  This illustrates how to capture the return value of a program, I
  assume that the directory dada does not exist. (This was also
  suggested by mike)


               #!/bin/bash
               cd /dada &> /dev/null
               echo rv: $?
               cd $(pwd) &> /dev/null
               echo rv: $?



  10.5.  Capturing a commands output

  This little scripts show all tables from all databases (assuming you
  got MySQL installed).  Also, consider changing the 'mysql' command to
  use a valid username and password.


               #!/bin/bash
               DBS=`mysql -uroot  -e"show databases"`
               for b in $DBS ;
               do
                       mysql -uroot -e"show tables from $b"
               done



  10.6.

  Multiple source files

  You can use multiple files with the command source.

  __TO-DO__

  11.


  Tables
  11.1.

  String comparison operators

  (1) s1 = s2

  (2) s1 != s2

  (3) s1 < s2

  (4) s1 > s2

  (5) -n s1

  (6) -z s1



  (1) s1 matches s2

  (2) s1 does not match s2

  (3) __TO-DO__

  (4) __TO-DO__

  (5) s1 is not null (contains one or more characters)

  (6) s1 is null

  11.2.

  String comparison examples

  Comparing two strings.


               #!/bin/bash
               S1='string'
               S2='String'
               if [ $S1=$S2 ];
               then
                       echo "S1('$S1') is not equal to S2('$S2')"
               fi
               if [ $S1=$S1 ];
               then
                       echo "S1('$S1') is equal to S1('$S1')"
               fi



  I quote here a note from a mail, sent buy Andreas Beck, refering to
  use if [ $1 = $2 ].

  This is not quite a good idea, as if either $S1 or $S2 is empty, you
  will get a parse error. x$1=x$2 or "$1"="$2" is better.


  11.3.

  Arithmetic operators

  +

  -

  *

  /

  % (remainder)

  11.4.

  Arithmetic relational operators

  -lt (<)

  -gt (>)

  -le (<=)

  -ge (>=)

  -eq (==)

  -ne (!=)

  C programmer's should simple map the operator to its corresponding
  parenthesis.

  11.5.

  Useful commands

  This section was re-written by Kees (see thank to...)

  Some of these command's almost contain complete programming languages.
  From those commands only the basics will be explained. For a more
  detailed description, have a closer look at the man pages of each
  command.

  sed (stream editor)


  Sed is a non-interactive editor. Instead of altering a file by moving
  the cursor on the screen, you use a script of editing instructions to
  sed, plus the name of the file to edit. You can also describe sed as a
  filter. Let's have a look at some examples:



               $sed 's/to_be_replaced/replaced/g' /tmp/dummy



  Sed replaces the string 'to_be_replaced' with the string 'replaced'
  and reads from the /tmp/dummy file. The result will be sent to stdout
  (normally the console) but you can also add '> capture' to the end of
  the line above so that sed sends the output to the file 'capture'.



               $sed 12, 18d /tmp/dummy



  Sed shows all lines except lines 12 to 18. The original file is not
  altered by this command.

  awk (manipulation of datafiles, text retrieval and processing)


  Many implementations of the AWK programming language exist (most known
  interpreters are GNU's gawk and 'new awk' mawk.) The principle is
  simple: AWK scans for a pattern, and for every matching pattern a
  action will be performed.

  Again, I've created a dummy file containing the following lines:

  "test123

  test

  tteesstt"



               $awk '/test/ {print}' /tmp/dummy



  test123


  test


  The pattern AWK looks for is 'test' and the action it performs when it
  found a line in the file /tmp/dummy with the string 'test' is 'print'.



               $awk '/test/ {i=i+1} END {print i}' /tmp/dummy



  3


  When you're searching for many patterns, you should replace the text
  between the quotes with '-f file.awk' so you can put all patterns and
  actions in 'file.awk'.

  grep (print lines matching a search pattern)


  We've already seen quite a few grep commands in the previous chapters,
  that display the lines matching a pattern. But grep can do more.


               $grep "look for this" /var/log/messages -c



  12

  The string "look for this" has been found 12 times in the file
  /var/log/messages.


  [ok, this example was a fake, the /var/log/messages was tweaked :-)]

  wc (counts lines, words and bytes)


  In the following example, we see that the output is not what we
  expected. The dummy file, as used in this example, contains the
  following text: "bash introduction
   howto test file"



               $wc --words --lines --bytes /tmp/dummy



  2 5 34 /tmp/dummy


  Wc doesn't care about the parameter order. Wc always prints them in a
  standard order, which is, as you can see: .

  sort (sort lines of text files)


  This time the dummy file contains the following text:

  "b

  c

  a"


               $sort /tmp/dummy



  This is what the output looks like:


  a

  b

  c


  Commands shouldn't be that easy :-) bc (a calculator programming
  language)


  Bc is accepting calculations from command line (input from file. not
  from redirector or pipe), but also from a user interface. The
  following demonstration shows some of the commands. Note that

  I start bc using the -q parameter to avoid a welcome message.



          $bc -q



  1 == 5

  0

  0.05 == 0.05

  1

  5 != 5

  0

  2 ^ 8

  256

  sqrt(9)

  3

  while (i != 9) {

  i = i + 1;

  print i

  }

  123456789

  quit

  tput (initialize a terminal or query terminfo database)


  A little demonstration of tput's capabilities:


               $tput cup 10 4



  The prompt appears at (y10,x4).


               $tput reset



  Clears screen and prompt appears at (y1,x1). Note that (y0,x0) is the
  upper left corner.


               $tput cols



  80

  Shows the number of characters possible in x direction.

  It it higly recommended to be familiarized with these programs (at
  least). There are tons of little programs that will let you do real
  magic on the command line.

  [some samples are taken from man pages or FAQs]

  12.

  More Scripts

  12.1.  Applying a command to all files in a directory.



  12.2.

  Sample: A very simple backup script (little bit better)



                   #!/bin/bash
                   SRCD="/home/"
                   TGTD="/var/backups/"
                   OF=home-$(date +%Y%m%d).tgz
                   tar -cZf $TGTD$OF $SRCD



  12.3.

  File re-namer



               #!/bin/sh
               # renna: rename multiple files according to several rules
               # written by felix hudson Jan - 2000

               #first check for the various 'modes' that this program has
               #if the first ($1) condition matches then we execute that portion of the
               #program and then exit

               # check for the prefix condition
               if [ $1 = p ]; then

               #we now get rid of the mode ($1) variable and prefix ($2)
                 prefix=$2 ; shift ; shift

               # a quick check to see if any files were given
               # if none then its better not to do anything than rename some non-existent
               # files!!

                 if [$1 = ]; then
                    echo "no files given"
                    exit 0
                 fi

               # this for loop iterates through all of the files that we gave the program
               # it does one rename per file given
                 for file in $*
                   do
                   mv ${file} $prefix$file
                 done

               #we now exit the program
                 exit 0
               fi

               # check for a suffix rename
               # the rest of this part is virtually identical to the previous section
               # please see those notes
               if [ $1 = s ]; then
                 suffix=$2 ; shift ; shift

                  if [$1 = ]; then
                   echo "no files given"
                  exit 0
                  fi

                for file in $*
                 do
                  mv ${file} $file$suffix
                done

                exit 0
               fi

               # check for the replacement rename
               if [ $1 = r ]; then

                 shift

               # i included this bit as to not damage any files if the user does not specify
               # anything to be done
               # just a safety measure

                 if [ $# -lt 3 ] ; then
                   echo "usage: renna r [expression] [replacement] files... "
                   exit 0
                 fi
               # remove other information
                 OLD=$1 ; NEW=$2 ; shift ; shift

               # this for loop iterates through all of the files that we give the program
               # it does one rename per file given using the program 'sed'
               # this is a sinple command line program that parses standard input and
               # replaces a set expression with a give string
               # here we pass it the file name ( as standard input) and replace the nessesary
               # text

                 for file in $*
                 do
                   new=`echo ${file} | sed s/${OLD}/${NEW}/g`
                   mv ${file} $new
                 done
               exit 0
               fi

               # if we have reached here then nothing proper was passed to the program
               # so we tell the user how to use it
               echo "usage;"
               echo " renna p [prefix] files.."
               echo " renna s [suffix] files.."
               echo " renna r [expression] [replacement] files.."
               exit 0

               # done!



  12.4.

  File renamer (simple)



            #!/bin/bash
            # renames.sh
            # basic file renamer

            criteria=$1
            re_match=$2
            replace=$3

            for i in $( ls *$criteria* );
            do
                src=$i
                tgt=$(echo $i | sed -e "s/$re_match/$replace/")
                mv $src $tgt
            done



  13.

  When something goes wrong (debugging)

  13.1.  Ways Calling BASH

  A nice thing to do is to add on the first line

            #!/bin/bash -x



  This will produce some intresting output information

  14.

  About the document

  Feel free to make suggestions/corrections, or whatever you think it
  would be interesting to see in this document. I'll try to update it as
  soon as I can.

  14.1.

  (no) warranty

  This documents comes with no warranty of any kind.  and all that

  14.2.

  Thanks to


    Nathan Hurst for sending a lot of corrections.

    Jon Abbott for sending comments about evaluating arithmetic
     expressions.

    Laurent Martelli for  translating this document to French (soon
     here the URL)

    Felix Hudson for writing the renna script

    Kees van den Broek (for sending many corrections, re-writting
     usefull comands section)

    Mike (pink) made some suggestions about locating bash and testing
     files

    Fiesh make a nice suggestion for the loops section.

    Lion suggested to mention a common error (./hello.sh: Command not
     found.)

    Andreas Beck made several corrections and coments.

  14.3.

  History

  Added the section usefull commands re-writen by Kess.

  More corrections and suggestions incorporated.

  Samples added on string comparison.

  v0.8 droped the versioning, I guess the date is enought.

  v0.7 More corrections and some old TO-DO sections written.

  v0.6 Minor corrections.

  v0.5 Added the redirection section.

  v0.4 disapperd from its location due to my ex-boss and thid doc found
  it's new place at the proper url: www.linuxdoc.org.

  prior:  I don't rememeber and I didn't use rcs nor cvs :(

  14.4.

  More resources


  Introduction to bash (under BE)
  http://org.laol.net/lamug/beforever/bashtut.htm

  Bourne Shell Programming http://207.213.123.70/book/



  Bash Prompt HOWTO
  Giles Orr, giles@interlog.com
  v0.76 31 December 1999

  Creating and controlling terminal and xterm prompts is discussed,
  including incorporating standard escape sequences to give username,
  current working directory, time, etc.  Further suggestions are made on
  how to modify xterm title bars, use external functions to provide
  prompt information, and how to use ANSI colours.
  ______________________________________________________________________

  Table of Contents



  1. Introduction and Administrivia

     1.1 Requirements
     1.2 How To Use This Document
     1.3 Translations
     1.4 Problems
     1.5 Send Me Comments and Suggestions
     1.6 Credits
     1.7 Copyright and Disclaimer

  2. Bash and Bash Prompts

     2.1 What is Bash?
     2.2 What Can Tweaking Your Bash Prompt Do For You?
     2.3 Why Bother?
     2.4 The First Step
     2.5 Bash Prompt Escape Sequences
     2.6 Setting the PS? Strings Permanently

  3. Bash Programming and Shell Scripts

     3.1 Variables
     3.2 Quotes and Special Characters
     3.3 Command Substitution
     3.4 Non-Printing Characters in Prompts
     3.5 Sourcing a File
     3.6 Functions, Aliases, and the Environment

  4. External Commands

     4.1 PROMPT_COMMAND
     4.2 External Commands in the Prompt
     4.3 What to Put in Your Prompt

  5. Xterm Title Bar Manipulations

  6. ANSI Escape Sequences: Colours and Cursor Movement

     6.1 Colours
     6.2 Cursor Movement
     6.3 Moving the Cursor With tput

  7. Special Characters: Octal Escape Sequences

  8. The Bash Prompt Package

     8.1 Availability
     8.2 Xterm Fonts
     8.3 Changing the Xterm Font

  9. Loading a Different Prompt

     9.1 Loading a Different Prompt, Later
     9.2 Loading a Different Prompt, Immediately

  10. Loading Prompt Colours Dynamically

     10.1 A "Proof of Concept" Example

  11. Prompt Code Snippets

     11.1 Built-in Escape Sequences
     11.2 Date and Time
     11.3 Counting Files in the Current Directory
     11.4 Total Bytes in the Current Directory
     11.5 Checking the Current TTY
     11.6 Suspended Job Count
     11.7 Uptime and Load
     11.8 Number of Processes
     11.9 Controlling the Width of $PWD
     11.10 Laptop Power
     11.11 Having the Prompt Ignored on Cut and Paste
     11.12 Setting the Window Title and Icon Title Separately

  12. Example Prompts

     12.1 Examples on the Web Over time, many people have e-mailed me excellent examples, and I've written some interesting ones myself.  There are far too many to include here, so I have put all of the examples together into some web pages which can be seen at
     12.2 A "Lightweight" Prompt
     12.3 Elite from Bashprompt Themes
     12.4 A "Power User" Prompt
     12.5 Prompt Depending on Connection Type
     12.6 A Prompt the Width of Your Term
     12.7 The Elegant Useless Clock Prompt


  ______________________________________________________________________

  1.  Introduction and Administrivia

  1.1.  Requirements

  You will need Bash.  The default version for most distributions is
  either 1.14.7, or 2.0.x.  1.14.7 was the standard for years, but is
  slowly being replaced.  I've been using Bash 2.0.x for quite a while
  now, but almost all code presented here should work under 1.14.7.  If
  I'm aware of a problem, I'll mention it.  You can check your Bash
  version by typing echo $BASH_VERSION at the prompt.  On my machine, it
  responds with 2.03.6(1)-release.


  Shell programming experience would be good, but isn't essential: the
  more you know, the more complex the prompts you'll be able to create.
  I assume a basic knowledge of shell programming and Unix utilities as
  I go through this tutorial.  However, my own shell programming skills
  are limited, so I give a lot of examples and explanation that may
  appear unnecessary to an experienced shell programmer.


  1.2.  How To Use This Document

  I include a lot of examples and explanatory text.  Different parts
  will be of varying usefulness to different people.  This has grown
  long enough that reading it straight through would be difficult - just
  read the sections you need, backtrack as necessary.


  1.3.  Translations

  Japanese: http://www.jf.linux.or.jp/JF/JF-ftp/other-formats/Bash-
  Prompt/Bash-Prompt-HOWTO.html, provided by Akira Endo,
  akendo@t3.rim.or.jp.


  German: translation is in progress by Thomas Keil, thomas@h-
  preissler.de.


  Italian: by Daniel Dui, ddui@iee.org, available at
  http://www.crs4.it/~dui/linux.html.



  Portugese: translation is in progress by Mrio Gamito,
  mario.gamito@mail.telepac.pt.



  Spanish: translation by Iosu Santurtn iosu@bigfoot.com at
  http://mipagina.euskaltel.es/iosus/linux/Bash-Prompt-HOWTO.html.


  Dutch: translation is in progress by Ellen Bokhorst elboh@gironet.nl,
  and it will be available at http://www.nl.linux.org/doc/HOWTO.


  Chinese: translation in progress by Allen Huang
  lancelot@tomail.com.tw.  I will include a URL when I have it.


  Many thanks to all of them!  URLs will be included as they're
  available.


  If you are working on a translation, please notify me - especially if
  it's available at a linkable URL.  Thanks.


  1.4.  Problems

  This is a list of problems I've noticed while programming prompts.
  Don't start reading here, and don't let this list discourage you -
  these are mostly quite minor details.  Just check back if you run into
  anything odd.


  o  Many Bash features (such as math within $(()) among others) are
     compile time options.  If you're using a binary distribution such
     as comes with a standard Linux distribution, all such features
     should be compiled in.  But if you're working on someone else's
     system, this is worth keeping in mind if something you expected to
     work doesn't.  Some notes about this in Learning the Bash Shell,
     p.260-262.

  o  The terminal screen manager "screen" doesn't always get along with
     ANSI colours.  I'm not a screen expert, unfortunately.  My current
     version of screen (3.7.6-1, an RPM package) seems to work well in
     all cases, but I've seen occasions where screen reduced all prompt
     colours to the standard foreground colour in X terminals.  This
     doesn't appear to be a problem on the console.

  o  Xdefaults files can override colours.  Look in ~/.Xdefaults for
     lines referring to XTerm*background and XTerm*foreground (or
     possibly XTerm*Background and XTerm*Foreground).

  o  One of the prompts mentioned in this document uses the output of
     "jobs" - as discussed at that time, "jobs" output to a pipe is
     broken in Bash 2.02.

  o  ANSI cursor movement escape sequences aren't all implemented in all
     X terminals.  That's discussed in its own section.

  o  Some nice looking pseudo-graphics can be created by using a VGA
     font rather than standard Linux fonts.  Unfortunately, these
     effects look awful if you don't use a VGA font, and there's no way
     to detect within a term what kind of font it's using.

  o  Bash 2.0+ is out, and it incorporates some new features, and
     changes some behaviour.  Things that work under 1.14.7 don't
     necessarily work the same under 2.0+, or vice versa.


  1.5.  Send Me Comments and Suggestions

  This is a learning experience for me.  I've come to know a fair bit
  about what can be done to create interesting and useful Bash Prompts,
  but I need your input to correct and improve this document.  I've
  tried to check suggestions I make against different versions of Bash
  (2.0x and 1.14.7), but let me know of any incompatibilities you find.


  The latest version of this document should always be available at
  http://www.interlog.com/~giles/bashprompt/.  Please check this out,
  and feel free to e-mail me at giles@interlog.com with suggestions.


  I use the Linux Documentation Project HOWTOs almost exclusively in the
  HTML format, so when I convert this from SGML (its native format),
  HTML is the only format I check thoroughly.  If there are problems
  with other formats, I may not know about them, and I'd appreciate a
  note about them.


  1.6.  Credits

  In producing this document, I have borrowed heavily from the work of
  the Bashprompt project at http://bash.current.nu/.  Other sources used
  include the xterm Title mini-HOWTO by Ric Lister, available at
  http://sunsite.unc.edu/LDP/HOWTO/mini/Xterm-Title.html, Ansi Prompts
  by Keebler, available at http://www.ncal.verio.com/~keebler/ansi.html,
  How to make a Bash Prompt Theme by Stephen Webb, available at
  http://bash.current.nu/bash/HOWTO.html, and X ANSI Fonts by Stumpy,
  available at http://home.earthlink.net/~us5zahns/enl/ansifont.html.


  Also of immense help were several conversations and e-mails from Dan,
  an ex-co-worker at Georgia College & State University, whose knowledge
  of Unix far exceeds mine.  He's given me several excellent
  suggestions, and ideas of his have led to some interesting prompts.


  Three books that have been very useful while programming prompts are
  Linux in a Nutshell by Jessica Heckman Perry (O'Reilly, 1997),
  Learning the Bash Shell by Cameron Newham and Bill Rosenblatt
  (O'Reilly, 2nd. ed., 1998) and Unix Shell Programming by Lowell Jay
  Arthur (Wiley, 1986.  This is the first edition, the fourth came out
  in 1997).


  1.7.  Copyright and Disclaimer

  This document is copyright 1998-1999 by Giles Orr.  You are encouraged
  to redistribute it.  You may not modify this document (see the section
  on contacting me: I incorporate most changes recommended by readers).
  Please contact me if you're interested in doing a translation.


  This document is available for free, and, while I have done the best I
  can to make it accurate and up to date, I take no responsibility for
  any problems you may encounter resulting from the use of this
  document.



  2.  Bash and Bash Prompts

  2.1.  What is Bash?

  Descended from the Bourne Shell, Bash is a GNU product, the "Bourne
  Again SHell."  It's the standard command line interface on most Linux
  machines.  It excels at interactivity, supporting command line
  editing, completion, and recall.  It also supports configurable
  prompts - most people realize this, but don't know how much can be
  done.


  2.2.  What Can Tweaking Your Bash Prompt Do For You?

  Most Linux systems have a default prompt in one colour (usually gray)
  that tells you your user name, the name of the machine you're working
  on, and some indication of your current working directory.  This is
  all useful information, but you can do much more with the prompt: all
  sorts of information can be displayed (tty number, time, date, load,
  number of users, uptime ...) and the prompt can use ANSI colours,
  either to make it look interesting, or to make certain information
  stand out.  You can also manipulate the title bar of an Xterm to
  reflect some of this information.


  2.3.  Why Bother?

  Beyond looking cool, it's often useful to keep track of system
  information.  One idea that I know appeals to some people is that it
  makes it possible to put prompts on different machines in different
  colours.  If you have several Xterms open on several different
  machines, or if you tend to forget what machine you're working on and
  delete the wrong files (or shut down the server instead of the
  workstation), you'll find this a great way to remember what machine
  you're on.


  For myself, I like the utility of having information about my machine
  and work environment available all the time.  And I like the challenge
  of trying to figure out how to put the maximum amount of information
  into the smallest possible space while maintaining readability.



  2.4.  The First Step

  The appearance of the prompt is governed by the shell variable PS1.
  Command continuations are indicated by the PS2 string, which can be
  modified in exactly the same ways discussed here - since controlling
  it is exactly the same, and it isn't as "interesting," I'll mostly be
  modifying the PS1 string.  (There are also PS3 and PS4 strings.  These
  are never seen by the average user - see the Bash man page if you're
  interested in their purpose.)  To change the way the prompt looks, you
  change the PS1 variable.  For experimentation purposes, you can enter
  the PS1 strings directly at the prompt, and see the results
  immediately (this only affects your current session, and the changes
  go away when you log out).  If you want to make a change to the prompt
  permanent, look at the section below ``Setting the PS? Strings
  Permanently''.


  Before we get started, it's important to remember that the PS1 string
  is stored in the environment like any other environment variable.  If
  you modify it at the command line, your prompt will change.  Before
  you make any changes, you can save your current prompt to another
  environment variable:
       [giles@nikola giles]$ SAVE=$PS1
       [giles@nikola giles]$



  The simplest prompt would be a single character, such as:



       [giles@nikola giles]$ PS1=$
       $ls
       bin   mail
       $



  This demonstrates the best way to experiment with basic prompts,
  entering them at the command line.  Notice that the text entered by
  the user appears immediately after the prompt: I prefer to use



       $PS1="$ "
       $ ls
       bin   mail
       $



  which forces a space after the prompt, making it more readable.  To
  restore your original prompt, just call up the variable you stored:



       $ PS1=$SAVE
       [giles@nikola giles]$



  2.5.  Bash Prompt Escape Sequences

  There are a lot of escape sequences offered by the Bash shell for
  insertion in the prompt.  From the Bash 2.02 man page:



  When executing interactively, bash  displays  the  primary
  prompt  PS1  when  it  is ready to read a command, and the
  secondary prompt PS2 when it needs more input to  complete
  a  command.   Bash  allows these prompt strings to be cus-
  tomized by inserting a number of backslash-escaped special
  characters that are decoded as follows:
         \a     an ASCII bell character (07)
         \d     the  date  in  "Weekday  Month  Date" format
                (e.g., "Tue May 26")
         \e     an ASCII escape character (033)
         \h     the hostname up to the first `.'
         \H     the hostname
         \n     newline
         \r     carriage return
         \s     the name of the shell, the  basename  of  $0
                (the portion following the final slash)
         \t     the current time in 24-hour HH:MM:SS format
         \T     the current time in 12-hour HH:MM:SS format
         \@     the current time in 12-hour am/pm format
         \u     the username of the current user
         \v     the version of bash (e.g., 2.00)
         \V     the  release  of  bash, version + patchlevel
                (e.g., 2.00.0)
         \w     the current working directory
         \W     the basename of the current  working  direc-
                tory
         \!     the history number of this command
         \#     the command number of this command
         \$     if  the effective UID is 0, a #, otherwise a
                $
         \nnn   the character  corresponding  to  the  octal
                number nnn
         \\     a backslash
         \[     begin a sequence of non-printing characters,
                which could be used to embed a terminal con-
                trol sequence into the prompt
         \]     end a sequence of non-printing characters



  Continuing where we left off:



       [giles@nikola giles]$ PS1="\u@\h \W> "
       giles@nikola giles> ls
       bin   mail
       giles@nikola giles>



  This is similar to the default on most Linux distributions.  I wanted
  a slightly different appearance, so I changed this to:



       giles@nikola giles> PS1="[\t][\u@\h:\w]\$ "
       [21:52:01][giles@nikola:~]$ ls
       bin   mail
       [21:52:15][giles@nikola:~]$


  2.6.  Setting the PS? Strings Permanently

  Various people and distributions set their PS? strings in different
  places.  The most common places are /etc/profile, /etc/bashrc,
  ~/.bash_profile, and ~/.bashrc .  Johan Kullstam (johan19@idt.net)
  writes:



       the PS1 string should be set in .bashrc.  this is because
       non-interactive bashes go out of their way to unset PS1.
       the bash man page tells how the presence or absence of PS1
       is a good way of knowing whether one is in an interactive vs
       non-interactive (ie script) bash session.


       the way i realized this is that startx is a bash script.
       what this means is, startx will wipe out your prompt.  when
       you set PS1 in .profile (or .bash_profile), login at
       console, fire up X via startx, your PS1 gets nuked in the
       process leaving you with the default prompt.


       one workaround is to launch xterms and rxvts with the -ls
       option to force them to read .profile.  but any time a shell
       is called via a non-interactive shell-script middleman PS1
       is lost.  system(3) uses sh -c which if sh is bash will kill
       PS1.  a better way is to place the PS1 definition in
       .bashrc.  this is read every time bash starts and is where
       interactive things - eg PS1 should go.


       therefore it should be stressed that PS1=..blah.. should be
       in .bashrc and not .profile.



  I tried to duplicate the problem he explains, and encountered a
  different one: my PROMPT_COMMAND variable (which will be introduced
  later) was blown away.  My knowledge in this area is somewhat shaky,
  so I'm going to go with what Johan says.



  3.  Bash Programming and Shell Scripts

  3.1.  Variables

  I'm not going to try to explain all the details of Bash scripting in a
  section of this HOWTO, just the details pertaining to prompts.  If you
  want to know more about shell programming and Bash in general, I
  highly recommend Learning the Bash Shell by Cameron Newham and Bill
  Rosenblatt (O'Reilly, 1998).  Oddly, my copy of this book is quite
  frayed.  Again, I'm going to assume that you know a fair bit about
  Bash already.  You can skip this section if you're only looking for
  the basics, but remember it and refer back if you proceed much
  farther.


  Variables in Bash are assigned much as they are in any programming
  language:



  testvar=5
  foo=zen
  bar="bash prompt"



  Quotes are only needed in an assignment if a space (or special
  character, discussed shortly) is a part of the variable.


  Variables are referenced slightly differently than they are assigned:



       > echo $testvar
       5
       > echo $foo
       zen
       > echo ${bar}
       bash prompt
       > echo $NotAssigned

       >



  A variable can be referred to as $bar or ${bar}.  The braces are
  useful when it is unclear what is being referenced: if I write $barley
  do I mean ${bar}ley or ${barley}?  Note also that referencing a value
  that hasn't been assigned doesn't generate an error, instead returning
  nothing.


  3.2.  Quotes and Special Characters

  If you wish to include a special character in a variable, you will
  have to quote it differently:



       > newvar=$testvar
       > echo $newvar
       5
       > newvar="$testvar"
       > echo $newvar
       5
       > newvar='$testvar'
       > echo $newvar
       $testvar
       > newvar=\$testvar
       > echo $newvar
       $testvar
       >



  The dollar sign isn't the only character that's special to the Bash
  shell, but it's a simple example.  An interesting step we can take to
  make use of assigning a variable name to another variable name is to
  use eval to dereference the stored variable name:



  > echo $testvar
  5
  > echo $newvar
  $testvar
  > eval echo $newvar
  5
  >



  Normally, the shell does only one round of substitutions on the
  expression it is evaluating: if you say echo $newvar the shell will
  only go so far as to determine that $newvar is equal to the text
  string $testvar, it won't evaluate what $testvar is equal to.  eval
  forces that evaluation.


  3.3.  Command Substitution

  In almost all cases in this document, I use the $(<command>)
  convention for command substitution: that is,



       $(date +%H%M)



  means "substitute the output from the date +%H%M command here." This
  works in Bash 2.0+.  In some older versions of Bash, prior to 1.14.7,
  you may need to use backquotes (`date +%H%M`).  Backquotes can be used
  in Bash 2.0+, but are being phased out in favor of $(), which nests
  better.  If you're using an earlier version of Bash, you can usually
  substitute backquotes where you see $().  If the command substitution
  is escaped (ie.  \$(command) ), then use backslashes to escape BOTH
  your backquotes (ie.  \'command\' ).


  3.4.  Non-Printing Characters in Prompts

  Many of the changes that can be made to Bash prompts that are
  discussed in this HOWTO use non-printing characters.  Changing the
  colour of the prompt text, changing an Xterm title bar, and moving the
  cursor position all require non-printing characters.


  If I want a very simple prompt consisting of a greater-than sign and a
  space:



       [giles@nikola giles]$ PS1='> '
       >



  This is just a two character prompt.  If I modify it so that it's a
  bright yellow greater-than sign (colours are discussed in their own
  section):



  > PS1='\033[1;33m>\033[0m '
  >



  This works fine - until you type in a large command line.  Because the
  prompt still only consists of two printing characters (a greater-than
  sign and a space) but the shell thinks that this prompt is eleven
  characters long (I think it counts '\033' , '[1' and '[0' as one
  character each).  You can see this by typing a really long command
  line - you will find that the shell wraps the text before it gets to
  the edge of the terminal, and in most cases wraps it badly.  This is
  because it's confused about the actual length of the prompt.


  So use this instead:



       > PS1='\[\033[1;33m\]>\[\033[0m\] '



  This is more complex, but it works.  Command lines wrap properly.
  What's been done is to enclose the '\033[1;33m' that starts the yellow
  colour in square brackets, including the brackets themselves, is a
  non-printing character."  The same is done with the '\033[0m' that
  ends the colour.


  3.5.  Sourcing a File

  When a file is sourced (by typing either source filename or .
  filename at the command line), the lines of code in the file are
  executed as if they were printed at the command line.  This is
  particularly useful with complex prompts, to allow them to be stored
  in files and called up by sourcing the file they are in.


  In examples, you will find that I often include #!/bin/bash at the
  beginning of files including functions.  This is not necessary if you
  are sourcing a file, just as it isn't necessary to chmod +x a file
  that is going to be sourced.  I do this because it makes Vim (my
  editor of choice, no flames please - you use what you like) think I'm
  editing a shell script and turn on colour syntax highlighting.


  3.6.  Functions, Aliases, and the Environment

  As mentioned earlier, PS1, PS2, PS3, PS4, and PROMPT_COMMAND are all
  stored in the Bash environment.  For those of us coming from a DOS
  background, the idea of tossing big hunks of code into the environment
  is horrifying, because that DOS environment was small, and didn't
  exactly grow well.  There are probably practical limits to what you
  can and should put in the environment, but I don't know what they are,
  and we're probably talking a couple of orders of magnitude larger than
  what DOS users are used to.  As Dan put it:


  "In my interactive shell I have 62 aliases and 25 functions.  My rule
  of thumb is that if I need something solely for interactive use and
  can handily write it in bash I make it a shell function (assuming it
  can't be easily expressed as an alias).  If these people are worried
  about memory they don't need to be using bash.  Bash is one of the
  largest programs I run on my linux box (outside of Oracle).  Run top
  sometime and press 'M' to sort by memory - see how close bash is to
  the top of the list.  Heck, it's bigger than sendmail!  Tell 'em to go
  get ash or something."


  I guess he was using console only the day he tried that: running X and
  X apps, I have a lot of stuff larger than Bash.  But the idea is the
  same: the environment is something to be used, and don't worry about
  overfilling it.


  I risk censure by Unix gurus when I say this (for the crime of over-
  simplification), but functions are basically small shell scripts that
  are loaded into the environment for the purpose of efficiency.
  Quoting Dan again: "Shell functions are about as efficient as they can
  be.  It is the approximate equivalent of sourcing a bash/bourne shell
  script save that no file I/O need be done as the function is already
  in memory.  The shell functions are typically loaded from [.bashrc or
  .bash_profile] depending on whether you want them only in the initial
  shell or in subshells as well.  Contrast this with running a shell
  script: Your shell forks, the child does an exec, potentially the path
  is searched, the kernel opens the file and examines enough bytes to
  determine how to run the file, in the case of a shell script a shell
  must be started with the name of the script as its argument, the shell
  then opens the file, reads it and executes the statements.  Compared
  to a shell function, everything other than executing the statements
  can be considered unnecessary overhead."


  Aliases are simple to create:



       alias d="ls --color=tty --classify"
       alias v="d --format=long"
       alias rm="rm -i"



  Any arguments you pass to the alias are passed to the command line of
  the aliased command (ls in the first two cases).  Note that aliases
  can be nested, and they can be used to make a normal unix command
  behave in a different way.  (I agree with the argument that you
  shouldn't use the latter kind of aliases - if you get in the habit of
  relying on "rm *" to ask you if you're sure, you may lose important
  files on a system that doesn't use your alias.)


  Functions are used for more complex program structures.  As a general
  rule, use an alias for anything that can be done in one line.
  Functions differ from shell scripts in that they are loaded into the
  environment so that they work more quickly.  As a general rule again,
  you would want to keep functions relatively small, and any shell
  script that gets relatively large should remain a shell script rather
  than turning it into a function.  Your decision to load something as a
  function is also going to depend on how often you use it.  If you use
  a small shell script infrequently, leave it as a shell script.  If you
  use it often, turn it into a function.


  To modify the behaviour of ls, you could do something like the
  following:


       function lf
       {
           ls --color=tty --classify $*
           echo "$(ls -l $* | wc -l) files"
       }



  This could readily be set as an alias, but for the sake of example,
  we'll make it a function.  If you type the text shown into a text file
  and then source that file, the function will be in your environment,
  and be immediately available at the command line without the overhead
  of a shell script mentioned previously.  The usefulness of this
  becomes more obvious if you consider adding more functionality to the
  above function, such as using an if statement to execute some special
  code when links are found in the listing.



  4.  External Commands

  4.1.  PROMPT_COMMAND

  Bash provides another environment variable called PROMPT_COMMAND.  The
  contents of this variable are executed as a regular Bash command just
  before Bash displays a prompt.



       [21:55:01][giles@nikola:~] PS1="[\u@\h:\w]\$ "
       [giles@nikola:~] PROMPT_COMMAND="date +%H%M"
       2155
       [giles@nikola:~] d
       bin   mail
       2156
       [giles@nikola:~]



  What happened above was that I changed PS1 to no longer include the \t
  escape sequence, so the time was no longer a part of the prompt.  Then
  I used date +%H%M to display the time in a format I like better.  But
  it appears on a different line than the prompt.  Tidying this up using
  echo -n ... as shown below works with Bash 2.0+, but appears not to
  work with Bash 1.14.7: apparently the prompt is drawn in a different
  way, and the following method results in overlapping text.



       2156
       [giles@nikola:~] PROMPT_COMMAND="echo -n [$(date +%H%M)]"
       [2156][giles@nikola:~]$
       [2156][giles@nikola:~]$ d
       bin   mail
       [2157][giles@nikola:~]$ unset PROMPT_COMMAND
       [giles@nikola:~]



  echo -n ... controls the output of the date command and supresses the
  trailing newline, allowing the prompt to appear all on one line.  At
  the end, I used the unset command to remove the PROMPT_COMMAND
  environment variable.



  4.2.  External Commands in the Prompt

  You can use the output of regular Linux commands directly in the
  prompt as well.  Obviously, you don't want to insert a lot of
  material, or it will create a large prompt.  You also want to use a
  fast command, because it's going to be executed every time your prompt
  appears on the screen, and delays in the appearance of your prompt
  while you're working can be very annoying.  (Unlike the previous
  example that this closely resembles, this does work with Bash 1.14.7.)



       [21:58:33][giles@nikola:~]$ PS1="[\$(date +%H%M)][\u@\h:\w]\$ "
       [2159][giles@nikola:~]$ ls
       bin   mail
       [2200][giles@nikola:~]$



  It's important to notice the backslash before the dollar sign of the
  command substitution.  Without it, the external command is executed
  exactly once: when the PS1 string is read into the environment.  For
  this prompt, that would mean that it would display the same time no
  matter how long the prompt was used.  The backslash protects the
  contents of $() from immediate shell interpretation, so "date" is
  called every time a prompt is generated.


  Linux comes with a lot of small utility programs like date, grep, or
  wc that allow you to manipulate data.  If you find yourself trying to
  create complex combinations of these programs within a prompt, it may
  be easier to make an alias, function, or shell script of your own, and
  call it from the prompt.  Escape sequences are often required in bash
  shell scripts to ensure that shell variables are expanded at the
  correct time (as seen above with the date command): this is raised to
  another level within the prompt PS1 line, and avoiding it by creating
  functions is a good idea.


  An example of a small shell script used within a prompt is given
  below:


  ______________________________________________________________________
  #!/bin/bash
  #     lsbytesum - sum the number of bytes in a directory listing
  TotalBytes=0
  for Bytes in $(ls -l | grep "^-" | cut -c30-41)
  do
      let TotalBytes=$TotalBytes+$Bytes
  done
  TotalMeg=$(echo -e "scale=3 \n$TotalBytes/1048576 \nquit" | bc)
  echo -n "$TotalMeg"
  ______________________________________________________________________



  I have at times kept this both as a function, or as a shell script in
  my ~/bin directory, which is on my path.  Used in a prompt:



       [2158][giles@nikola:~]$ PS1="[\u@\h:\w (\$(lsbytesum) Mb)]\$ "
       [giles@nikola:~ (0 Mb)]$ cd /bin
       [giles@nikola:/bin (4.498 Mb)]$



  4.3.  What to Put in Your Prompt

  You'll find I put username, machine name, time, and current directory
  name in most of my prompts.  With the exception of the time, these are
  very standard items to find in a prompt, and time is probably the next
  most common addition.  But what you include is entirely a matter of
  personal taste.  Here are examples from people I know to help give you
  ideas.


  Dan's prompt is minimal but very effective, particularly for the way
  he works.



       [giles@nikola:~]$ cur_tty=$(tty | sed -e "s/.*tty\(.*\)/\1/")
       [giles@nikola:~]$ echo $cur_tty
       p4
       [giles@nikola:~]$ PS1="\!,$cur_tty,\$?\$ "
       1095,p4,0$



  Dan doesn't like that having the current working directory can resize
  the prompt drastically as you move through the directory tree, so he
  keeps track of that in his head (or types "pwd").  He learned Unix
  with csh and tcsh, so he uses his command history extensively
  (something many of us weened on Bash do not do), so the first item in
  the prompt is the history number.  The second item is the significant
  characters of the tty (the output of "tty" is cropped with sed), an
  item that can be useful to "screen" users.  The third item is the exit
  value of the last command/pipeline (note that this is rendered useless
  by any command executed within the prompt - you could work around that
  by capturing it to a variable and playing it back, though).  Finally,
  the "\$" is a dollar sign for a regular user, and switches to a hash
  mark ("#") if the user is root.


  Torben Fjerdingstad (tfj@fjerdingstad.dk) wrote to tell me that he
  often suspends jobs and then forgets about them.  He uses his prompt
  to remind himself of suspended jobs:



  [giles@nikola:~]$ function jobcount {
  > jobs|wc -l| awk '{print $1}'
  > }
  [giles@nikola:~]$ export PS1='\W[`jobcount`]# '
  giles[0]# man ls &
  [1] 4150

  [1]+  Stopped (tty output)    man ls
  giles[1]#



  Torben uses awk to trim the whitespace from the output of wc, while I
  would have used sed or tr - not because they're better, but because
  I'm more familiar with them.  There are probably other ways as well.
  Torben also surrounds his PS1 string in single quotes, which prevent
  Bash from immediately interpreting the backquotes, so he doesn't have
  to escape them as I have mentioned.


  NOTE: There is a known bug in Bash 2.02 that causes the jobs command
  (a shell builtin) to return nothing to a pipe.  If you try the above
  under Bash 2.02, you will always get a "0" back regardless of how many
  jobs you have suspended.  This problem is fixed in 2.03.



  5.  Xterm Title Bar Manipulations

  Non-printing escape sequences can be used to produce interesting
  effects in prompts.  To use these escape sequences, you need to
  enclose them in \[ and \] (as discussed in ``Non-Printing Characters
  in Prompts''), telling Bash to ignore this material while calculating
  the size of the prompt.  Failing to include these delimiters results
  in line editing code placing the cursor incorrectly because it doesn't
  know the actual size of the prompt.  Escape sequences must also be
  preceded by \033[ in Bash prior to version 2, or by either \033[ or
  \e[ in later versions.


  If you try to change the title bar of your Xterm with your prompt when
  you're at the console, you'll produce garbage in your prompt.  To
  avoid this, test the TERM environment variable to tell if your prompt
  is going to be in an Xterm.



  ______________________________________________________________________
  function proml
  {
  case $TERM in
      xterm*)
          local TITLEBAR='\[\033]0;\u@\h:\w\007\]'
          ;;
      *)
          local TITLEBAR=''
          ;;
  esac

  PS1="${TITLEBAR}\
  [\$(date +%H%M)]\
  [\u@\h:\w]\
  \$ "
  PS2='> '
  PS4='+ '
  }
  ______________________________________________________________________



  This is a function that can be incorporated into ~/.bashrc.  The
  function name could then be called to execute the function.  The
  function, like the PS1 string, is stored in the environment.  Once the
  PS1 string is set by the function, you can remove the function from
  the environment with unset proml.  Since the prompt can't change from
  being in an Xterm to being at the console, the TERM variable isn't
  tested every time the prompt is generated.  I used continuation
  markers (backslashes) in the definition of the prompt, to allow it to
  be continued on multiple lines.  This improves readability, making it
  easier to modify and debug.


  I define this as a function because this is how the Bashprompt package
  (discussed later in this document: ``The Bash Prompt Package'') deals
  with prompts: it's not the only way to do it, but it works well.  As
  the prompts you use become more complex, it becomes more and more
  cumbersome to type them in at the prompt, and more practical to make
  them into some sort of text file.  In this case, to test this at the
  prompt, save the above as a text file called "proml".  You can work
  with it as follows:



       [giles@nikola:/bin (4.498 Mb)]$ cd          -> Go where you want to save the prompt
       [giles@nikola:~ (0 Mb)]$ vi proml           -> Edit the prompt file
       ...                                         -> Enter the text given above
       [giles@nikola:~ (0 Mb)]$ source proml       -> Read the prompt function
       [giles@nikola:~ (0 Mb)]$ proml              -> Execute the prompt function



  The first step in creating this prompt is to test if the shell we're
  starting is an xterm or not: if it is, the shell variable
  (${TITLEBAR}) is defined.  It consists of the appropriate escape
  sequences, and \u@\h:\w, which puts <user>@<machine>:<working
  directory> in the Xterm title bar.  This is particularly useful with
  minimized Xterms, making them more rapidly identifiable.  The other
  material in this prompt should be familiar from previous prompts we've
  created.

  The only drawback to manipulating the Xterm title bar like this occurs
  when you log into a system on which you haven't set up the title bar
  hack: the Xterm will continue to show the information from the
  previous system that had the title bar hack in place.



  6.  ANSI Escape Sequences: Colours and Cursor Movement

  6.1.  Colours

  As mentioned before, non-printing escape sequences have to be enclosed
  in \[\033[ and \].  For colour escape sequences, they should also be
  followed by a lowercase m.


  If you try out the following prompts in an xterm and find that you
  aren't seeing the colours named, check out your ~/.Xdefaults file (and
  possibly its bretheren) for lines like "XTerm*Foreground:
  BlanchedAlmond".  This can be commented out by placing an exclamation
  mark ("!") in front of it.  Of course, this will also be dependent on
  what terminal emulator you're using.  This is the likeliest place that
  your term foreground colours would be overridden.


  To include blue text in the prompt:



       PS1="\[\033[34m\][\$(date +%H%M)][\u@\h:\w]$ "



  The problem with this prompt is that the blue colour that starts with
  the 34 colour code is never switched back to the regular colour, so
  any text you type after the prompt is still in the colour of the
  prompt.  This is also a dark shade of blue, so combining it with the
  bold code might help:



       PS1="\[\033[1;34m\][\$(date +%H%M)][\u@\h:\w]$\[\033[0m\] "



  The prompt is now in light blue, and it ends by switching the colour
  back to nothing (whatever foreground colour you had previously).


  Here are the rest of the colour equivalences:



       Black       0;30     Dark Gray     1;30
       Blue        0;34     Light Blue    1;34
       Green       0;32     Light Green   1;32
       Cyan        0;36     Light Cyan    1;36
       Red         0;31     Light Red     1;31
       Purple      0;35     Light Purple  1;35
       Brown       0;33     Yellow        1;33
       Light Gray  0;37     White         1;37

  Daniel Dui (ddui@iee.org) points out that to be strictly accurate, we
  must mention that the list above is for colours at the console.  In an
  xterm, the code 1;31 isn't "Light Red," but "Bold Red."  This is true
  of all the colours.


  You can also set background colours by using 44 for Blue background,
  41 for a Red background, etc.  There are no bold background colours.
  Combinations can be used, like Light Red text on a Blue background:
  \[\033[44;1;31m\], although setting the colours separately seems to
  work better (ie. \[\033[44m\]\[\033[1;31m\]).  Other codes available
  include 4: Underscore, 5: Blink, 7: Inverse, and 8: Concealed.


  Aside: Many people (myself included) object strongly to the "blink"
  attribute.  Fortunately, it doesn't work in any terminal emulators
  that I'm aware of - but it will still work on the console.  And, if
  you were wondering (as I did) "What use is a 'Concealed' attribute?!"
  - I saw it used in an example shell script (not a prompt) to allow
  someone to type in a password without it being echoed to the screen.


  Based on a prompt called "elite2" in the Bashprompt package (which I
  have modified to work better on a standard console, rather than with
  the special xterm fonts required to view the original properly), this
  is a prompt I've used a lot:


  ______________________________________________________________________

  function elite
  {

  local GRAY="\[\033[1;30m\]"
  local LIGHT_GRAY="\[\033[0;37m\]"
  local CYAN="\[\033[0;36m\]"
  local LIGHT_CYAN="\[\033[1;36m\]"

  case $TERM in
      xterm*)
          local TITLEBAR='\[\033]0;\u@\h:\w\007\]'
          ;;
      *)
          local TITLEBAR=""
          ;;
  esac

  local GRAD1=$(tty|cut -d/ -f3)
  PS1="$TITLEBAR\
  $GRAY-$CYAN-$LIGHT_CYAN(\
  $CYAN\u$GRAY@$CYAN\h\
  $LIGHT_CYAN)$CYAN-$LIGHT_CYAN(\
  $CYAN\#$GRAY/$CYAN$GRAD1\
  $LIGHT_CYAN)$CYAN-$LIGHT_CYAN(\
  $CYAN\$(date +%H%M)$GRAY/$CYAN\$(date +%d-%b-%y)\
  $LIGHT_CYAN)$CYAN-$GRAY-\
  $LIGHT_GRAY\n\
  $GRAY-$CYAN-$LIGHT_CYAN(\
  $CYAN\$$GRAY:$CYAN\w\
  $LIGHT_CYAN)$CYAN-$GRAY-$LIGHT_GRAY "
  PS2="$LIGHT_CYAN-$CYAN-$GRAY-$LIGHT_GRAY "
  }
  ______________________________________________________________________



  I define the colours as temporary shell variables in the name of
  readability.  It's easier to work with.  The "GRAD1" variable is a
  check to determine what terminal you're on.  Like the test to
  determine if you're working in an Xterm, it only needs to be done
  once.  The prompt you see look like this, except in colour:



       --(giles@nikola)-(75/ttyp7)-(1908/12-Oct-98)--
       --($:~/tmp)--



  To help myself remember what colours are available, I wrote the
  following script which echoes all the colours to screen:



  ______________________________________________________________________
  #!/bin/bash
  #
  #   This file echoes a bunch of colour codes to the terminal to demonstrate
  #   what's available.  Each line is one colour on black and gray
  #   backgrounds, with the code in the middle.  Verified to work on white,
  #   black, and green BGs (2 Dec 98).
  #
  echo "  On Light Gray:        On Black:"
  echo -e "\033[47m\033[1;37m  White        \033[0m\
   1;37m \
  \033[40m\033[1;37m  White        \033[0m"
  echo -e "\033[47m\033[37m  Light Gray   \033[0m\
     37m \
  \033[40m\033[37m  Light Gray   \033[0m"
  echo -e "\033[47m\033[1;30m  Gray         \033[0m\
   1;30m \
  \033[40m\033[1;30m  Gray         \033[0m"
  echo -e "\033[47m\033[30m  Black        \033[0m\
     30m \
  \033[40m\033[30m  Black        \033[0m"
  echo -e "\033[47m\033[31m  Red          \033[0m\
     31m \
  \033[40m\033[31m  Red          \033[0m"
  echo -e "\033[47m\033[1;31m  Light Red    \033[0m\
   1;31m \
  \033[40m\033[1;31m  Light Red    \033[0m"
  echo -e "\033[47m\033[32m  Green        \033[0m\
     32m \
  \033[40m\033[32m  Green        \033[0m"
  echo -e "\033[47m\033[1;32m  Light Green  \033[0m\
   1;32m \
  \033[40m\033[1;32m  Light Green  \033[0m"
  echo -e "\033[47m\033[33m  Brown        \033[0m\
     33m \
  \033[40m\033[33m  Brown        \033[0m"
  echo -e "\033[47m\033[1;33m  Yellow       \033[0m\
   1;33m \
  \033[40m\033[1;33m  Yellow       \033[0m"
  echo -e "\033[47m\033[34m  Blue         \033[0m\
     34m \
  \033[40m\033[34m  Blue         \033[0m"
  echo -e "\033[47m\033[1;34m  Light Blue   \033[0m\
   1;34m \
  \033[40m\033[1;34m  Light Blue   \033[0m"
  echo -e "\033[47m\033[35m  Purple       \033[0m\
     35m \
  \033[40m\033[35m  Purple       \033[0m"
  echo -e "\033[47m\033[1;35m  Pink         \033[0m\
   1;35m \
  \033[40m\033[1;35m  Pink         \033[0m"
  echo -e "\033[47m\033[36m  Cyan         \033[0m\
     36m \
  \033[40m\033[36m  Cyan         \033[0m"
  echo -e "\033[47m\033[1;36m  Light Cyan   \033[0m\
   1;36m \
  \033[40m\033[1;36m  Light Cyan   \033[0m"
  ______________________________________________________________________



  6.2.  Cursor Movement

  ANSI escape sequences allow you to move the cursor around the screen
  at will.  This is more useful for full screen user interfaces
  generated by shell scripts, but can also be used in prompts.  The
  movement escape sequences are as follows:



       - Position the Cursor:
         \033[<L>;<C>H
            Or
         \033[<L>;<C>f
         puts the cursor at line L and column C.
       - Move the cursor up N lines:
         \033[<N>A
       - Move the cursor down N lines:
         \033[<N>B
       - Move the cursor forward N columns:
         \033[<N>C
       - Move the cursor backward N columns:
         \033[<N>D

       - Clear the screen, move to (0,0):
         \033[2J
       - Erase to end of line:
         \033[K

       - Save cursor position:
         \033[s
       - Restore cursor position:
         \033[u



  The latter two codes are NOT honoured by many terminal emulators.  The
  only ones that I'm aware of that do are xterm and nxterm - even though
  the majority of terminal emulators are based on xterm code.  As far as
  I can tell, rxvt, kvt, xiterm, and Eterm do not support them.  They
  are supported on the console.


  Try putting in the following line of code at the prompt (it's a little
  clearer what it does if the prompt is several lines down the terminal
  when you put this in): echo -en "\033[7A\033[1;35m BASH
  \033[7B\033[6D" This should move the cursor seven lines up screen,
  print the word " BASH ", and then return to where it started to
  produce a normal prompt.  This isn't a prompt: it's just a
  demonstration of moving the cursor on screen, using colour to
  emphasize what has been done.


  Save this in a file called "clock":



  ______________________________________________________________________
  #!/bin/bash

  function prompt_command {
  let prompt_x=$COLUMNS-5
  }

  PROMPT_COMMAND=prompt_command

  function clock {
  local       BLUE="\[\033[0;34m\]"
  local        RED="\[\033[0;31m\]"
  local  LIGHT_RED="\[\033[1;31m\]"
  local      WHITE="\[\033[1;37m\]"
  local  NO_COLOUR="\[\033[0m\]"
  case $TERM in
      xterm*)
          TITLEBAR='\[\033]0;\u@\h:\w\007\]'
          ;;
      *)
          TITLEBAR=""
          ;;
  esac

  PS1="${TITLEBAR}\
  \[\033[s\033[1;\$(echo -n \${prompt_x})H\]\
  $BLUE[$LIGHT_RED\$(date +%H%M)$BLUE]\[\033[u\033[1A\]
  $BLUE[$LIGHT_RED\u@\h:\w$BLUE]\
  $WHITE\$$NO_COLOUR "
  PS2='> '
  PS4='+ '
  }
  ______________________________________________________________________



  This prompt is fairly plain, except that it keeps a 24 hour clock in
  the upper right corner of the terminal (even if the terminal is
  resized).  This will NOT work on the terminal emulators that I
  mentioned that don't accept the save and restore cursor position
  codes.  If you try to run this prompt in any of those terminal
  emulators, the clock will appear correctly, but the prompt will be
  trapped on the second line of the terminal.


  See also ``The Elegant Useless Clock Prompt'' for a more extensive use
  of these codes.


  6.3.  Moving the Cursor With tput

  As with so many things in Unix, there is more than one way to achieve
  the same ends.  A utility called "tput" can also be used to move the
  cursor around the screen, or get back information about the status of
  the terminal.  "tput" for cursor positioning is less flexible than
  ANSI escape sequences: you can only move the cursor to an absolute
  position, you can't move it relative to its current position.  I don't
  use "tput," so I'm not going to explain it in detail.  Type "man tput"
  and you'll know as much as I do.



  7.  Special Characters: Octal Escape Sequences

  Outside of the characters that you can type on your keyboard, there
  are a lot of other characters you can print on your screen.  I've
  created a script to allow you to check out what the font you're using
  has available for you.  The main command you need to use to utilise
  these characters is "echo -e".  The "-e" switch tells echo to enable
  interpretation of backslash-escaped characters.  What you see when you
  look at octal 200-400 will be very different with a VGA font from what
  you will see with a standard Linux font.  Be warned that some of these
  escape sequences have odd effects on your terminal, and I haven't
  tried to prevent them from doing whatever they do.  The linedraw and
  block characters (which many of us became familiar with with Word
  Perfect) that are used heavily by the Bashprompt project are between
  octal 260 and 337.



  ______________________________________________________________________
  #!/bin/bash

  #   Script: escgen

  function usage {
     echo -e "\033[1;34mescgen\033[0m <lower_octal_value> [<higher_octal_value>]"
     echo "   Octal escape sequence generator: print all octal escape sequences"
     echo "   between the lower value and the upper value.  If a second value"
     echo "   isn't supplied, print eight characters."
     echo "   1998 - Giles Orr, no warranty."
     exit 1
  }

  if [ "$#" -eq "0" ]
  then
     echo -e "\033[1;31mPlease supply one or two values.\033[0m"
     usage
  fi
  let lower_val=${1}
  if [ "$#" -eq "1" ]
  then
     #   If they don't supply a closing value, give them eight characters.
     upper_val=$(echo -e "obase=8 \n ibase=8 \n $lower_val+10 \n quit" | bc)
  else
     let upper_val=${2}
  fi
  if [ "$#" -gt "2" ]
  then
     echo -e "\033[1;31mPlease supply two values.\033[0m"
     echo
     usage
  fi
  if [ "${lower_val}" -gt "${upper_val}" ]
  then
     echo -e "\033[1;31m${lower_val} is larger than ${upper_val}."
     echo
     usage
  fi
  if [ "${upper_val}" -gt "777" ]
     then
     echo -e "\033[1;31mValues cannot exceed 777.\033[0m"
     echo
     usage
  fi

  let i=$lower_val
  let line_count=1
  let limit=$upper_val
  while [ "$i" -lt "$limit" ]
  do
     octal_escape="\\$i"
     echo -en "$i:'$octal_escape' "
     if [ "$line_count" -gt "7" ]
     then
        echo
        #   Put a hard return in.
        let line_count=0
     fi
     let i=$(echo -e "obase=8 \n ibase=8 \n $i+1 \n quit" | bc)
     let line_count=$line_count+1
  done
  echo
  ______________________________________________________________________


  You can also use xfd to display all the characters in an X font, with
  the command "xfd -fn <fontname>".  Clicking on any given character
  will give you lots of information about that character, including its
  octal value.  The script given above will be useful on the console,
  and if you aren't sure of the current font name.



  8.  The Bash Prompt Package

  8.1.  Availability

  The Bash Prompt package is available at http://bash.current.nu, and is
  the work of several people, co-ordinated by Rob Current (aka
  BadLandZ).  The package is an early beta, but offers a simple way of
  using multiple prompts (or themes), allowing you to set prompts for
  login shells, and for subshells (ie. putting PS1 strings in
  ~/.bash_profile and ~/.bashrc).  Most of the themes use the extended
  VGA character set, so they look bad unless they're used with VGA fonts
  (which aren't the default on most systems).


  8.2.  Xterm Fonts

  To use some of the most attractive prompts in the Bash Prompt package,
  you need to get and install fonts that support the character sets
  expected by the prompts.  These are "VGA Fonts," which support
  different character sets than regular Xterm fonts.  Standard Xterm
  fonts support an extended alphabet, including a lot of letters with
  accents.  In VGA fonts, this material is replaced by graphical
  characters - blocks, dots, lines.  I asked for an explanation of this
  difference, and Srgio Vale e Pace (space@gold.com.br) wrote me:



       I love computer history so here goes:


       When IBM designed the first PC they needed some character
       codes to use, so they got the ASCII character table (128
       numbers, letters, and some punctuation) and to fill a byte
       addressed table they added 128 more characters.  Since the
       PC was designed to be a home computer, they fill the
       remaining 128 characters with dots, lines, points, etc, to
       be able to do borders, and grayscale effects (remember that
       we are talking about 2 color graphics).


       Time passes, PCs become a standard, IBM creates more
       powerful systems and the VGA standard is born, along with
       256 colour graphics, and IBM continues to include their IBM-
       ASCII characters table.


       More time passes, IBM has lost their leadership in the PC
       market, and the OS authors dicover that there are other
       languages in the world that use non-english characters, so
       they add international alphabet support in their systems.
       Since we now have bright and colorful screens, we can trash
       the dots, lines, etc. and use their space for accented
       characters and some greek letters, which you'll see in
       Linux.



  8.3.  Changing the Xterm Font

  Getting and installing these fonts is a somewhat involved process.
  First, retrieve the font(s).  Next, ensure they're .pcf or .pcf.gz
  files.  If they're .bdf files, investigate the "bdftopcf" command (ie.
  read the man page).  Drop the .pcf or .pcf.gz files into the
  /usr/X11R6/lib/X11/fonts/misc dir (this is the correct directory for
  RedHat 5.1 and Slackware 3.4, it may be different on other
  distributions).  "cd" to that directory, and run the "mkfontdir"
  command.  Then run "xset fp rehash".  Sometimes it's a good idea to go
  into the fonts.alias file in the same directory, and create shorter
  alias names for the fonts.


  To use the new fonts, you start your Xterm program of choice with the
  appropriate command to your Xterm, which can be found either in the
  man page or by using the "--help" parameter on the command line.
  Popular terms would be used as follows:



       xterm -font <fontname>



  OR


       xterm -fn <fontname> -fb <fontname-bold>
       Eterm -F <fontname>
       rxvt -fn <fontname>



  VGA fonts are available from Stumpy's ANSI Fonts page at
  http://home.earthlink.net/~us5zahns/enl/ansifont.html (which I have
  borrowed from extensively while writing this).



  9.  Loading a Different Prompt

  9.1.  Loading a Different Prompt, Later

  The explanations in this HOWTO have shown how to make PS1 environment
  variables, or how to incorporate those PS1 and PS2 strings into
  functions that could be called by ~/.bashrc or as a theme by the
  bashprompt package.


  Using the bashprompt package, you would type bashprompt -i to see a
  list of available themes.  To set the prompt in future login shells
  (primarily the console, but also telnet and Xterms, depending on how
  your Xterms are set up), you would type bashprompt -l themename.
  bashprompt then modifies your ~/.bash_profile to call the requested
  theme when it starts.  To set the prompt in future subshells (usually
  Xterms, rxvt, etc.), you type bashprompt -s themename, and bashprompt
  modifies your ~/.bashrc file to call the appropriate theme at startup.


  See also ``Setting the PS? Strings Permanently'' for Johan Kullstam's
  note regarding the importance of putting the PS?  strings in ~/.bashrc
  .


  9.2.  Loading a Different Prompt, Immediately

  You can change the prompt in your current terminal (using the example
  "elite" function above) by typing "source elite" followed by "elite"
  (assuming that the elite function file is the working directory).
  This is somewhat cumbersome, and leaves you with an extra function
  (elite) in your environment space - if you want to clean up the
  environment, you would have to type "unset elite" as well.  This would
  seem like an ideal candidate for a small shell script, but a script
  doesn't work here because the script cannot change the environment of
  your current shell: it can only change the environment of the subshell
  it runs in.  As soon as the script stops, the subshell goes away, and
  the changes the script made to the environment are gone.  What can
  change environment variables of your current shell are environment
  functions.  The bashprompt package puts a function called
  "callbashprompt" into your environment, and, while they don't document
  it, it can be called to load any bashprompt theme on the fly.  It
  looks in the theme directory it installed (the theme you're calling
  has to be there), sources the function you asked for, loads the
  function, and then unsets the function, thus keeping your environment
  uncluttered.  "callbashprompt" wasn't intended to be used this way,
  and has no error checking, but if you keep that in mind, it works
  quite well.



  10.  Loading Prompt Colours Dynamically

  10.1.  A "Proof of Concept" Example

  This is a "proof of concept" more than an attractive prompt: changing
  colours within the prompt dynamically.  In this example, the colour of
  the host name changes depending on the load (as a warning).



  ______________________________________________________________________
  #!/bin/bash
  #   "hostloadcolour" - 17 October 98, by Giles
  #
  #   The idea here is to change the colour of the host name in the prompt,
  #   depending on a threshold load value.

  # THRESHOLD_LOAD is the value of the one minute load (multiplied
  # by one hundred) at which you want
  # the prompt to change from COLOUR_LOW to COLOUR_HIGH
  THRESHOLD_LOAD=200
  COLOUR_LOW='1;34'
            # light blue
  COLOUR_HIGH='1;31'
             # light red

  function prompt_command {
  ONE=$(uptime | sed -e "s/.*load average: \(.*\...\), \(.*\...\), \(.*\...\)/\1/" -e "s/ //g")
  #   Apparently, "scale" in bc doesn't apply to multiplication, but does
  #   apply to division.
  ONEHUNDRED=$(echo -e "scale=0 \n $ONE/0.01 \nquit \n" | bc)
  if [ $ONEHUNDRED -gt $THRESHOLD_LOAD ]
  then
      HOST_COLOUR=$COLOUR_HIGH
          # Light Red
  else
      HOST_COLOUR=$COLOUR_LOW
          # Light Blue
  fi
  }

  function hostloadcolour {

  PROMPT_COMMAND=prompt_command
  PS1="[$(date +%H%M)][\u@\[\033[\$(echo -n \$HOST_COLOUR)m\]\h\[\033[0m\]:\w]$ "
  }
  ______________________________________________________________________



  Using your favorite editor, save this to a file named
  "hostloadcolour".  If you have the Bashprompt package installed, this
  will work as a theme.  If you don't, type source hostloadcolour and
  then hostloadcolour.  Either way, "prompt_command" becomes a function
  in your environment.  If you examine the code, you will notice that
  the colours ($COLOUR_HIGH and $COLOUR_LOW) are set using only a
  partial colour code, ie. "1;34" instead of "\[\033[1;34m\]", which I
  would have preferred.  I have been unable to get it to work with the
  complete code.  Please let me know if you manage this.



  11.  Prompt Code Snippets

  This section shows how to put various pieces of information into the
  Bash prompt.  There are an infinite number of things that could be put
  in your prompt.  Feel free to send me examples, I will try to include
  what I think will be most widely used.  If you have an alternate way
  to retrieve a piece of information here, and feel your method is more
  efficient, please contact me.  It's easy to write bad code, I do it
  often, but it's great to write elegant code, and a pleasure to read
  it.  I manage it every once in a while, and would love to have more of
  it to put in here.

  To incorporate shell code in prompts, it has to be escaped.  Usually,
  this will mean putting it inside \$(<command>) so that the output of
  command is substituted each time the prompt is generated.


  11.1.  Built-in Escape Sequences

  See ``Bash Prompt Escape Sequences'' for a complete list of built-in
  escape sequences.  This list is taken directly from the Bash man page,
  so you can also look there.


  11.2.  Date and Time

  If you don't like the built-ins for date and time, extracting the same
  information from the date command is relatively easy.  Examples
  already seen in this HOWTO include date +%H%M, which will put in the
  hour in 24 hour format, and the minute.  date "+%A, %d %B %Y" will
  give something like "Sunday, 06 June 1999".  For a full list of the
  interpreted sequences, type date --help or man date.


  11.3.  Counting Files in the Current Directory

  To determine how many files there are in the current directory, put in
  ls -l | wc -l.  This uses wc wordcount to do a count of the number of
  lines (-l) in the output of ls -l.  It doesn't count dotfiles.  If you
  want to count only files and NOT include symbolic links (just an
  example of what else you could do), you could use ls -l | grep -v ^l |
  wc -l.  Here, grep checks for any line beginning with "l" (indicating
  a link), and discards that line (-v).


  11.4.  Total Bytes in the Current Directory

  If you want to know how much space the contents of the current
  directory take up, you can use something like the following:



  ______________________________________________________________________
  # The sed command replaces all the spaces with only one space.
  # cut -d" " -f5 : -d determines a delimiter, which means that (in
  # this case) a space begins a new column.
  # -f says to take out a certain column, in this case the fifth one

  let TotalBytes=0

  for Bytes in $(ls -l | grep "^-" | sed -e "s/ \+/ /g" | cut -d" " -f5)
  do
     let TotalBytes=$TotalBytes+$Bytes
  done

  # The if...fi's give a more specific output in byte, kilobyte, megabyte,
  # and gigabyte

  if [ $TotalBytes -lt 1024 ]; then
     TotalSize=$(echo -e "scale=3 \n$TotalBytes \nquit" | bc)
  else if [ $TotalBytes -lt 1048576 ]; then
     TotalSize=$(echo -e "scale=3 \n$TotalBytes/1024 \nquit" | bc)
  else if [ $TotalBytes -lt 1073741824 ]; then
     TotalSize=$(echo -e "scale=3 \n$TotalBytes/1048576 \nquit" | bc)
  else
     TotalSize=$(echo -e "scale=3 \n$TotalBytes/1073741824 \nquit" | bc)
  fi
  fi
  fi
  ______________________________________________________________________



  Code courtesy of Sam Schmit (id@pt.lu) and his uncle Jean-Paul, who
  ironed out a fairly major bug in my original code, and just generally
  cleaned it up.


  11.5.  Checking the Current TTY

  The tty command returns the filename of the terminal connected to
  standard input.  This comes in two formats on the Linux systems I have
  used, either "/dev/tty4" or "/dev/pts/2".  I have taken to using a
  more general solution to this: tty | sed -e "s:/dev/::", which removes
  the leading "/dev/".  Older systems (in my experience, RedHat through
  5.2) returned only filenames in the "/dev/tty4" format, so I used tty
  | sed -e "s/.*tty\(.*\)/\1/".


  An alternative method: ps aux | grep $$ | awk '{ print $7 }'.


  11.6.  Suspended Job Count

  To find out how many suspended jobs you have, use jobs | wc -l | awk
  otherwise include blank spaces that waste space in a prompt.  If you
  start netscape from an xterm, this will also be counted.  If you want
  to avoid that, and only count stopped jobs, use jobs -s instead.  Type
  help jobs for more info on jobs.  jobs will always return nothing to a
  pipe in version 2.02 of Bash: this problem is not present in any other
  version.


  11.7.  Uptime and Load

  Current load is taken from the uptime command.  What I use at the
  moment is uptime | sed -e "s/.*load average: \(.*\...\), .*\...,
  .*\.../\1/" -e "s/ //g" which is clunky in the extreme, but works.
  Replacements welcome.  uptime can also be used in a very similar
  manner to find out how long the machine has been up (obviously) or how
  many users are logged in, and the data could be massaged with sed to
  look the way you want it to.


  11.8.  Number of Processes

  ps ax | wc -l | tr -d " " OR ps ax | wc -l | awk each case, tr or awk
  or sed is used to remove the undesirable whitespace.


  11.9.  Controlling the Width of $PWD

  Unix allows long file names, which can lead to the value of $PWD being
  very long.  Some people (notably the default RedHat prompt) choose to
  use the basename of the current working directory (ie. "giles" if
  $PWD="/home/giles").  I like more info than that, but it's often
  desirable to limit the length of the directory name, and it makes the
  most sense to truncate on the left.


  ______________________________________________________________________
  #   How many characters of the $PWD should be kept
  local pwd_length=30
  if [ $(echo -n $PWD | wc -c | tr -d " ") -gt $pwd_length ]
  then
     newPWD="...$(echo -n $PWD | sed -e "s/.*\(.\{$pwd_length\}\)/\1/")"
  else
     newPWD="$(echo -n $PWD)"
  fi
  ______________________________________________________________________



  The above code can be executed as part of PROMPT_COMMAND, and the
  environment variable generated (newPWD) can then be included in the
  prompt.


  11.10.  Laptop Power

  Again, this isn't elegant, but it works (most of the time).  If you
  have a laptop with APM installed, try power=$(apm | sed -e "s/.*:
  \([1-9][0-9]*\)%/\1/" | tr -d " ") executed from PROMPT_COMMAND to
  create an environment variable you can add to your prompt.  This will
  indicate percentage power remaining.


  11.11.  Having the Prompt Ignored on Cut and Paste


  This one is weird but cool.  Rory Toma (rory@corp.webtv.net) wrote to
  suggest a prompt like this: : rory@demon ; .  How is this useful?
  Well, if you type a command after the prompt (odd idea, that), you can
  triple click on that line (in Linux, anyway) to highlight the whole
  line, then paste that line in front of another prompt, and the stuff
  between the ":" and the """ is ignored, like so:



  ______________________________________________________________________
  : rory@demon ; uptime
    5:15pm  up 6 days, 23:04,  2 users,  load average: 0.00, 0.00, 0.00
  : rory@demon ; : rory@demon ; uptime
    5:15pm  up 6 days, 23:04,  2 users,  load average: 0.00, 0.00, 0.00
  ______________________________________________________________________



  The prompt is a no-op, and if your PS2 is set to a space, multiple
  lines can be cut and pasted as well.


  11.12.  Setting the Window Title and Icon Title Separately

  A suggestion from Charles Lepple (clepple@negativezero.org) on setting
  the window title of the Xterm and the title of the corresponding icon
  separately (first check out the earlier section ``Xterm Title Bar
  Manipulations'').  He uses this under WindowMaker because the title
  that's appropriate for an Xterm is usually too long for a 64x64 icon.
  "\[\e]1;icon-title\007\e]2;main-title\007\]".  He says to set this in
  the prompt command because "I tried putting the string in PS1, but it
  causes flickering under some window managers because it results in
  setting the prompt multiple times when you are editing a multi-line
  command (at least under bash 1.4.x -- and I was too lazy to fully
  explore the reasons behind it)."  I had no trouble with it in the PS1
  string, but didn't use any multi-line commands.  He also points out
  that it works under xterm, xwsh, and dtterm, but not gnome-terminal
  (which uses only the main title).  I also found it to work with rxvt,
  but not kterm.



  12.  Example Prompts

  12.1.  Over time, many people have e-mailed me excellent examples, and
  I've written some interesting ones myself.  There are far too many to
  include here, so I have put all of the examples together into some web
  pages which can be seen athttp://www.interlog.com/~giles/bash-
  prompt/prompts.  Web pages also allow me to include pictures, which I
  can't include in a standard HOWTO.  All of the examples given here
  except Bradley Alexander's "Prompts Depending on Connection Types" can
  also be seen on the web.  Examples on the Web

  12.2.  A "Lightweight" Prompt



  ______________________________________________________________________

  function proml {
  local BLUE="\[\033[0;34m\]"
  local RED="\[\033[0;31m\]"
  local LIGHT_RED="\[\033[1;31m\]"
  local WHITE="\[\033[1;37m\]"
  local NO_COLOUR="\[\033[0m\]"
  case $TERM in
      xterm*)
          TITLEBAR='\[\033]0;\u@\h:\w\007\]'
          ;;
      *)
          TITLEBAR=""
          ;;
  esac

  PS1="${TITLEBAR}\
  $BLUE[$RED\$(date +%H%M)$BLUE]\
  $BLUE[$LIGHT_RED\u@\h:\w$BLUE]\
  $WHITE\$$NO_COLOUR "
  PS2='> '
  PS4='+ '
  }
  ______________________________________________________________________



  12.3.  Elite from Bashprompt Themes

  Note that this requires a VGA font.


  ______________________________________________________________________

  # Created by KrON from windowmaker on IRC
  # Changed by Spidey 08/06
  function elite {
  PS1="\[\033[31m\]\332\304\[\033[34m\](\[\033[31m\]\u\[\033[34m\]@\[\033[31m\]\h\
  \[\033[34m\])\[\033[31m\]-\[\033[34m\](\[\033[31m\]\$(date +%I:%M%P)\
  \[\033[34m\]-:-\[\033[31m\]\$(date +%m)\[\033[34m\033[31m\]/\$(date +%d)\
  \[\033[34m\])\[\033[31m\]\304-\[\033[34m]\\371\[\033[31m\]-\371\371\
  \[\033[34m\]\372\n\[\033[31m\]\300\304\[\033[34m\](\[\033[31m\]\W\[\033[34m\])\
  \[\033[31m\]\304\371\[\033[34m\]\372\[\033[00m\]"
  PS2="> "
  }
  ______________________________________________________________________



  12.4.  A "Power User" Prompt

  I actually do use this prompt, but it results in noticeable delays in
  the appearance of the prompt on a single-user PII-400, so I wouldn't
  recommend using it on a multi-user P-100 or anything ...  Look at it
  for ideas, rather than as a practical prompt.



  ______________________________________________________________________

  #!/bin/bash
  #----------------------------------------------------------------------
  #       POWER USER PROMPT "pprom2"
  #----------------------------------------------------------------------
  #
  #   Created August 98, Last Modified 9 November 98 by Giles
  #
  #   Problem: when load is going down, it says "1.35down-.08", get rid
  #   of the negative

  function prompt_command
  {
  #   Create TotalMeg variable: sum of visible file sizes in current directory
  local TotalBytes=0
  for Bytes in $(ls -l | grep "^-" | cut -c30-41)
  do
      let TotalBytes=$TotalBytes+$Bytes
  done
  TotalMeg=$(echo -e "scale=3 \nx=$TotalBytes/1048576\n if (x<1) {print \"0\"} \n print x \nquit" | bc)

  #      This is used to calculate the differential in load values
  #      provided by the "uptime" command.  "uptime" gives load
  #      averages at 1, 5, and 15 minute marks.
  #
  local one=$(uptime | sed -e "s/.*load average: \(.*\...\), \(.*\...\), \(.*\...\)/\1/" -e "s/ //g")
  local five=$(uptime | sed -e "s/.*load average: \(.*\...\), \(.*\...\), \(.*\...\).*/\2/" -e "s/ //g")
  local diff1_5=$(echo -e "scale = scale ($one) \nx=$one - $five\n if (x>0) {print \"up\"} else {print \"down\"}\n print x \nquit \n" | bc)
  loaddiff="$(echo -n "${one}${diff1_5}")"

  #   Count visible files:
  let files=$(ls -l | grep "^-" | wc -l | tr -d " ")
  let hiddenfiles=$(ls -l -d .* | grep "^-" | wc -l | tr -d " ")
  let executables=$(ls -l | grep ^-..x | wc -l | tr -d " ")
  let directories=$(ls -l | grep "^d" | wc -l | tr -d " ")
  let hiddendirectories=$(ls -l -d .* | grep "^d" | wc -l | tr -d " ")-2
  let linktemp=$(ls -l | grep "^l" | wc -l | tr -d " ")
  if [ "$linktemp" -eq "0" ]
  then
      links=""
  else
      links=" ${linktemp}l"
  fi
  unset linktemp
  let devicetemp=$(ls -l | grep "^[bc]" | wc -l | tr -d " ")
  if [ "$devicetemp" -eq "0" ]
  then
      devices=""
  else
      devices=" ${devicetemp}bc"
  fi
  unset devicetemp

  }

  PROMPT_COMMAND=prompt_command

  function pprom2 {

  local        BLUE="\[\033[0;34m\]"
  local  LIGHT_GRAY="\[\033[0;37m\]"
  local LIGHT_GREEN="\[\033[1;32m\]"
  local  LIGHT_BLUE="\[\033[1;34m\]"
  local  LIGHT_CYAN="\[\033[1;36m\]"
  local      YELLOW="\[\033[1;33m\]"
  local       WHITE="\[\033[1;37m\]"
  local         RED="\[\033[0;31m\]"
  local   NO_COLOUR="\[\033[0m\]"

  case $TERM in
      xterm*)
          TITLEBAR='\[\033]0;\u@\h:\w\007\]'
          ;;
      *)
          TITLEBAR=""
          ;;
  esac

  PS1="$TITLEBAR\
  $BLUE[$RED\$(date +%H%M)$BLUE]\
  $BLUE[$RED\u@\h$BLUE]\
  $BLUE[\
  $LIGHT_GRAY\${files}.\${hiddenfiles}-\
  $LIGHT_GREEN\${executables}x \
  $LIGHT_GRAY(\${TotalMeg}Mb) \
  $LIGHT_BLUE\${directories}.\
  \${hiddendirectories}d\
  $LIGHT_CYAN\${links}\
  $YELLOW\${devices}\
  $BLUE]\
  $BLUE[${WHITE}\${loaddiff}$BLUE]\
  $BLUE[\
  $WHITE\$(ps ax | wc -l | sed -e \"s: ::g\")proc\
  $BLUE]\
  \n\
  $BLUE[$RED\$PWD$BLUE]\
  $WHITE\$\
  \
  $NO_COLOUR "
  PS2='> '
  PS4='+ '
  }
  ______________________________________________________________________



  12.5.  Prompt Depending on Connection Type

  Bradley M Alexander (storm@tux.org) had the excellent idea of
  reminding his users what kind of connection they were using to his
  machine(s), so he colour-codes prompts dependent on connection type.
  Here's the bashrc he supplied to me:



  ______________________________________________________________________

  # /etc/bashrc

  # System wide functions and aliases
  # Environment stuff goes in /etc/profile

  # For some unknown reason bash refuses to inherit
  # PS1 in some circumstances that I can't figure out.
  # Putting PS1 here ensures that it gets loaded every time.

  # Set up prompts. Color code them for logins. Red for root, white for
  # user logins, green for ssh sessions, cyan for telnet,
  # magenta with red "(ssh)" for ssh + su, magenta for telnet.
  THIS_TTY=tty`ps aux | grep $$ | grep bash | awk '{ print $7 }'`
  SESS_SRC=`who | grep $THIS_TTY | awk '{ print $6 }'`

  SSH_FLAG=0
  SSH_IP=`echo $SSH_CLIENT | awk '{ print $1 }'`
  if [ $SSH_IP ] ; then
    SSH_FLAG=1
  fi
  SSH2_IP=`echo $SSH2_CLIENT | awk '{ print $1 }'`
  if [ $SSH2_IP ] ; then
    SSH_FLAG=1
  fi
  if [ $SSH_FLAG -eq 1 ] ; then
    CONN=ssh
  elif [ -z $SESS_SRC ] ; then
    CONN=lcl
  elif [ $SESS_SRC = "(:0.0)" -o $SESS_SRC = "" ] ; then
    CONN=lcl
  else
    CONN=tel
  fi

  # Okay...Now who we be?
  if [ `/usr/bin/whoami` = "root" ] ; then
    USR=priv
  else
    USR=nopriv
  fi

  #Set some prompts...
  if [ $CONN = lcl -a $USR = nopriv ] ; then
    PS1="[\u \W]\\$ "
  elif [ $CONN = lcl -a $USR = priv ] ; then
    PS1="\[\033[01;31m\][\w]\\$\[\033[00m\] "
  elif [ $CONN = tel -a $USR = nopriv ] ; then
    PS1="\[\033[01;34m\][\u@\h \W]\\$\[\033[00m\] "
  elif [ $CONN = tel -a $USR = priv ] ; then
    PS1="\[\033[01;30;45m\][\u@\h \W]\\$\[\033[00m\] "
  elif [ $CONN = ssh -a $USR = nopriv ] ; then
    PS1="\[\033[01;32m\][\u@\h \W]\\$\[\033[00m\] "
  elif [ $CONN = ssh -a $USR = priv ] ; then
    PS1="\[\033[01;35m\][\u@\h \W]\\$\[\033[00m\] "
  fi

  # PS1="[\u@\h \W]\\$ "
  export PS1
  alias which="type -path"
  alias dir="ls -lF --color"
  alias dirs="ls -lFS --color"
  alias h=history
  ______________________________________________________________________

  12.6.  A Prompt the Width of Your Term

  A friend complained that he didn't like having a prompt that kept
  changing size because it had $PWD in it, so I wrote this prompt that
  adjusts its size to exactly the width of your term, with the working
  directory on the top line of two.



  ______________________________________________________________________

  #!/bin/bash

  #   termwide prompt
  #      by Giles - created 2 November 98
  #
  #   The idea here is to have the upper line of this two line prompt
  #   always be the width of your term.  Do this by calculating the
  #   width of the text elements, and putting in fill as appropriate
  #   or left-truncating $PWD.
  #

  function prompt_command {

  TERMWIDTH=${COLUMNS}

  #   Calculate the width of the prompt:

  hostnam=$(echo -n $HOSTNAME | sed -e "s/[\.].*//")
  #   "whoami" and "pwd" include a trailing newline
  usernam=$(whoami)
  let usersize=$(echo -n $usernam | wc -c | tr -d " ")
  newPWD="${PWD}"
  let pwdsize=$(echo -n ${newPWD} | wc -c | tr -d " ")
  #   Add all the accessories below ...
  let promptsize=$(echo -n "--(${usernam}@${hostnam})---(${PWD})--" \
                   | wc -c | tr -d " ")
  let fillsize=${TERMWIDTH}-${promptsize}
  fill=""
  while [ "$fillsize" -gt "0" ]
  do
     fill="${fill}-"
     let fillsize=${fillsize}-1
  done

  if [ "$fillsize" -lt "0" ]
  then
     let cut=3-${fillsize}
     newPWD="...$(echo -n $PWD | sed -e "s/\(^.\{$cut\}\)\(.*\)/\2/")"
  fi
  }

  PROMPT_COMMAND=prompt_command

  function termwide {

  local GRAY="\[\033[1;30m\]"
  local LIGHT_GRAY="\[\033[0;37m\]"
  local WHITE="\[\033[1;37m\]"
  local NO_COLOUR="\[\033[0m\]"

  local LIGHT_BLUE="\[\033[1;34m\]"
  local YELLOW="\[\033[1;33m\]"

  case $TERM in
      xterm*)
          TITLEBAR='\[\033]0;\u@\h:\w\007\]'
          ;;
      *)
          TITLEBAR=""
          ;;
  esac

  PS1="$TITLEBAR\
  $YELLOW-$LIGHT_BLUE-(\
  $YELLOW\${usernam}$LIGHT_BLUE@$YELLOW\${hostnam}\
  ${LIGHT_BLUE})-${YELLOW}-\${fill}${LIGHT_BLUE}-(\
  $YELLOW\${newPWD}\
  $LIGHT_BLUE)-$YELLOW-\
  \n\
  $YELLOW-$LIGHT_BLUE-(\
  $YELLOW\$(date +%H%M)$LIGHT_BLUE:$YELLOW\$(date \"+%a,%d %b %y\")\
  $LIGHT_BLUE:$WHITE\$$LIGHT_BLUE)-\
  $YELLOW-\
  $NO_COLOUR "

  PS2="$LIGHT_BLUE-$YELLOW-$YELLOW-$NO_COLOUR "

  }
  ______________________________________________________________________



  12.7.  The Elegant Useless Clock Prompt

  This is one of the more attractive (and useless) prompts I've made.
  Because many X terminal emulators don't implement cursor position save
  and restore, the alternative when putting a clock in the upper right
  corner is to anchor the cursor at the bottom of the terminal.  This
  builds on the idea of the "termwide" prompt above, drawing a line up
  the right side of the screen from the prompt to the clock.  A VGA font
  is required.


  Note: There is an odd substitution in here, that may not print
  properly being translated from SGML to other formats: I had to
  substitute the screen character for \304 - I would normally have just
  included the sequence "\304", but it was necessary to make this
  substitution in this case.



  ______________________________________________________________________

  #!/bin/bash

  #   This prompt requires a VGA font.  The prompt is anchored at the bottom
  #   of the terminal, fills the width of the terminal, and draws a line up
  #   the right side of the terminal to attach itself to a clock in the upper
  #   right corner of the terminal.

  function prompt_command {
  #   Calculate the width of the prompt:
  hostnam=$(echo -n $HOSTNAME | sed -e "s/[\.].*//")
  #   "whoami" and "pwd" include a trailing newline
  usernam=$(whoami)
  newPWD="${PWD}"
  #   Add all the accessories below ...
  let promptsize=$(echo -n "--(${usernam}@${hostnam})---(${PWD})-----" \
                   | wc -c | tr -d " ")
  #   Figure out how much to add between user@host and PWD (or how much to
  #   remove from PWD)
  let fillsize=${COLUMNS}-${promptsize}
  fill=""
  #   Make the filler if prompt isn't as wide as the terminal:
  while [ "$fillsize" -gt "0" ]
  do
     fill="${fill}"
     # The A with the umlaut over it (it will appear as a long dash if
     # you're using a VGA font) is \304, but I cut and pasted it in
     # because Bash will only do one substitution - which in this case is
     # putting $fill in the prompt.
     let fillsize=${fillsize}-1
  done
  #   Right-truncate PWD if the prompt is going to be wider than the terminal:
  if [ "$fillsize" -lt "0" ]
  then
     let cutt=3-${fillsize}
     newPWD="...$(echo -n $PWD | sed -e "s/\(^.\{$cutt\}\)\(.*\)/\2/")"
  fi
  #
  #   Create the clock and the bar that runs up the right side of the term
  #
  local LIGHT_BLUE="\033[1;34m"
  local     YELLOW="\033[1;33m"
  #   Position the cursor to print the clock:
  echo -en "\033[2;$((${COLUMNS}-9))H"
  echo -en "$LIGHT_BLUE($YELLOW$(date +%H%M)$LIGHT_BLUE)\304$YELLOW\304\304\277"
  local i=${LINES}
  echo -en "\033[2;${COLUMNS}H"
  #   Print vertical dashes down the side of the terminal:
  while [ $i -ge 4 ]
  do
     echo -en "\033[$(($i-1));${COLUMNS}H\263"
     let i=$i-1
  done

  let prompt_line=${LINES}-1
  #   This is needed because doing \${LINES} inside a Bash mathematical
  #   expression (ie. $(())) doesn't seem to work.
  }

  PROMPT_COMMAND=prompt_command

  function clock3 {
  local LIGHT_BLUE="\[\033[1;34m\]"
  local     YELLOW="\[\033[1;33m\]"
  local      WHITE="\[\033[1;37m\]"
  local LIGHT_GRAY="\[\033[0;37m\]"
  local  NO_COLOUR="\[\033[0m\]"

  case $TERM in
      xterm*)
          TITLEBAR='\[\033]0;\u@\h:\w\007\]'
          ;;
      *)
          TITLEBAR=""
          ;;
  esac

  PS1="$TITLEBAR\
  \[\033[\${prompt_line};0H\]
  $YELLOW\332$LIGHT_BLUE\304(\
  $YELLOW\${usernam}$LIGHT_BLUE@$YELLOW\${hostnam}\
  ${LIGHT_BLUE})\304${YELLOW}\304\${fill}${LIGHT_BLUE}\304(\
  $YELLOW\${newPWD}\
  $LIGHT_BLUE)\304$YELLOW\304\304\304\331\
  \n\
  $YELLOW\300$LIGHT_BLUE\304(\
  $YELLOW\$(date \"+%a,%d %b %y\")\
  $LIGHT_BLUE:$WHITE\$$LIGHT_BLUE)\304\
  $YELLOW\304\
  $LIGHT_GRAY "

  PS2="$LIGHT_BLUE\304$YELLOW\304$YELLOW\304$NO_COLOUR "

  }
  ______________________________________________________________________



  Linux Benchmarking HOWTO
  by Andr D. Balsa, andrewbalsa@usa.net  <mailto:andrew-
  balsa@usa.net>
  v0.12, 15 August 1997

  The Linux Benchmarking HOWTO discusses some issues associated with the
  benchmarking of Linux systems and presents a basic benchmarking
  toolkit, as well as an associated form, which enable one to produce
  significant benchmarking information in a couple of hours. Perhaps it
  will also help diminish the amount of useless articles in
  comp.os.linux.hardware...
  ______________________________________________________________________

  Table of Contents


  1. Introduction

     1.1 Why is benchmarking so important ?
     1.2 Invalid benchmarking considerations

  2. Benchmarking procedures and interpretation of results

     2.1 Understanding benchmarking choices
        2.1.1 Synthetic vs. applications benchmarks
        2.1.2 High-level vs. low-level benchmarks
     2.2 Standard benchmarks available for Linux
     2.3 Links and references

  3. The Linux Benchmarking Toolkit (LBT)

     3.1 Rationale
     3.2 Benchmark selection
     3.3 Test duration
     3.4 Comments
        3.4.1 Kernel 2.0.0 compilation:
        3.4.2 Whetstone:
        3.4.3 Xbench-0.2:
        3.4.4 UnixBench version 4.01:
        3.4.5 BYTE Magazine's BYTEmark benchmarks:
     3.5 Possible improvements
     3.6 LBT Report Form
     3.7 Network performance tests
     3.8 SMP tests

  4. Example run and results

  5. Pitfalls and caveats of benchmarking

     5.1 Comparing apples and oranges
     5.2 Incomplete information
     5.3 Proprietary hardware/software
     5.4 Relevance

  6. FAQ

  7. Copyright, acknowledgments and miscellaneous

     7.1 How this document was produced
     7.2 Copyright
     7.3 New versions of this document
     7.4 Feedback
     7.5 Acknowledgments
     7.6 Disclaimer
     7.7 Trademarks

  ______________________________________________________________________

  1.  Introduction


  "What we cannot speak about we must pass over in silence."

       Ludwig Wittgenstein (1889-1951), Austrian philosopher


  Benchmarking means measuring the speed with which a computer system
  will execute a computing task, in a way that will allow comparison
  between different hard/software combinations. It does not involve
  user-friendliness, aesthetic or ergonomic considerations or any other
  subjective judgment.

  Benchmarking is a tedious, repetitive task, and takes attention to
  details. Very often the results are not what one would expect, and
  subject to interpretation (which actually may be the most important
  part of a benchmarking procedure).

  Finally, benchmarking deals with facts and figures, not opinion or
  approximation.

  1.1.  Why is benchmarking so important ?


  Apart from the reasons pointed out in the BogoMips Mini-HOWTO (section
  7, paragraph 2), one occasionally is confronted with a limited budget
  and/or minimum performance requirements while putting together a Linux
  box. In other words, when confronted with the following questions:

  o  How do I maximize performance within a given budget ?

  o  How do I minimize costs for a required minimum performance level ?

  o  How do I obtain the best performance/cost ratio (within a given
     budget or given performance requirements)?

  one will have to examine, compare and/or produce benchmarks.
  Minimizing costs with no performance requirements usually involves
  putting together a machine with leftover parts (that old 386SX-16 box
  lying around in the garage will do fine) and does not require
  benchmarks, and maximizing performance with no cost ceiling is not a
  realistic situation (unless one is willing to put a Cray box in
  his/her living room - the leather-covered power supplies around it
  look nice, don't they ?).

  Benchmarking per se is senseless, a waste of time and money; it is
  only meaningful as part of a decision process, i.e. if one has to make
  a choice between two or more alternatives.

  Usually another parameter in the decision process is cost, but it
  could be availability, service, reliability, strategic considerations
  or any other rational, measurable characteristic of a computer system.
  When comparing the performance of different Linux kernel versions, for
  example, stability is almost always more important than speed.

  1.2.  Invalid benchmarking considerations


  Very often read in newsgroups and mailing lists, unfortunately:

  1. Reputation of manufacturer (unmeasurable and meaningless).


  2. Market share of manufacturer (meaningless and irrelevant).

  3. Irrational parameters (for example, superstition or prejudice:
     would you buy a processor labeled 131313ZAP and painted pink ?)

  4. Perceived value (meaningless, unmeasurable and irrational).

  5. Amount of marketing hype: this one is the worst, I guess. I
     personally am fed up with the "XXX inside" or "kkkkkws compatible"
     logos (now the "aaaaaPowered" has joined the band - what next ?).
     IMHO, the billions of dollars spent on such campaigns would be
     better used by research teams on the design of new, faster,
     (cheaper :-) bug-free processors. No amount of marketing hype will
     remove a floating-point bug in the FPU of the brand-new processor
     you just plugged in your motherboard, but an exchange against a
     redesigned processor will.

  6. "You get what you pay for" opinions are just that: opinions. Give
     me the facts, please.

  2.  Benchmarking procedures and interpretation of results


  A few semi-obvious recommendations:

  1. First and foremost, identify your benchmarking goals. What is it
     you are exactly trying to benchmark ? In what way will the
     benchmarking process help later in your decision making ? How much
     time and resources are you willing to put into your benchmarking
     effort ?

  2. Use standard tools. Use a current, stable kernel version, standard,
     current gcc and libc and a standard benchmark. In short, use the
     LBT (see below).

  3. Give a complete description of your setup (see the LBT report form
     below).

  4. Try to isolate a single variable. Comparative benchmarking is more
     informative than "absolute" benchmarking. I cannot stress this
     enough.

  5. Verify your results. Run your benchmarks a few times and verify the
     variations in your results, if any. Unexplained variations will
     invalidate your results.

  6. If you think your benchmarking effort produced meaningful
     information, share it with the Linux community in a precise and
     concise way.

  7. Please forget about BogoMips. I promise myself I shall someday
     implement a very fast ASIC with the BogoMips loop wired in. Then we
     shall see what we shall see !

  2.1.  Understanding benchmarking choices


  2.1.1.  Synthetic vs. applications benchmarks


  Before spending any amount of time on benchmarking chores, a basic
  choice must be made between "synthetic" benchmarks and "applications"
  benchmarks.

  Synthetic benchmarks are specifically designed to measure the
  performance of individual components of a computer system, usually by
  exercising the chosen component to its maximum capacity. An example of
  a well-known synthetic benchmark is the Whetstone suite, originally
  programmed in 1972 by Harold Curnow in FORTRAN (or was that ALGOL ?)
  and still in widespread use nowadays. The Whestone suite will measure
  the floating-point performance of a CPU.

  The main critic that can be made to synthetic benchmarks is that they
  do not represent a computer system's performance in real-life
  situations. Take for example the Whetstone suite: the main loop is
  very short and will easily fit in the primary cache of a CPU, keeping
  the FPU pipeline constantly filled and so exercising the FPU to its
  maximum speed. We cannot really criticize the Whetstone suite if we
  remember it was programmed 25 years ago (its design dates even earlier
  than that !), but we must make sure we interpret its results with
  care, when it comes to benchmarking modern microprocessors.

  Another very important point to note about synthetic benchmarks is
  that, ideally, they should tell us something about a specific aspect
  of the system being tested, independently of all other aspects: a
  synthetic benchmark for Ethernet card I/O throughput should result in
  the same or similar figures whether it is run on a 386SX-16 with 4
  MBytes of RAM or a Pentium 200 MMX with 64 MBytes of RAM. Otherwise,
  the test will be measuring the overall performance of the
  CPU/Motherboard/Bus/Ethernet card/Memory subsystem/DMA combination:
  not very useful since the variation in CPU will cause a greater impact
  than the change in Ethernet network card (this of course assumes we
  are using the same kernel/driver combination, which could cause an
  even greater variation)!

  Finally, a very common mistake is to average various synthetic
  benchmarks and claim that such an average is a good representation of
  real-life performance for any given system.

  Here is a comment on FPU benchmarks quoted with permission from the
  Cyrix Corp. Web site:

       "A Floating Point Unit (FPU) accelerates software designed
       to use floating point mathematics : typically CAD programs,
       spreadsheets, 3D games and design applications. However,
       today's most popular PC applications make use of both float-
       ing point and integer instructions. As a result, Cyrix chose
       to emphasize "parallelism" in the design of the 6x86 proces-
       sor to speed up software that intermixes these two instruc-
       tion types.



       The x86 floating point exception model allows integer
       instructions to issue and complete while a floating point
       instruction is executing. In contrast, a second floating
       point instruction cannot begin execution while a previous
       floating point instruction is executing. To remove the per-
       formance limitation created by the floating point exception
       model, the 6x86 can speculatively issue up to four floating
       point instructions to the on-chip FPU while continuing to
       issue and execute integer instructions. As an example, in a
       code sequence of two floating point instructions (FLTs) fol-
       lowed by six integer instructions (INTs) followed by two
       FLTs, the 6x86 processor can issue all ten instructions to
       the appropriate execution units prior to completion of the
       first FLT. If none of the instructions fault (the typical
       case), execution continues with both the integer and float-
       ing point units completing instructions in parallel. If one
       of the FLTs faults (the atypical case), the speculative exe-
       cution capability of the 6x86 allows the processor state to
       be restored in such a way that it is compatible with the x86
  floating point exception model.



       Examination of benchmark tests reveals that synthetic float-
       ing point benchmarks use a pure floating point-only code
       stream not found in real-world applications. This type of
       benchmark does not take advantage of the speculative execu-
       tion capability of the 6x86 processor. Cyrix believes that
       non-synthetic benchmarks based on real-world applications
       better reflect the actual performance users will achieve.
       Real-world applications contain intermixed integer and
       floating point instructions and therefore benefit from the
       6x86 speculative execution capability."


  So, the recent trend in benchmarking is to choose common applications
  and use them to test the performance of complete computer systems. For
  example, SPEC, the non-profit corporation that designed the well-known
  SPECINT and SPECFP synthetic benchmark suites, has launched a project
  for a new applications benchmark suite. But then again, it is very
  unlikely that such commercial benchmarks will ever include any Linux
  code.

  Summarizing, synthetic benchmarks are valid as long as you understand
  their purposes and limitations. Applications benchmarks will better
  reflect a computer system's performance, but none are available for
  Linux.

  2.1.2.  High-level vs. low-level benchmarks


  Low-level benchmarks will directly measure the performance of the
  hardware: CPU clock, DRAM and cache SRAM cycle times, hard disk
  average access time, latency, track-to-track stepping time, etc...
  This can be useful in case you bought a system and are wondering what
  components it was built with, but a better way to check these figures
  would be to open the case, list whatever part numbers you can find and
  somehow obtain the data sheet for each part (usually on the Web).

  Another use for low-level benchmarks is to check that a kernel driver
  was correctly configured for a specific piece of hardware: if you have
  the data sheet for the component, you can compare the results of the
  low-level benchmarks to the theoretical, printed specs.

  High-level benchmarks are more concerned with the performance of the
  hardware/driver/OS combination for a specific aspect of a
  microcomputer system, for example file I/O performance, or even for a
  specific hardware/driver/OS/application performance, e.g. an Apache
  benchmark on different microcomputer systems.

  Of course, all low-level benchmarks are synthetic. High-level
  benchmarks may be synthetic or applications benchmarks.

  2.2.  Standard benchmarks available for Linux


  IMHO a simple test that anyone can do while upgrading any component in
  his/her Linux box is to launch a kernel compile before and after the
  hard/software upgrade and compare compilation times. If all other
  conditions are kept equal then the test is valid as a measure of
  compilation performance and one can be confident to say that:

       "Changing A to B led to an improvement of x % in the compile
       time of the Linux kernel under such and such conditions".

  No more, no less !

  Since kernel compilation is a very usual task under Linux, and since
  it exercises most functions that get exercised by normal benchmarks
  (except floating-point performance), it constitutes a rather good
  individual test. In most cases, however, results from such a test
  cannot be reproduced by other Linux users because of variations in
  hard/software configurations and so this kind of test cannot be used
  as a "yardstick" to compare dissimilar systems (unless we all agree on
  a standard kernel to compile - see below).

  Unfortunately, there are no Linux-specific benchmarking tools, except
  perhaps the Byte Linux Benchmarks which are a slightly modified
  version of the Byte Unix Benchmarks dating back from May 1991 (Linux
  mods by Jon Tombs, original authors Ben Smith, Rick Grehan and Tom
  Yager).

  There is a central Web site for the Byte Linux Benchmarks.

  An improved, updated version of the Byte Unix Benchmarks was put
  together by David C. Niemi. It is called UnixBench 4.01 to avoid
  confusion with earlier versions. Here is what David wrote about his
  mods:

       "The original and slightly modified BYTE Unix benchmarks are
       broken in quite a number of ways which make them an unusu-
       ally unreliable indicator of system performance. I inten-
       tionally made my "index" values look a lot different to
       avoid confusion with the old benchmarks."


  David has setup a majordomo mailing list for discussion of
  benchmarking on Linux and competing OSs. Join with "subscribe bench"
  sent in the body of a message to majordomo@wauug.erols.com
  <mailto:majordomo@wauug.erols.com>. The Washington Area Unix User
  Group is also in the process of setting up a  Web site for Linux
  benchmarks.

  Also recently, Uwe F. Mayer, mayer@math.vanderbilt.edu
  <mailto:mayer@math.vanderbilt.edu>ported the BYTE Bytemark suite to
  Linux. This is a modern suite carefully put together by Rick Grehan at
  BYTE Magazine to test the CPU, FPU and memory system performance of
  modern microcomputer systems (these are strictly processor-performance
  oriented benchmarks, no I/O or system performance is taken into
  account).

  Uwe has also put together a Web site with a database of test results
  for his version of the Linux BYTEmark benchmarks.

  While searching for synthetic benchmarks for Linux, you will notice
  that sunsite.unc.edu carries few benchmarking tools. To test the
  relative speed of X servers and graphics cards, the xbench-0.2 suite
  by Claus Gittinger is available from sunsite.unc.edu, ftp.x.org and
  other sites. Xfree86.org refuses (wisely) to carry or recommend any
  benchmarks.

  The XFree86-benchmarks Survey is a Web site with a database of x-bench
  results.

  For pure disk I/O throughput, the hdparm program (included with most
  distributions, otherwise available from sunsite.unc.edu) will measure
  transfer rates if called with the -t and -T switches.

  There are many other tools freely available on the Internet to test
  various performance aspects of your Linux box.

  2.3.  Links and references


  The comp.benchmarks.faq by Dave Sill is the standard reference for
  benchmarking. It is not Linux specific, but recommended reading for
  anybody serious about benchmarking. It is available from a number of
  FTP and web sites and lists 56 different benchmarks, with links to FTP
  or Web sites that carry them. Some of the benchmarks listed are
  commercial (SPEC for example), though.

  I will not go through each one of the benchmarks mentionned in the
  comp.benchmarks.faq, but there is at least one low-level suite which I
  would like to comment on: the  lmbench suite, by Larry McVoy. Quoting
  David C. Niemi:

       "Linus and David Miller use this a lot because it does some
       useful low-level measurements and can also measure network
       throughput and latency if you have 2 boxes to test with. But
       it does not attempt to come up with anything like an overall
       "figure of merit"..."


  A rather complete FTP site for freely available benchmarks was put
  together by Alfred Aburto. The Whetstone suite used in the LBT can be
  found at this site.

  There is a multipart FAQ by Eugene Miya that gets posted regularly to
  comp.benchmarks; it is an excellent reference.

  3.  The Linux Benchmarking Toolkit (LBT)


  I will propose a basic benchmarking toolkit for Linux. This is a
  preliminary version of a comprehensive Linux Benchmarking Toolkit, to
  be expanded and improved. Take it for what it's worth, i.e. as a
  proposal. If you don't think it is a valid test suite, feel free to
  email me your critics and I will be glad to make the changes and
  improve it if I can. Before getting into an argument, however, read
  this HOWTO and the mentionned references: informed criticism is
  welcomed, empty criticism is not.

  3.1.  Rationale


  This is just common sense:

  1. It should not take a whole day to run. When it comes to comparative
     benchmarking (various runs), nobody wants to spend days trying to
     figure out the fastest setup for a given system. Ideally, the
     entire benchmark set should take about 15 minutes to complete on an
     average machine.

  2. All source code for the software used must be freely available on
     the Net, for obvious reasons.

  3. Benchmarks should provide simple figures reflecting the measured
     performance.

  4. There should be a mix of synthetic benchmarks and application
     benchmarks (with separate results, of course).

  5. Each synthetic benchmarks should exercise a particular subsystem to
     its maximum capacity.

  6. Results of synthetic benchmarks should not be averaged into a
     single figure of merit (that defeats the whole idea behind
     synthetic benchmarks, with considerable loss of information).

  7. Applications benchmarks should consist of commonly executed tasks
     on Linux systems.

  3.2.  Benchmark selection


  I have selected five different benchmark suites, trying as much as
  possible to avoid overlap in the tests:

  1. Kernel 2.0.0 (default configuration) compilation using gcc.

  2. Whetstone version 10/03/97 (latest version by Roy Longbottom).

  3. xbench-0.2 (with fast execution parameters).

  4. UnixBench benchmarks version 4.01 (partial results).

  5. BYTE Magazine's BYTEmark benchmarks beta release 2 (partial
     results).

  For tests 4 and 5, "(partial results)" means that not all results
  produced by these benchmarks are considered.

  3.3.  Test duration


  1. Kernel 2.0.0 compilation: 5 - 30 minutes, depending on the real
     performance of your system.

  2. Whetstone: 100 seconds.

  3. Xbench-0.2: < 1 hour.

  4. UnixBench benchmarks version 4.01: approx. 15 minutes.

  5. BYTE Magazine's BYTEmark benchmarks: approx. 10 minutes.

  3.4.  Comments


  3.4.1.  Kernel 2.0.0 compilation:


  o  What: it is the only application benchmark in the LBT.

  o  The code is widely available (i.e. I finally found some use for my
     old Linux CD-ROMs).

  o  Most linuxers recompile the kernel quite often, so it is a
     significant measure of overall performance.

  o  The kernel is large and gcc uses a large chunk of memory:
     attenuates L2 cache size bias with small tests.

  o  It does frequent I/O to disk.

  o  Test procedure: get a pristine 2.0.0 source, compile with default
     options (make config, press Enter repeatedly). The reported time
     should be the time spent on compilation i.e. after you type make
     zImage, not including make dep, make clean. Note that the default
     target architecture for the kernel is the i386, so if compiled on
     another architecture, gcc too should be set to cross-compile, with
     i386 as the target architecture.

  o  Results: compilation time in minutes and seconds (please don't
     report fractions of seconds).

  3.4.2.  Whetstone:


  o  What: measures pure floating point performance with a short, tight
     loop. The source (in C) is quite readable and it is very easy to
     see which floating-point operations are involved.

  o  Shortest test in the LBT :-).

  o  It's an "Old Classic" test: comparable figures are available, its
     flaws and shortcomings are well known.

  o  Test procedure: the newest C source should be obtained from
     Aburto's site. Compile and run in double precision mode. Specify
     gcc and -O2 as precompiler and precompiler options, and define
     POSIX 1 to specify machine type.

  o  Results: a floating-point performance figure in MWIPS.

  3.4.3.  Xbench-0.2:


  o  What: measures X server performance.

  o  The xStones measure provided by xbench is a weighted average of
     several tests indexed to an old Sun station with a single-bit-depth
     display. Hmmm... it is questionable as a test of modern X servers,
     but it's still the best tool I have found.

  o  Test procedure: compile with -O2. We specify a few options for a
     shorter run: ./xbench -timegoal 3 >
     results/name_of_your_linux_box.out. To get the xStones rating, we
     must run an awk script; the simplest way is to type make
     summary.ms. Check the summary.ms file: the xStone rating for your
     system is in the last column of the line with your machine name
     specified during the test.

  o  Results: an X performance figure in xStones.

  o  Note: this test, as it stands, is outdated. It should be re-coded.

  3.4.4.  UnixBench version 4.01:


  o  What: measures overall Unix performance. This test will exercice
     the file I/O and kernel multitasking performance.

  o  I have discarded all arithmetic test results, keeping only the
     system-related test results.

  o  Test procedure: make with -O2. Execute with ./Run -1 (run each test
     once). You will find the results in the ./results/report file.
     Calculate the geometric mean of the EXECL THROUGHPUT, FILECOPY 1,
     2, 3, PIPE THROUGHPUT, PIPE-BASED CONTEXT SWITCHING, PROCESS
     CREATION, SHELL SCRIPTS and SYSTEM CALL OVERHEAD indexes.

  o  Results: a system index.

  3.4.5.  BYTE Magazine's BYTEmark benchmarks:


  o  What: provides a good measure of CPU performance. Here is an
     excerpt from the documentation: "These benchmarks are meant to
     expose the theoretical upper limit of the CPU, FPU, and memory
     architecture of a system. They cannot measure video, disk, or
     network throughput (those are the domains of a different set of
     benchmarks). You should, therefore, use the results of these tests
     as part, not all, of any evaluation of a system."

  o  I have discarded the FPU test results since the Whetstone test is
     just as representative of FPU performance.

  o  I have split the integer tests in two groups: those more
     representative of memory-cache-CPU performance and the CPU integer
     tests.

  o  Test procedure: make with -O2. Run the test with ./nbench >
     myresults.dat or similar. Then, from myresults.dat, calculate
     geometric mean of STRING SORT, ASSIGNMENT and BITFIELD test
     indexes; this is the memory index; calculate the geometric mean of
     NUMERIC SORT, IDEA, HUFFMAN and FP EMULATION test indexes; this is
     the integer index.

  o  Results: a memory index and an integer index calculated as
     explained above.

  3.5.  Possible improvements


  The ideal benchmark suite would run in a few minutes, with synthetic
  benchmarks testing every subsystem separately and applications
  benchmarks providing results for different applications. It would also
  automatically generate a complete report and eventually email the
  report to a central database on the Web.

  We are not really interested in portability here, but it should at
  least run on all recent (> 2.0.0) versions and flavours (i386, Alpha,
  Sparc...) of Linux.

  If anybody has any idea about benchmarking network performance in a
  simple, easy and reliable way, with a short (less than 30 minutes to
  setup and run) test, please contact me.

  3.6.  LBT Report Form


  Besides the tests, the benchmarking procedure would not be complete
  without a form describing the setup, so here it is (following the
  guidelines from comp.benchmarks.faq):

  ______________________________________________________________________
  LINUX BENCHMARKING TOOLKIT REPORT FORM
  ______________________________________________________________________



  ______________________________________________________________________
  CPU
  ==
  Vendor:
  Model:
  Core clock:
  Motherboard vendor:
  Mbd. model:
  Mbd. chipset:
  Bus type:
  Bus clock:
  Cache total:
  Cache type/speed:
  SMP (number of processors):
  ______________________________________________________________________



  ______________________________________________________________________
  RAM
  ====
  Total:
  Type:
  Speed:
  ______________________________________________________________________



  ______________________________________________________________________
  Disk
  ====
  Vendor:
  Model:
  Size:
  Interface:
  Driver/Settings:
  ______________________________________________________________________



  ______________________________________________________________________
  Video board
  ===========
  Vendor:
  Model:
  Bus:
  Video RAM type:
  Video RAM total:
  X server vendor:
  X server version:
  X server chipset choice:
  Resolution/vert. refresh rate:
  Color depth:
  ______________________________________________________________________



  ______________________________________________________________________
  Kernel
  =====
  Version:
  Swap size:
  ______________________________________________________________________



  ______________________________________________________________________
  gcc
  ===
  Version:
  Options:
  libc version:
  ______________________________________________________________________



  ______________________________________________________________________
  Test notes
  ==========
  ______________________________________________________________________



  ______________________________________________________________________
  RESULTS
  ========
  Linux kernel 2.0.0 Compilation Time: (minutes and seconds)
  Whetstones: results are in MWIPS.
  Xbench: results are in xstones.
  Unixbench Benchmarks 4.01 system INDEX:
  BYTEmark integer INDEX:
  BYTEmark memory INDEX:
  ______________________________________________________________________



  ______________________________________________________________________
  Comments*
  =========
  * This field is included for possible interpretations of the results, and as
  such, it is optional. It could be the most significant part of your report,
  though, specially if you are doing comparative benchmarking.
  ______________________________________________________________________



  3.7.  Network performance tests


  Testing network performance is a challenging task since it involves at
  least two machines, a server and a client machine, hence twice the
  time to setup and many more variables to control, etc... On an
  ethernet network, I guess your best bet would be the ttcp package. (to
  be expanded)

  3.8.  SMP tests


  SMP tests are another challenge, and any benchmark specifically
  designed for SMP testing will have a hard time proving itself valid in
  real-life settings, since algorithms that can take advantage of SMP
  are hard to come by. It seems later versions of the Linux kernel (>
  2.1.30 or around that) will do "fine-grained" multiprocessing, but I
  have no more information than that for the moment.

  According to David Niemi, " ... shell8 [part of the Unixbench 4.01
  benchmaks]does a good job at comparing similar hardware/OS in SMP and
  UP modes."



  4.  Example run and results


  The LBT was run on my home machine, a Pentium-class Linux box that I
  put together myself and that I used to write this HOWTO. Here is the
  LBT Report Form for this system:

  LINUX BENCHMARKING TOOLKIT REPORT FORM



  CPU



  ==



  Vendor: Cyrix/IBM



  Model: 6x86L P166+



  Core clock: 133 MHz



  Motherboard vendor: Elite Computer Systems (ECS)



  Mbd. model: P5VX-Be



  Mbd. chipset: Intel VX



  Bus type: PCI



  Bus clock: 33 MHz



  Cache total: 256 KB



  Cache type/speed: Pipeline burst 6 ns



  SMP (number of processors): 1



  RAM


  ====



  Total: 32 MB



  Type: EDO SIMMs



  Speed: 60 ns



  Disk



  ====



  Vendor: IBM



  Model: IBM-DAQA-33240



  Size: 3.2 GB



  Interface: EIDE



  Driver/Settings: Bus Master DMA mode 2



  Video board



  ===========



  Vendor: Generic S3



  Model: Trio64-V2



  Bus: PCI



  Video RAM type: EDO DRAM

  Video RAM total: 2 MB



  X server vendor: XFree86



  X server version: 3.3



  X server chipset choice: S3 accelerated



  Resolution/vert. refresh rate: 1152x864 @ 70 Hz



  Color depth: 16 bits



  Kernel



  =====



  Version: 2.0.29



  Swap size: 64 MB



  gcc



  ===



  Version: 2.7.2.1



  Options: -O2



  libc version: 5.4.23



  Test notes



  ==========

  Very light load. The above tests were run with some of the special
  Cyrix/IBM 6x86 features enabled with the setx86 program: fast ADS,
  fast IORT, Enable DTE, fast LOOP, fast Lin. VidMem.



  RESULTS



  ========



  Linux kernel 2.0.0 Compilation Time: 7m12s



  Whetstones: 38.169 MWIPS.



  Xbench: 97243 xStones.



  BYTE Unix Benchmarks 4.01 system INDEX: 58.43



  BYTEmark integer INDEX: 1.50



  BYTEmark memory INDEX: 2.50



  Comments



  =========



  This is a very stable system with homogeneous performance, ideal
  for home use and/or Linux development. I will report results
  with a 6x86MX processor as soon as I can get my hands on one!



  5.  Pitfalls and caveats of benchmarking


  After putting together this HOWTO I began to understand why the words
  "pitfalls" and "caveats" are so often associated with benchmarking...

  5.1.  Comparing apples and oranges


  Or should I say Apples and PCs ? This is so obvious and such an old
  dispute that I won't go into any details. I doubt the time it takes to
  load Word on a Mac compared to an average Pentium is a real measure of
  anything. Likewise booting Linux and Windows NT, etc... Try as much as
  possible to compare identical machines with a single modification.
  5.2.  Incomplete information


  A single example will illustrate this very common mistake. One often
  reads in comp.os.linux.hardware the following or similar statement: "I
  just plugged in processor XYZ running at nnn MHz and now compiling the
  linux kernel only takes i minutes" (adjust XYZ, nnn and i as
  required). This is irritating, because no other information is given,
  i.e. we don't even know the amount of RAM, size of swap, other tasks
  running simultaneously, kernel version, modules selected, hard disk
  type, gcc version, etc... I recommend you use the LBT Report Form,
  which at least provides a standard information framework.

  5.3.  Proprietary hardware/software


  A well-known processor manufacturer once published results of
  benchmarks produced by a special, customized version of gcc. Ethical
  considerations apart, those results were meaningless, since 100% of
  the Linux community would go on using the standard version of gcc. The
  same goes for proprietary hardware. Benchmarking is much more useful
  when it deals with off-the-shelf hardware and free (in the GNU/GPL
  sense) software.

  5.4.  Relevance


  We are talking Linux, right ? So we should forget about benchmarks
  produced on other operating systems (this is a special case of the
  "Comparing apples and oranges" pitfall above). Also, if one is going
  to benchmark Web server performance, do not quote FPU performance and
  other irrelevant information. In such cases, less is more. Also, you
  do not need to mention the age of your cat, your mood while
  benchmarking, etc..

  6.  FAQ


     Q1.
        Is there any single figure of merit for Linux systems ?

     A: No, thankfully nobody has yet come up with a Lhinuxstone (tm)
        measurement. And if there was one, it would not make much sense:
        Linux systems are used for many different tasks, from heavily
        loaded Web servers to graphics workstations for individual use.
        No single figure of merit can describe the performance of a
        Linux system under such different situations.

     Q2.
        Then, how about a dozen figures summarizing the performance of
        diverse Linux systems ?

     A: That would be the ideal situation. I would like to see that come
        true. Anybody volunteers for a Linux Benchmarking Project ? With
        a Web site and an on-line, complete, well-designed reports
        database ?

     Q3.
        ... BogoMips ... ?

     A: BogoMips has nothing to do with the performance of your system.
        Check the BogoMips Mini-HOWTO.

     Q4.
        What is the "best" benchmark for Linux ?

     A: It all depends on which performance aspect of a Linux system one
        wants to measure. There are different benchmarks to measure the
        network (Ethernet sustained transfer rates), file server (NFS),
        disk I/O, FPU, integer, graphics, 3D, processor-memory
        bandwidth, CAD performance, transaction time, SQL performance,
        Web server performance, real-time performance, CD-ROM
        performance, Quake performance (!), etc ... AFAIK no bechmark
        suite exists for Linux that supports all these tests.

     Q5.
        What is the fastest processor under Linux ?

     A: Fastest at what task ? If one is heavily number-crunching
        oriented, a very high clock rate Alpha (600 MHz and going)
        should be faster than anything else, since Alphas have been
        designed for that kind of performance. If, on the other hand,
        one wants to put together a very fast news server, it is
        probable that the choice of a fast hard disk subsystem and lots
        of RAM will result in higher performance improvements than a
        change of processor, for the same amount of $.

     Q6.
        Let me rephrase the last question, then: is there a processor
        that is fastest for general purpose applications ?

     A: This is a tricky question but it takes a very simple answer: NO.
        One can always design a faster system even for general purpose
        applications, independent of the processor. Usually, all other
        things being equal, higher clock rates will result in higher
        performance systems (and more headaches too). Taking out an old
        100 MHz Pentium from an (usually not) upgradable motherboard,
        and plugging in the 200 MHz version, one should feel the extra
        "hummph". Of course, with only 16 MBytes of RAM, the same
        investment would have been more wisely spent on extra SIMMs...

     Q7.
        So clock rates influence the performance of a system ?

     A: For most tasks except for NOP empty loops (BTW these get removed
        by modern optimizing compilers), an increase in clock rate will
        not give you a linear increase in performance. Very small
        processor intensive programs that will fit entirely in the
        primary cache inside the processor (the L1 cache, usually 8 or
        16 K) will have a performance increase equivalent to the clock
        rate increase, but most "true" programs are much larger than
        that, have loops that do not fit in the L1 cache, share the L2
        (external) cache with other processes, depend on external
        components and will give much smaller performance increases.
        This is because the L1 cache runs at the same clock rate as the
        processor, whereas most L2 caches and all other subsystems
        (DRAM, for example) will run asynchronously at lower clock
        rates.

     Q8.
        OK, then, one last question on that matter: which is the
        processor with the best price/performance ratio for general
        purpose Linux use ?

     A: Defining "general purpose Linux use" in not an easy thing ! For
        any particular application, there is always a processor with THE
        BEST price/performance ratio at any given time, but it changes
        rather frequently as manufacturers release new processors, so
        answering Processor XYZ running at n MHz would be a snapshot
        answer. However, the price of the processor is insignificant
        when compared to the price of the whole system one will be
        putting together. So, really, the question should be how can one
        maximize the price/performance ratio for a given system ? And
        the answer to that question depends heavily on the minimum
        performance requirements and/or maximum cost established for the
        configuration being considered. Sometimes, off-the-shelf
        hardware will not meet minimum performance requirements and
        expensive RISC systems will be the only alternative. For home
        use, I recommend a balanced, homogeneous system for overall
        performance (now go figure what I mean by balanced and
        homogeneous :-); the choice of a processor is an important
        decision , but no more than choosing hard disk type and
        capacity, amount of RAM, video card, etc...

     Q9.
        What is a "significant" increase in performance ?

     A: I would say that anything under 1% is not significant (could be
        described as "marginal"). We, humans, will hardly perceive the
        difference between two systems with a 5 % difference in response
        time. Of course some hard-core benchmarkers are not humans and
        will tell you that, when comparing systems with 65.9 and 66.5
        performance indexes, the later is "definitely faster".

     Q10.
        How do I obtain "significant" increases in performance at the
        lowest cost ?

     A: Since most source code is available for Linux, careful
        examination and algorithmic redesign of key subroutines could
        yield order-of-magnitude increases in performance in some cases.
        If one is dealing with a commercial project and does not wish to
        delve deeply in C source code a Linux consultant should be
        called in. See the Consultants-HOWTO.


  7.  Copyright, acknowledgments and miscellaneous


  7.1.  How this document was produced


  The first step was reading section 4 "Writing and submitting a HOWTO"
  of the HOWTO Index by Tim Bynum.

  I knew absolutely nothing about SGML or LaTeX, but was tempted to use
  an automated documentation generation package after reading the
  various comments about SGML-Tools. However, inserting tags manually in
  a document reminds me of the days I hand-assembled a 512 byte monitor
  program for a now defunct 8-bit microprocessor, so I got hold of the
  LyX sources, compiled it, and used its LinuxDoc mode. Highly
  recommended combination: LyX and SGML-Tools.

  7.2.  Copyright


  The Linux Benchmarking HOWTO is copyright (C) 1997 by Andr D. Balsa.
  Linux HOWTO documents may be reproduced and distributed in whole or in
  part, in any medium physical or electronic, as long as this copyright
  notice is retained on all copies. Commercial redistribution is allowed
  and encouraged; however, the author would like to be notified of any
  such distributions.

  All translations, derivative works, or aggregate works incorporating
  any Linux HOWTO documents must be covered under this copyright notice.
  That is, you may not produce a derivative work from a HOWTO and impose
  additional restrictions on its distribution. Exceptions to these rules
  may be granted under certain conditions; please contact the Linux
  HOWTO coordinator at the address given below.

  In short, we wish to promote dissemination of this information through
  as many channels as possible. However, we do wish to retain copyright
  on the HOWTO documents, and would like to be notified of any plans to
  redistribute the HOWTOs.

  If you have questions, please contact Tim Bynum, the Linux HOWTO
  coordinator, at linux-howto@sunsite.unc.edu via email.

  7.3.  New versions of this document


  New versions of the Linux Benchmarking-HOWTO will be placed on
  sunsite.unc.edu and mirror sites. There are other formats, such as a
  Postscript and dvi version in the other-formats directory. The Linux
  Benchmarking-HOWTO is also available for WWW clients such as Grail, a
  Web browser written in Python. It will also be posted regularly to
  comp.os.linux.answers.

  7.4.  Feedback


  Suggestions, corrections, additions wanted. Contributors wanted and
  acknowledged. Flames not wanted.

  I can always be reached at andrewbalsa@usa.net.

  7.5.  Acknowledgments


  David Niemi, the author of the Unixbench suite, has proved to be an
  endless source of information and (valid) criticism.

  I also want to thank Greg Hankins one of the main contributors to the
  SGML-tools package, Linus Torvalds and the entire Linux community.
  This HOWTO is my way of giving back.

  7.6.  Disclaimer


  Your mileage may, and will, vary. Be aware that benchmarking is a
  touchy subject and a great time-and-energy consuming activity.

  7.7.  Trademarks


  Pentium and Windows NT are trademarks of Intel and Microsoft
  Corporations respectively.

  BYTE and BYTEmark are trademarks of McGraw-Hill, Inc.

  Cyrix and 6x86 are trademarks of Cyrix Corporation.

  Linux is not a trademark, hopefully never will be.



  Beowulf HOWTO
  Jacek Radajewski and Douglas Eadline
  v1.1.1, 22 November 1998

  This document introduces the Beowulf Supercomputer architecture and
  provides background information on parallel programming, including
  links to other more specific documents, and web pages.
  ______________________________________________________________________

  Table of Contents



  1. Preamble

     1.1 Disclaimer
     1.2 Copyright
     1.3 About this HOWTO
     1.4 About the authors
     1.5 Acknowledgements

  2. Introduction

     2.1 Who should read this HOWTO ?
     2.2 What is a Beowulf ?
     2.3 Classification

  3. Architecture Overview

     3.1 What does it look like ?
     3.2 How to utilise the other nodes ?
     3.3 How does Beowulf differ from a COW ?

  4. System Design

     4.1 A brief background on parallel computing.
     4.2 The methods of parallel computing
        4.2.1 Why more than one CPU?
        4.2.2 The Parallel Computing Store
           4.2.2.1 Single-tasking Operating System
           4.2.2.2 Multi-tasking Operating System:
           4.2.2.3 Multitasking Operating Systems with Multiple CPUs:
           4.2.2.4 Threads on a Multitasking Operating Systems extra CPUs
           4.2.2.5 Sending Messages on Multitasking Operating Systems with extra CPUs:
     4.3 Architectures for parallel computing
        4.3.1 Hardware Architectures
        4.3.2 Software API Architectures
           4.3.2.1 Messages
           4.3.2.2 Threads
        4.3.3 Application Architecture
     4.4 Suitability
     4.5 Writing and porting parallel software
        4.5.1 Determine concurrent parts of your program
        4.5.2 Estimate parallel efficiency
        4.5.3 Describing the concurrent parts of your program
           4.5.3.1 Explicit Methods
           4.5.3.2 Implicit Methods

  5. Beowulf Resources

     5.1 Starting Points
     5.2 Documentation
     5.3 Papers
     5.4 Software
     5.5 Beowulf Machines
     5.6 Other Interesting Sites
     5.7 History

  6. Source code

     6.1 sum.c
     6.2 sigmasqrt.c
     6.3 prun.sh


  ______________________________________________________________________



  1.  Preamble

  1.1.  Disclaimer

  We will not accept any responsibility for any incorrect information
  within this document, nor for any damage it might cause when applied.


  1.2.  Copyright

  Copyright (C) 1997 - 1998 Jacek Radajewski and Douglas Eadline.
  Permission to distribute and modify this document is granted under the
  GNU General Public Licence.


  1.3.  About this HOWTO

  Jacek Radajewski started work on this document in November 1997 and
  was soon joined by Douglas Eadline.  Over a few months the Beowulf
  HOWTO grew into a large document, and in August 1998 it was split into
  three documents: Beowulf HOWTO, Beowulf Architecture Design HOWTO, and
  the Beowulf Installation and Administration HOWTO.  Version 1.0.0 of
  the Beowulf HOWTO was released to the Linux Documentation Project on
  11 November 1998.  We hope that this is only the beginning of what
  will become a complete Beowulf Documentation Project.


  1.4.  About the authors


  o  Jacek Radajewski works as a Network Manager, and is studying for an
     honors degree in computer science at the University of Southern
     Queensland, Australia.  Jacek's first contact with Linux was in
     1995 and it was love at first sight.  Jacek built his first Beowulf
     cluster in May 1997 and has been playing with the technology ever
     since, always trying to find new and better ways of setting things
     up.  You can contact Jacek by sending e-mail to jacek@usq.edu.au

  o  Douglas Eadline, Ph.D. is President and Principal Scientist at
     Paralogic, Inc., Bethlehem, PA, USA.  Trained as
     Physical/Analytical Chemist, he has been involved with computers
     since 1978 when he built his first single board computer for use
     with chemical instrumentation.  Dr. Eadline's interests now include
     Linux, Beowulf clusters, and parallel algorithms.  Dr. Eadline can
     be contacted by sending email to deadline@plogic.com


  1.5.  Acknowledgements

  The writing of the Beowulf HOWTO was a long proces and is finally
  complete, thanks to many individuals.  I would like to thank the
  following people for their help and contribution to this HOWTO.

  o  Becky for her love, support, and understanding.

  o  Tom Sterling, Don Becker, and other people at NASA who started the
     Beowulf project.

  o  Thanh Tran-Cong and the Faculty of Engineering and Surveying for
     making the topcat Beowulf machine available for experiments.

  o  My supervisor Christopher Vance for many great ideas.

  o  My friend Russell Waldron for great programming ideas, his general
     interest in the project, and support.

  o  My friend David Smith for proof reading this document.

  o  Many other people on the Beowulf mailing list who provided me with
     feedback and ideas.

  o  All the people who are responsible for the Linux operating system
     and all the other free software packages used on topcat and other
     Beowulf machines.


  2.  Introduction


  As the performance of commodity computer and network hardware
  increase, and their prices decrease, it becomes more and more
  practical to build parallel computational systems from off-the-shelf
  components, rather than buying CPU time on very expensive
  Supercomputers.  In fact, the price per performance ratio of a Beowulf
  type machine is between three to ten times better than that for
  traditional supercomputers.  Beowulf architecture scales well, it is
  easy to construct and you only pay for the hardware as most of the
  software is free.


  2.1.  Who should read this HOWTO ?

  This HOWTO is designed for a person with at least some exposure to the
  Linux operating system.  Knowledge of Beowulf technology or
  understanding of more complex operating system and networking concepts
  is not essential, but some exposure to parallel computing would be
  advantageous (after all you must have some reason to read this
  document).  This HOWTO will not answer all possible questions you
  might have about Beowulf, but hopefully will give you ideas and guide
  you in the right direction.  The purpose of this HOWTO is to provide
  background information, links and references to more advanced
  documents.


  2.2.  What is a Beowulf ?

  Famed was this Beowulf: far flew the boast of him, son of Scyld, in
  the Scandian lands.  So becomes it a youth to quit him well with his
  father's friends, by fee and gift, that to aid him, aged, in after
  days, come warriors willing, should war draw nigh, liegemen loyal: by
  lauded deeds shall an earl have honor in every clan. Beowulf is the
  earliest surviving epic poem written in English.  It is a story about
  a hero of great strength and courage who defeted a monster called
  Grendel.  See ``History'' to find out more about the Beowulf hero.

  There are probably as many Beowulf definitions as there are people who
  build or use Beowulf Supercomputer facilities.  Some claim that one
  can call their system Beowulf only if it is built in the same way as
  the NASA's original machine.  Others go to the other extreme and call
  Beowulf any system of workstations running parallel code.  My
  definition of Beowulf fits somewhere between the two views described
  above, and is based on many postings to the Beowulf mailing list:


  Beowulf is a multi computer architecture which can be used for
  parallel computations.  It is a system which usually consists of one
  server node, and one or more client nodes connected together via
  Ethernet or some other network.  It is a system built using commodity
  hardware components, like any PC capable of running Linux, standard
  Ethernet adapters, and switches.  It does not contain any custom
  hardware components and is trivially reproducible.  Beowulf also uses
  commodity software like the Linux operating system, Parallel Virtual
  Machine (PVM) and Message Passing Interface (MPI).  The server node
  controls the whole cluster and serves files to the client nodes.  It
  is also the cluster's console and gateway to the outside world.  Large
  Beowulf machines might have more than one server node, and possibly
  other nodes dedicated to particular tasks, for example consoles or
  monitoring stations.  In most cases client nodes in a Beowulf system
  are dumb, the dumber the better.  Nodes are configured and controlled
  by the server node, and do only what they are told to do.  In a disk-
  less client configuration, client nodes don't even know their IP
  address or name until the server tells them what it is.  One of the
  main differences between Beowulf and a Cluster of Workstations (COW)
  is the fact that Beowulf behaves more like a single machine rather
  than many workstations.  In most cases client nodes do not have
  keyboards or monitors, and are accessed only via remote login or
  possibly serial terminal.  Beowulf nodes can be thought of as a CPU +
  memory package which can be plugged in to the cluster, just like a CPU
  or memory module can be plugged into a motherboard.


  Beowulf is not a special software package, new network topology or the
  latest kernel hack.  Beowulf is a technology of clustering Linux
  computers to form a parallel, virtual supercomputer.  Although there
  are many software packages such as kernel modifications, PVM and MPI
  libraries, and configuration tools which make the Beowulf architecture
  faster, easier to configure, and much more usable, one can build a
  Beowulf class machine using standard Linux distribution without any
  additional software.  If you have two networked Linux computers which
  share at least the /home file system via NFS, and trust each other to
  execute remote shells (rsh), then it could be argued that you have a
  simple, two node Beowulf machine.



  2.3.  Classification

  Beowulf systems have been constructed from a variety of parts.  For
  the sake of performance some non-commodity components (i.e. produced
  by a single manufacturer) have been employed.   In order to account
  for the different types of systems and to make discussions about
  machines a bit easier, we propose the following simple classification
  scheme:

  CLASS I BEOWULF:

  This class of machines built entirely from commodity "off-the-shelf"
  parts.  We shall use the "Computer Shopper" certification test to
  define commodity "off-the-shelf" parts.  (Computer Shopper is a 1 inch
  thick monthly magazine/catalog of PC systems and components.) The test
  is as follows:

  A CLASS I Beowulf is a machine that can be assembled from parts found
  in at least 3 nationally/globally circulated advertising catalogs.

  The advantages of a CLASS I system are:

  o  hardware is available form multiple sources (low prices, easy
     maintenance)

  o  no reliance on a single hardware vendor

  o  driver support from Linux commodity

  o  usually based on standards (SCSI, Ethernet, etc.)

  The disadvantages of a CLASS I system are:

  o  best performance may require CLASS II hardware

  CLASS II BEOWULF

  A CLASS II Beowulf is simply any machine that does not pass the
  Computer Shopper certification test.  This is not a bad thing.
  Indeed, it is merely a classification of the machine.

  The advantages of a CLASS II system are:

  o  Performance can be quite good!

  The disadvantages of a CLASS II system are:

  o  driver support may vary

  o  reliance on single hardware vendor

  o  may be more expensive than CLASS I systems.

  One CLASS is not necessarily better than the other.  It all depends on
  your needs and budget.  This classification system is only intended to
  make discussions about Beowulf systems a bit more succinct.  The
  "System Design" section may help determine what kind of system is best
  suited for your needs.



  3.  Architecture Overview



  3.1.  What does it look like ?

  I think that the best way of describing the Beowulf supercomputer
  architecture is to use an example which is very similar to the actual
  Beowulf, but familiar to most system administrators.  The example that
  is closest to a Beowulf machine is a Unix computer laboratory with a
  server and a number of clients.  To be more specific I'll use the DEC
  Alpha undergraduate computer laboratory at the Faculty of Sciences,
  USQ as the example.  The server computer is called beldin and the
  client machines are called scilab01, scilab02, scilab03, up to
  scilab20.  All clients have a local copy of the Digital Unix 4.0
  operating system installed, but get the user file space (/home) and
  /usr/local from the server via NFS (Network File System).  Each client
  has an entry for the server and all the other clients in its
  /etc/hosts.equiv file, so all clients can execute a remote shell (rsh)
  to all others.  The server machine is a NIS server for the whole
  laboratory, so account information is the same across all the
  machines.  A person can sit at the scilab02 console, login, and have
  the same environment as if he logged onto the server or scilab15.  The
  reason all the clients have the same look and feel is that the
  operating system is installed and configured in the same way on all
  machines, and both the user's /home and /usr/local areas are
  physically on the server and accessed by the clients via NFS.  For
  more information on NIS and NFS please read the NIS and NFS HOWTOs.



  3.2.  How to utilise the other nodes ?


  Now that we have some idea about the system architecture, let us take
  a look at how we can utilise the available CPU cycles of the machines
  in the computer laboratory.  Any person can logon to any of the
  machines, and run a program in their home directory, but they can also
  spawn the same job on a different machine simply by executing remote
  shell.  For example, assume that we want to calculate the sum of the
  square roots of all integers between 1 and 10 inclusive. We write a
  simple program called sigmasqrt (please see ``source code'') which
  does exactly that.  To calculate the sum of the square roots of
  numbers from 1 to 10 we execute :

  [jacek@beldin sigmasqrt]$ time ./sigmasqrt 1 10
  22.468278

  real    0m0.029s
  user    0m0.001s
  sys     0m0.024s


  The time command allows us to check the wall-clock (the elapsed time)
  of running this job.  As we can see, this example took only a small
  fraction of a second (0.029 sec) to execute, but what if I want to add
  the square root of integers from 1 to 1 000 000 000 ?  Let us try
  this, and again calculate the wall-clock time.


  [jacek@beldin sigmasqrt]$ time ./sigmasqrt 1 1000000000
  21081851083600.559000

  real    16m45.937s
  user    16m43.527s
  sys     0m0.108s



  This time, the execution time of the program is considerably longer.
  The obvious question to ask is what can we do to speed up the
  execution time of the job?  How can we change the way the job is
  running to minimize the wall-clock time of running this job?  The
  obvious answer is to split the job into a number of sub-jobs and to
  run these sub-jobs in parallel on all computers.  We could split one
  big addition task into 20 parts, calculating one range of square roots
  and adding them on each node.  When all nodes finish the calculation
  and return their results, the 20 numbers could be added together to
  obtain the final solution.  Before we run this job we will make a
  named pipe which will be used by all processes to write their results.


  [jacek@beldin sigmasqrt]$ mkfifo output
  [jacek@beldin sigmasqrt]$ ./prun.sh & time cat output | ./sum
  [1] 5085
  21081851083600.941000
  [1]+  Done                    ./prun.sh

  real    0m58.539s
  user    0m0.061s
  sys     0m0.206s



  This time we get about 58.5 seconds.  This is the time from starting
  the job until all the nodes have finished their computations and
  written their results into the pipe.  The time does not include the
  final addition of the twenty numbers, but this time is a very small
  fraction of a second and can be ignored.  We can see that there is a
  significant improvement in running this job in parallel.  In fact the
  parallel job ran about 17 times faster, which is very reasonable for a
  20 fold increase in the number of CPUs.  The purpose of the above
  example is to illustrate the simplest method of parallelising
  concurrent code.  In practice such simple examples are rare and
  different techniques (PVM and PMI APIs) are used to achieve the
  parallelism.



  3.3.  How does Beowulf differ from a COW ?

  The computer laboratory described above is a perfect example of a
  Cluster of Workstations (COW).  So what is so special about Beowulf,
  and how is it different from a COW?  The truth is that there is not
  much difference, but Beowulf does have few unique characteristics.
  First of all, in most cases client nodes in a Beowulf cluster do not
  have keyboards, mice, video cards nor monitors.  All access to the
  client nodes is done via remote connections from the server node,
  dedicated console node, or a serial console.  Because there is no need
  for client nodes to access machines outside the cluster, nor for
  machines outside the cluster to access client nodes directly, it is a
  common practice for the client nodes to use private IP addresses like
  the 10.0.0.0/8 or 192.168.0.0/16 address ranges (RFC 1918
  http://www.alternic.net/rfcs/1900/rfc1918.txt.html).  Usually the only
  machine that is also connected to the outside world using a second
  network card is the server node.  The most common ways of using the
  system is to access the server's console directly, or either telnet or
  remote login to the server node from personal workstation.  Once on
  the server node, users can edit and compile their code, and also spawn
  jobs on all nodes in the cluster.  In most cases COWs are used for
  parallel computations at night, and over weekends when people do not
  actually use the workstations for every day work, thus utilising idle
  CPU cycles.  Beowulf on the other hand is a machine usually dedicated
  to parallel computing, and optimised for this purpose.  Beowulf also
  gives better price/performance ratio as it is built from off-the-shelf
  components and runs mainly free software.  Beowulf has also more
  single system image features which help the users to see the Beowulf
  cluster as a single computing workstation.



  4.  System Design

  Before you purchase any hardware, it may be a good idea to consider
  the design of your system.  There are basically two hardware issues
  involved with design of a Beowulf system: the type of nodes or
  computers you are going to use; and way you connect the computer
  nodes.  There is one software issue that may effect your hardware
  decisions; the communication library or API. A more detailed
  discussion of hardware and communication software is provided later in
  this document.

  While the number of choices is not large, there are some important
  design decisions that must be made when constructing a Beowulf
  systems.  Because the science (or art) of "parallel computing" has
  many different interpretations, an introduction is provided below.  If
  you do not like to read background material, you may skip this
  section, but it is advised that you read section  ``Suitability''
  before you make you final hardware decisions.


  4.1.  A brief background on parallel computing.

  This section provides background on parallel computing concepts.  It
  is NOT an exhaustive or complete description of parallel computing
  science and technology. It is a brief description of the issues that
  may be important to a Beowulf designer and user.
  As you design and build your Beowulf, many of these issues described
  below will become important in your decision process. Due to its
  component nature, a Beowulf Supercomputer requires that we consider
  many factors carefully because they are now under our control. In
  general, it is not all that difficult to understand the issues
  involved with parallel computing.  Indeed, once the issues are
  understood, your expectations will be more realistic and success will
  be more likely. Unlike the "sequential world" where processor speed is
  considered the single most important factor, processor speed in the
  "parallel world" is just one of several factors that will determine
  overall system performance and efficiency.



  4.2.  The methods of parallel computing

  Parallel computing can take many forms.  From a user's perspective, it
  is important to consider the advantages and disadvantages of each
  methodology.  The following section attempts to provide some
  perspective on the methods of parallel computing and indicate where
  the Beowulf machine falls on this continuum.


  4.2.1.  Why more than one CPU?

  Answering this question is important.  Using 8 CPUs to run your word
  processor sounds a little like "over-kill" -- and it is.  What about a
  web server, a database, a rendering program, or a project scheduler?
  Maybe extra CPUs would help.  What about a complex simulation, a fluid
  dynamics code, or a data mining application.  Extra CPUs definitely
  help in these situations.  Indeed, multiple CPUs are being used to
  solve more and more problems.

  The next question usually is: "Why do I need two or four CPUs, I will
  just wait for the 986 turbo-hyper chip." There are several reasons:

  1. Due to the use of multi-tasking Operating Systems, it is possible
     to do several things at once.  This is a natural "parallelism" that
     is easily exploited by more than one low cost CPU.

  2. Processor speeds have been doubling every 18 months, but what about
     RAM speeds or hard disk speeds? Unfortunately, these speeds are not
     increasing as fast as the CPU speeds.  Keep in mind most
     applications require "out of cache memory access" and hard disk
     access.  Doing things in parallel is one way to get around some of
     these limitations.

  3. Predictions indicate that processor speeds will not continue to
     double every 18 months after the year 2005. There are some very
     serious obstacles to overcome in order to maintain this trend.

  4. Depending on the application, parallel computing can speed things
     up by any where from 2 to 500 times faster (in some cases even
     faster). Such performance is not available using a single
     processor.  Even supercomputers that at one time used very fast
     custom processors are now built from multiple "commodity- off-the-
     shelf" CPUs.

  If you need speed - either due to a compute bound problem and/or an
  I/O bound problem, parallel is worth considering.  Because parallel
  computing is implemented in a variety of ways, solving your problem in
  parallel will require some very important decisions to be made.  These
  decisions may dramatically effect portability, performance, and cost
  of your application.


  Before we get technical, let's look take a look at a real "parallel
  computing problem" using an example with which we are familiar -
  waiting in long lines at a store.


  4.2.2.  The Parallel Computing Store

  Consider a big store with 8 cash registers grouped together in the
  front of the store.  Assume each cash register/cashier is a CPU and
  each customer is a computer program.  The size of the computer program
  (amount of work) is the size of each customer's order. The following
  analogies can be used to illustrate parallel computing concepts.


  4.2.2.1.  Single-tasking Operating System

  One cash register open (is in use) and must process each customer one
  at a time.

  Computer Example: MS DOS


  4.2.2.2.  Multi-tasking Operating System:

  One cash register open, but now we process only a part of each order
  at a time, move to the next person and process some of their order.
  Everyone "seems" to be moving through the line together, but if no one
  else is in the line, you will get through the line faster.

  Computer Example: UNIX, NT using a single CPU


  4.2.2.3.  Multitasking Operating Systems with Multiple CPUs:

  Now we open several cash registers in the store. Each order can be
  processed by a separate cash register and the line can move much
  faster.  This is called SMP - Symmetric Multi-processing.  Although
  there are extra cash registers open, you will still never get through
  the line any faster than just you and a single cash register.

  Computer Example: UNIX and NT with multiple CPUs



  4.2.2.4.  Threads on a Multitasking Operating Systems extra CPUs

  If you "break-up" the items in your order, you might be able to move
  through the line faster by using several cash registers at one time.
  First, we must assume you have a large amount of goods, because the
  time you invest "breaking up your order" must be regained by using
  multiple cash registers.   In theory, you should be able to move
  through the line "n" times faster than before*; where "n" is the
  number of cash registers.  When the cashiers need to get sub- totals,
  they can exchange information quickly by looking and talking to all
  the other "local" cash registers. They can even snoop around the other
  cash registers to find information they need to work faster. There is
  a limit, however, as to how many cash registers the store can
  effectively locate in any one place.

  Amdals law will also limit the application speed-up to the slowest
  sequential portion of the program.

  Computer Example: UNIX or NT with extra CPU on the same motherboard
  running multi-threaded programs.


  4.2.2.5.  Sending Messages on Multitasking Operating Systems with
  extra CPUs:

  In order to improve performance, the store adds 8 cash registers at
  the back of the store.  Because the new cash registers are far away
  from the front cash registers, the cashiers must call on the phone to
  send their sub-totals to the front of the store. This distance adds
  extra overhead (time) to communication between cashiers, but if
  communication is minimized, it is not a problem.   If you have a
  really big order, one that requires all the cash registers, then as
  before your speed can be improved by using all cash registers at the
  same time, the extra overhead must be considered. In some cases, the
  store may have single cash registers (or islands of cash registers)
  located all over the store - each cash register (or island) must
  communicate by phone.  Since all the cashiers working the cash
  registers can talk to each other by phone, it does not matter too much
  where they are.

  Computer Example: One or several copies of UNIX or NT with extra CPUs
  on the same or different motherboard communicating through messages.

  The above scenarios, although not exact, are a good representation of
  constraints placed on parallel systems.  Unlike a single CPU (or cash
  register) communication is an issue.


  4.3.  Architectures for parallel computing

  The common methods and architectures of parallel computing are
  presented below.  While this description is by no means exhaustive, it
  is enough to understand the basic issues involved with Beowulf design.


  4.3.1.  Hardware Architectures


  There are basically two ways parallel computer hardware is put
  together:


  1. Local memory machines that communicate by messages (Beowulf
     Clusters)

  2. Shared memory machines that communicate through memory (SMP
     machines)

  A typical Beowulf is a collection of single CPU machines connected
  using fast Ethernet and is, therefore, a local memory machine.  A 4
  way SMP box is a shared memory machine and can be used for parallel
  computing - parallel applications communicate using shared memory.
  Just as in the computer store analogy, local memory machines
  (individual cash registers) can be scaled up to large numbers of CPUs,
  while the number of CPUs shared memory machines (the number of cash
  registers you can place in one spot) can have is limited due to memory
  contention.

  It is possible, however, to connect many shared memory machines to
  create a "hybrid" shared memory machine.  These hybrid machines "look"
  like a single large SMP machine to the user and are often called NUMA
  (non uniform memory access) machines because the global memory seen by
  the programmer and shared by all the CPUs can have different
  latencies.  At some level, however, a NUMA machine must "pass
  messages" between local shared memory pools.

  It is also possible to connect SMP machines as local memory compute
  nodes.  Typical CLASS I motherboards have either 2 or 4 CPUs and are
  often used as a means to reduce the overall system cost. The Linux
  internal scheduler determines how these CPUs get shared.  The user
  cannot (at this point) assign a specific task to a specific SMP
  processor.  The user can however, start two independent processes or a
  threaded processes and expect to see a performance increase over a
  single CPU system.


  4.3.2.  Software API Architectures

  There basically two ways to "express" concurrency in a program:

  1. Using Messages sent between processors

  2. Using operating system Threads

  Other methods do exist, but these are the two most widely used. It is
  important to remember that the expression of concurrency is not
  necessary controlled by the underlying hardware.  Both Messages and
  Threads can be implemented on SMP, NUMA-SMP, and clusters - although
  as explained below efficiently and portability are important issues.


  4.3.2.1.  Messages

  Historically, messages passing technology reflected the design of
  early local memory parallel computers. Messages require copying data
  while Threads use data in place.  The latency and speed at which
  messages can be copied are the limiting factor with message passing
  models. A Message is quite simple: some data and a destination
  processor.  Common message passing APIs are PVM or MPI. Message
  passing can be efficiently implemented using Threads and Messages work
  well both on SMP machine and between clusters of machines.  The
  advantage to using messages on an SMP machine, as opposed to Threads,
  is that if you decided to use clusters in the future it is easy to add
  machines or scale your application.


  4.3.2.2.  Threads

  Operating system Threads were developed because shared memory SMP
  (symmetrical multiprocessing) designs allowed very fast shared memory
  communication and synchronization between concurrent parts of a
  program.  Threads work well on SMP systems because communication is
  through shared memory.  For this reason the user must isolate local
  data from global data, otherwise programs will not work properly. In
  contrast to messages, a large amount of copying can be eliminated with
  threads because the data is shared between processes (threads). Linux
  supports POSIX threads. The problem with threads is that it is
  difficult to extend them beyond one SMP machine and because data is
  shared between CPUs, cache coherence issues can contribute to
  overhead. Extending threads beyond the SMP boundary efficiently
  requires NUMA technology which is expensive and not natively supported
  by Linux. Implementing threads on top of messages has been done
  ((http://syntron.com/ptools/ptools_pg.htm)), but Threads are often
  inefficient when implemented using messages.

  The following can be stated about performance:



            SMP machine     cluster of machines  scalability
            performance        performance
            -----------     -------------------  -----------
  messages    good                best              best

  threads     best               poor*              poor*

  * requires expensive NUMA technology.



  4.3.3.  Application Architecture

  In order to run an application in parallel on multiple CPUs, it must
  be explicitly broken in to concurrent parts.  A standard single CPU
  application will run no faster than a single CPU application on
  multiple processors.  There are some tools and compilers that can
  break up programs, but parallelizing codes is not a "plug and play"
  operation.  Depending on the application, parallelizing code can be
  easy, extremely difficult, or in some cases impossible due to
  algorithm dependencies.

  Before the software issues can be addressed the concept of Suitability
  needs to be introduced.


  4.4.  Suitability

  Most questions about parallel computing have the same answer:

  "It all depends upon the application."

  Before we jump into the issues, there is one very important
  distinction that needs to be made - the difference between CONCURRENT
  and PARALLEL.  For the sake of this discussion we will define these
  two concepts as follows:

  CONCURRENT parts of a program are those that can be computed
  independently.

  PARALLEL parts of a program are those CONCURRENT parts that are
  executed on separate processing elements at the same time.

  The distinction is very important, because CONCURRENCY is a property
  of the program and efficient PARALLELISM is a property of the machine.
  Ideally, PARALLEL execution should result in faster performance.  The
  limiting factor in parallel performance is the communication speed and
  latency between compute nodes. (Latency also exists with threaded SMP
  applications due to cache coherency.) Many of the common parallel
  benchmarks are highly parallel and communication and latency are not
  the bottle neck. This type of problem can be called  "obviously
  parallel".  Other applications are not so simple and executing
  CONCURRENT parts of the program in PARALLEL may actually cause the
  program to run slower, thus offsetting any performance gains in other
  CONCURRENT parts of the program.   In simple terms, the cost of
  communication time must pay for the savings in computation time,
  otherwise the PARALLEL execution of the CONCURRENT part is
  inefficient.

  The task of the programmer is to determining what CONCURRENT parts of
  the program SHOULD be executed in PARALLEL and what parts SHOULD NOT.
  The answer to this will determine the EFFICIENCY of application.  The
  following graph summarizes the situation for the programmer:


           | *
           | *
           | *
   % of    | *
   appli-  |  *
   cations |  *
           |  *
           |  *
           |    *
           |     *
           |      *
           |        ****
           |            ****
           |                ********************
           +-----------------------------------
            communication time/processing time



  In a perfect parallel computer, the ratio of communication/processing
  would be equal and anything that is CONCURRENT could be implemented in
  PARALLEL.  Unfortunately, Real parallel computers, including shared
  memory machines, are subject to the effects described in this graph.
  When designing a Beowulf, the user may want to keep this graph in mind
  because parallel efficiency depends upon ratio of communication time
  and processing time for A SPECIFIC PARALLEL COMPUTER.  Applications
  may be portable between parallel computers, but there is no guarantee
  they will be efficient on a different platform.

  IN GENERAL, THERE IS NO SUCH THING AS A PORTABLE AND EFFICIENT
  PARALLEL PROGRAM

  There is yet another consequence to the above graph.  Since efficiency
  depends upon the comm./process. ratio, just changing one component of
  the ratio does not necessary mean a specific application will perform
  faster.  A change in processor speed, while keeping the communication
  speed that same may have non- intuitive effects on your program.   For
  example, doubling or tripling the CPU speed, while keeping the
  communication speed the same, may now make some previously efficient
  PARALLEL portions of your program, more efficient if they were
  executed SEQUENTIALLY.  That is, it may now be faster to run the
  previously PARALLEL parts as SEQUENTIAL.  Furthermore, running
  inefficient parts in parallel will actually keep your application from
  reaching its maximum speed. Thus, by adding faster processor, you may
  actually slowed down your application (you are keeping the new CPU
  from running at its maximum speed for that application)

  UPGRADING TO A FASTER CPU MAY ACTUALLY SLOW DOWN YOUR APPLICATION

  So, in conclusion, to know whether or not you can use a parallel
  hardware environment, you need to have some insight into the
  suitability of a particular machine to your application.  You need to
  look at a lot of issues including CPU speeds, compiler, message
  passing API, network, etc.  Please note, just profiling an
  application, does not give the whole story.  You may identify a
  computationally heavy portion of your program, but you do not know the
  communication cost for this portion.  It may be that for a given
  system, the communication cost as do not make parallelizing this code
  efficient.


  A final note about a common misconception. It is often stated that "a
  program is PARALLELIZED", but in reality only the CONCURRENT parts of
  the program have been located. For all the reasons given above, the
  program is not PARALLELIZED.   Efficient PARALLELIZATION is a property
  of the machine.
  4.5.  Writing and porting parallel software

  Once you decide that you need parallel computing and would like to
  design and build a Beowulf, a few moments considering your application
  with respect to the previous discussion may be a good idea.

  In general there are two things you can do:

  1. Go ahead and construct a CLASS I Beowulf and then "fit" your
     application to it.  Or run existing parallel applications that you
     know work on your Beowulf (but beware of the portability and
     efficiently issues mentioned above)

  2. Look at the applications you need to run on your Beowulf and make
     some estimations as to the type of hardware and software you need.

  In either case, at some point you will need to look at the efficiency
  issues.  In general, there are three things you need to do:

  1. Determine concurrent parts of your program

  2. Estimate parallel efficiently

  3. Describing the concurrent parts of your program

  Let's look at these one at a time.


  4.5.1.  Determine concurrent parts of your program

  This step is often considered "parallelizing your program".
  Parallelization decisions will be made in step 2.  In this step, you
  need to determine data dependencies.

  >From a practical standpoint, applications may exhibit two types of
  concurrency: compute (number crunching) and I/O (database). Although
  in many cases compute and I/O concurrency are orthogonal, there are
  application that require both. There are tools available that can
  perform concurrency analysis on existing applications.  Most of these
  tools are designed for FORTRAN.  There are two reasons FORTRAN is
  used: historically most number crunching applications were written in
  FORTRAN and it is easier to analyze.  If no tools are available, then
  this step can be some what difficult for existing applications.


  4.5.2.  Estimate parallel efficiency

  Without the help of tools, this step may require trial and error tests
  or just a plain old educated guess.  If you have a specific
  application in mind, try to determine if it is CPU limited (compute
  bound) or hard disk limited (I/O bound).  The requirements of your
  Beowulf may be quite different depending upon your needs.  For
  example, a compute bound problem may need a few very fast CPUs and
  high speed low latency network, while an I/O bound problem may work
  better with more slower CPUs and fast Ethernet.

  This recommendation often comes as a surprise to most people because,
  the standard assumption is that faster processor are always better.
  While this is true if your have an unlimited budget, real systems may
  have cost constraints that should be maximized.  For I/O bound
  problems, there is a little known rule (called the Eadline-Dedkov Law)
  that is quite helpful:

  For two given parallel computers with the same cumulative CPU
  performance index, the one which has slower processors (and a probably
  correspondingly slower interprocessor communication network) will have
  better performance for I/O-dominant applications.

  While the proof of this rule is beyond the scope of this document, you
  find it interesting to download the paper Performance Considerations
  for I/O-Dominant Applications on Parallel Computers (Postscript format
  109K ) (ftp://www.plogic.com/pub/papers/exs-pap6.ps)

  Once you have determined what type of concurrency you have in your
  application, you will need to estimate how efficient it will be in
  parallel.  See Section ``Software'' for a description of Software
  tools.

  In the absence of tools, you may try to guess your way through this
  step.  If a compute bound loop measured in minutes and the data can be
  transferred in seconds, then it might be a good candidate for
  parallelization.  But remember, if you take a 16 minute loop and break
  it into 32 parts, and your data transfers require several seconds per
  part, then things are going to get tight.  You will reach a point of
  diminishing returns.


  4.5.3.  Describing the concurrent parts of your program

  There are several ways to describe concurrent parts of your program:

  1. Explicit parallel execution

  2. Implicit parallel execution

  The major difference between the two is that explicit parallelism is
  determined by the user where implicit parallelism is determined by the
  compiler.


  4.5.3.1.  Explicit Methods

  These are basically method where the user must modify source code
  specifically for a parallel computer.  The user must either add
  messages using PVM or MPI or add threads using POSIX threads. (Keep in
  mind however, threads can not move between SMP motherboards).

  Explicit methods tend to be the most difficult to implement and debug.
  Users typically embed explicit function calls in standard FORTRAN 77
  or C/C++ source code.  The MPI library has added some functions to
  make some standard parallel methods easier to implement (i.e.
  scatter/gather functions).  In addition, it is also possible to use
  standard libraries that have been written for parallel computers.
  Keep in mind, however, the portability vs. efficiently trade-off)


  For historical reasons, most number crunching codes are written in
  FORTRAN.  For this reasons, FORTRAN has the largest amount of support
  (tools, libraries, etc.) for parallel computing.  Many programmers now
  use C or re- write existing FORTRAN applications in C with the notion
  the C will allow faster execution.  While this may be true as C is the
  closest thing to a universal machine code, it has some major
  drawbacks.  The use of pointers in C makes determining data
  dependencies extremely difficult.  Automatic analysis of pointers is
  extremely difficult. If you have an existing FORTRAN program and think
  that you might want to parallelize it in the future - DO NOT CONVERT
  IT TO C!



  4.5.3.2.  Implicit Methods

  Implicit methods are those where the user gives up some (or all) of
  the parallelization decisions to the compiler.  Examples are FORTRAN
  90, High Performance FORTRAN (HPF), Bulk Synchronous Parallel (BSP),
  and a whole collection of other methods that are under development.

  Implicit methods require the user to provide some information about
  the concurrent nature of their application, but the compiler will then
  make many decisions about how to execute this concurrency in parallel.
  These methods provide some level of portability and efficiency, but
  there is still no "best way" to describe a concurrent problem for a
  parallel computer.


  5.  Beowulf Resources



  5.1.  Starting Points



  o  Beowulf mailing list.  To subscribe send mail to beowulf-
     request@cesdis.gsfc.nasa.gov with the word subscribe in the message
     body.

  o  Beowulf Homepage http://www.beowulf.org

  o  Extreme Linux http://www.extremelinux.org

  o  Extreme Linux Software from Red Hat http://www.redhat.com/extreme



  5.2.  Documentation



  o  The latest version of the Beowulf HOWTO
     http://www.sci.usq.edu.au/staff/jacek/beowulf.

  o  Building a Beowulf System
     http://www.cacr.caltech.edu/beowulf/tutorial/building.html

  o  Jacek's Beowulf Links
     http://www.sci.usq.edu.au/staff/jacek/beowulf.

  o  Beowulf Installation and Administration HOWTO (DRAFT)
     http://www.sci.usq.edu.au/staff/jacek/beowulf.

  o  Linux Parallel Processing HOWTO
     http://yara.ecn.purdue.edu/~pplinux/PPHOWTO/pphowto.html



  5.3.  Papers



  o  Chance Reschke, Thomas Sterling, Daniel Ridge, Daniel Savarese,
     Donald Becker, and Phillip Merkey A Design Study of Alternative
     Network Topologies for the Beowulf Parallel Workstation.
     Proceedings Fifth IEEE International Symposium on High Performance
     Distributed Computing, 1996.
     http://www.beowulf.org/papers/HPDC96/hpdc96.html
  o  Daniel Ridge, Donald Becker, Phillip Merkey, Thomas Sterling
     Becker, and Phillip Merkey. Harnessing the Power of Parallelism in
     a Pile-of-PCs.  Proceedings, IEEE Aerospace, 1997.
     http://www.beowulf.org/papers/AA97/aa97.ps


  o  Thomas Sterling, Donald J. Becker, Daniel Savarese, Michael R.
     Berry, and Chance Res. Achieving a Balanced Low-Cost Architecture
     for Mass Storage Management through Multiple Fast Ethernet Channels
     on the Beowulf Parallel Workstation.  Proceedings, International
     Parallel Processing Symposium, 1996.
     http://www.beowulf.org/papers/IPPS96/ipps96.html


  o  Donald J. Becker, Thomas Sterling, Daniel Savarese, Bruce Fryxell,
     Kevin Olson. Communication Overhead for Space Science Applications
     on the Beowulf Parallel Workstation.  Proceedings,High Performance
     and Distributed Computing, 1995.
     http://www.beowulf.org/papers/HPDC95/hpdc95.html



  o  Donald J. Becker, Thomas Sterling, Daniel Savarese, John E.
     Dorband, Udaya A. Ranawak, Charles V.  Packer. BEOWULF: A PARALLEL
     WORKSTATION FOR SCIENTIFIC COMPUTATION.  Proceedings, International
     Conference on Parallel Processing, 95.
     http://www.beowulf.org/papers/ICPP95/icpp95.html

  o  Papers at the Beowulf site
     http://www.beowulf.org/papers/papers.html



  5.4.  Software


  o  PVM - Parallel Virtual Machine
     http://www.epm.ornl.gov/pvm/pvm_home.html



  o  LAM/MPI (Local Area Multicomputer / Message Passing Interface
     http://www.mpi.nd.edu/lam

  o  BERT77 - FORTRAN conversion tool http://www.plogic.com/bert.html

  o  Beowulf software from Beowulf Project Page
     http://beowulf.gsfc.nasa.gov/software/software.html

  o  Jacek's Beowulf-utils ftp://ftp.sci.usq.edu.au/pub/jacek/beowulf-
     utils

  o  bWatch - cluster monitoring tool
     http://www.sci.usq.edu.au/staff/jacek/bWatch



  5.5.  Beowulf Machines


  o  Avalon consists of 140 Alpha processors, 36 GB of RAM, and is
     probably the fastest Beowulf machine, cruising at 47.7 Gflops and
     ranking 114th on the Top 500 list.  http://swift.lanl.gov/avalon/

  o  Megalon-A Massively PArallel CompuTer Resource (MPACTR) consists of
     14, quad CPU Pentium Pro 200 nodes, and 14 GB of RAM.
     http://megalon.ca.sandia.gov/description.html

  o  theHIVE - Highly-parallel Integrated Virtual Environment is another
     fast Beowulf Supercomputer.  theHIVE is a 64 node, 128 CPU machine
     with the total of 4 GB RAM.  http://newton.gsfc.nasa.gov/thehive/

  o  Topcat is a much smaller machine and consists of 16 CPUs and 1.2 GB
     RAM.  http://www.sci.usq.edu.au/staff/jacek/topcat

  o  MAGI cluster - this is a very interesting site with many good
     links. http://noel.feld.cvut.cz/magi/



  5.6.  Other Interesting Sites



  o  SMP Linux http://www.linux.org.uk/SMP/title.html

  o  Paralogic - Buy a Beowulf http://www.plogic.com


  5.7.  History


  o  Legends - Beowulf  http://legends.dm.net/beowulf/index.html

  o  The Adventures of Beowulf
     http://www.lnstar.com/literature/beowulf/beowulf.html


  6.  Source code


  6.1.  sum.c


  /* Jacek Radajewski jacek@usq.edu.au */
  /* 21/08/1998 */

  #include <stdio.h>
  #include <math.h>

  int main (void) {

    double result = 0.0;
    double number = 0.0;
    char string[80];


    while (scanf("%s", string) != EOF) {

      number = atof(string);
      result = result + number;
    }

    printf("%lf\n", result);

    return 0;

  }

  6.2.  sigmasqrt.c


  /* Jacek Radajewski jacek@usq.edu.au */
  /* 21/08/1998 */

  #include <stdio.h>
  #include <math.h>

  int main (int argc, char** argv) {

    long number1, number2, counter;
    double result;

    if (argc < 3) {
      printf ("usage : %s number1 number2\n",argv[0]);
      exit(1);
    } else {
      number1 = atol (argv[1]);
      number2 = atol (argv[2]);
      result = 0.0;
    }

    for (counter = number1; counter <= number2; counter++) {
      result = result + sqrt((double)counter);
    }

    printf("%lf\n", result);

    return 0;

  }



  6.3.  prun.sh



  #!/bin/bash
  # Jacek Radajewski jacek@usq.edu.au
  # 21/08/1998

  export SIGMASQRT=/home/staff/jacek/beowulf/HOWTO/example1/sigmasqrt

  # $OUTPUT must be a named pipe
  # mkfifo output

  export OUTPUT=/home/staff/jacek/beowulf/HOWTO/example1/output

  rsh scilab01 $SIGMASQRT         1  50000000 > $OUTPUT < /dev/null&
  rsh scilab02 $SIGMASQRT  50000001 100000000 > $OUTPUT < /dev/null&
  rsh scilab03 $SIGMASQRT 100000001 150000000 > $OUTPUT < /dev/null&
  rsh scilab04 $SIGMASQRT 150000001 200000000 > $OUTPUT < /dev/null&
  rsh scilab05 $SIGMASQRT 200000001 250000000 > $OUTPUT < /dev/null&
  rsh scilab06 $SIGMASQRT 250000001 300000000 > $OUTPUT < /dev/null&
  rsh scilab07 $SIGMASQRT 300000001 350000000 > $OUTPUT < /dev/null&
  rsh scilab08 $SIGMASQRT 350000001 400000000 > $OUTPUT < /dev/null&
  rsh scilab09 $SIGMASQRT 400000001 450000000 > $OUTPUT < /dev/null&
  rsh scilab10 $SIGMASQRT 450000001 500000000 > $OUTPUT < /dev/null&
  rsh scilab11 $SIGMASQRT 500000001 550000000 > $OUTPUT < /dev/null&
  rsh scilab12 $SIGMASQRT 550000001 600000000 > $OUTPUT < /dev/null&
  rsh scilab13 $SIGMASQRT 600000001 650000000 > $OUTPUT < /dev/null&
  rsh scilab14 $SIGMASQRT 650000001 700000000 > $OUTPUT < /dev/null&
  rsh scilab15 $SIGMASQRT 700000001 750000000 > $OUTPUT < /dev/null&
  rsh scilab16 $SIGMASQRT 750000001 800000000 > $OUTPUT < /dev/null&
  rsh scilab17 $SIGMASQRT 800000001 850000000 > $OUTPUT < /dev/null&
  rsh scilab18 $SIGMASQRT 850000001 900000000 > $OUTPUT < /dev/null&
  rsh scilab19 $SIGMASQRT 900000001 950000000 > $OUTPUT < /dev/null&
  rsh scilab20 $SIGMASQRT 950000001 1000000000 > $OUTPUT < /dev/null&



  The Linux BootPrompt-HowTo
  by Paul Gortmaker.
  v1.2, May 1999

  This is the BootPrompt-Howto, which is a compilation of all the possi-
  ble boot time arguments that can be passed to the Linux kernel at boot
  time. This includes all kernel and device parameters.  A discussion of
  how the kernel sorts boot time arguments, along with an overview of
  some of the popular software used to boot Linux kernels is also
  included.
  ______________________________________________________________________

  Table of Contents



  1. Introduction

     1.1 Disclaimer and Copyright
     1.2 Intended Audience and Applicability
     1.3 Related Documentation
     1.4 The Linux Newsgroups
     1.5 New Versions of this Document

  2. Overview of Boot Prompt Arguments

     2.1 LILO (LInux LOader)
     2.2 LoadLin
     2.3 The ``rdev'' utility
     2.4 How the Kernel Sorts the Arguments
     2.5 Setting Environment Variables.
     2.6 Passing Arguments to the `init' program

  3. General Non-Device Specific Boot Args

     3.1 Root Filesystem options
        3.1.1 The `root=' Argument
        3.1.2 The `ro' Argument
        3.1.3 The `rw' Argument
     3.2 Options Relating to RAM Disk Management
        3.2.1 The `ramdisk_start=' Argument
        3.2.2 The `load_ramdisk=' Argument
        3.2.3 The `prompt_ramdisk=' Argument
        3.2.4 The `ramdisk_size=' Argument
        3.2.5 The `ramdisk=' Argument (obsolete)
        3.2.6 The `noinitrd' (initial RAM disk) Argument
     3.3 Boot Arguments Related to Memory Handling
        3.3.1 The `mem=' Argument
        3.3.2 The `swap=' Argument
        3.3.3 The `buff=' Argument
     3.4 Boot Arguments for NFS Root Filesystem
        3.4.1 The `nfsroot=' Argument
        3.4.2 The `nfsaddrs=' Argument
     3.5 Other Misc. Kernel Boot Arguments
        3.5.1 The `debug' Argument
        3.5.2 The `init=' Argument
        3.5.3 The `kbd-reset' Argument
        3.5.4 The `maxcpus=' Argument
        3.5.5 The `mca-pentium' Argument
        3.5.6 The `md=' Argument
        3.5.7 The `no387' Argument
        3.5.8 The `no-hlt' Argument
        3.5.9 The `no-scroll' Argument
        3.5.10 The `noapic' Argument
        3.5.11 The `nosmp' Argument
        3.5.12 The `panic=' Argument
        3.5.13 The `pci=' Argument
        3.5.14 The `pirq=' Argument
        3.5.15 The `profile=' Argument
        3.5.16 The `reboot=' Argument
        3.5.17 The `reserve=' Argument
        3.5.18 The `vga=' Argument

  4. Boot Arguments to Control PCI Bus Behaviour (`pci=')

     4.1 The `pci=bios' and `pci=nobios' Arguments
     4.2 The `pci=conf1' and `pci=conf2' Arguments
     4.3 The `pci=io=' Argument
     4.4 The `pci=nopeer' Argument
     4.5 The `pci=nosort' Argument
     4.6 The `pci=off' Argument
     4.7 The `pci=reverse' Argument
  5. Boot Arguments for Video Frame Buffer Drivers

     5.1 The `video=map:...' Argument
     5.2 The `video=scrollback:...' Argument
     5.3 The `video=vc:...' Argument

  6. Boot Arguments for SCSI Peripherals.

     6.1 Arguments for Mid-level Drivers
        6.1.1 Maximum Probed LUNs (`max_scsi_luns=')
        6.1.2 SCSI Logging (`scsi_logging=')
        6.1.3 Parameters for the SCSI Tape Driver (`st=')
     6.2 Arguments for SCSI Host Adapters
        6.2.1 Adaptec aha151x, aha152x, aic6260, aic6360, SB16-SCSI (`aha152x=')
        6.2.2 Adaptec aha154x (`aha1542=')
        6.2.3 Adaptec aha274x, aha284x, aic7xxx (`aic7xxx=')
        6.2.4 AdvanSys SCSI Host Adaptors (`advansys=')
        6.2.5 Always IN2000 Host Adaptor (`in2000=')
        6.2.6 AMD AM53C974 based hardware (`AM53C974=')
        6.2.7 BusLogic SCSI Hosts with v1.2 kernels (`buslogic=')
        6.2.8 BusLogic SCSI Hosts with v2.x kernels (`BusLogic=')
        6.2.9 EATA SCSI Cards (`eata=')
        6.2.10 Future Domain TMC-8xx, TMC-950 (`tmc8xx=')
        6.2.11 Future Domain TMC-16xx, TMC-3260, AHA-2920 (`fdomain=')
        6.2.12 IOMEGA Parallel Port / ZIP drive (`ppa=')
        6.2.13 NCR5380 based controllers (`ncr5380=')
        6.2.14 NCR53c400 based controllers (`ncr53c400=')
        6.2.15 NCR53c406a based controllers (`ncr53c406a=')
        6.2.16 Pro Audio Spectrum (`pas16=')
        6.2.17 Seagate ST-0x (`st0x=')
        6.2.18 Trantor T128 (`t128=')
        6.2.19 Ultrastor SCSI cards (`u14-34f=')
        6.2.20 Western Digital WD7000 cards (`wd7000=')
     6.3 SCSI Host Adapters that don't Accept Boot Args

  7. Hard Disks

     7.1 IDE Disk/CD-ROM Driver Parameters
     7.2 Standard ST-506 Disk Driver Options (`hd=')
     7.3 XT Disk Driver Options (`xd=')

  8. CD-ROMs (Non-SCSI/ATAPI/IDE)

     8.1 The Aztech Interface (`aztcd=')
     8.2 The CDU-31A and CDU-33A Sony Interface (`cdu31a=')
     8.3 The CDU-535 Sony Interface (`sonycd535=')
     8.4 The GoldStar Interface (`gscd=')
     8.5 The ISP16 Interface (`isp16=')
     8.6 The Mitsumi Standard Interface (`mcd=')
     8.7 The Mitsumi XA/MultiSession Interface (`mcdx=')
     8.8 The Optics Storage Interface (`optcd=')
     8.9 The Phillips CM206 Interface (`cm206=')
     8.10 The Sanyo Interface (`sjcd=')
     8.11 The SoundBlaster Pro Interface (`sbpcd=')

  9. Serial and ISDN Drivers

     9.1 The ICN ISDN driver (`icn=')
     9.2 The PCBIT ISDN driver (`pcbit=')
     9.3 The Teles ISDN driver (`teles=')
     9.4 The DigiBoard Driver (`digi=')
     9.5 The RISCom/8 Multiport Serial Driver (`riscom8=')
     9.6 The Baycom Serial/Parallel Radio Modem (`baycom=')

  10. Other Hardware Devices

     10.1 Ethernet Devices (`ether=')
     10.2 The Floppy Disk Driver (`floppy=')
     10.3 The Sound Driver (`sound=')
     10.4 The Bus Mouse Driver (`bmouse=')
     10.5 The MS Bus Mouse Driver (`msmouse=')
     10.6 The Printer Driver (`lp=')

  11. Copying, Translations, Closing, etc.

     11.1 Copyright and Disclaimer
     11.2 Closing


  ______________________________________________________________________

  1.  Introduction


  The kernel has a limited capability to accept information at boot in
  the form of a `command line', similar to an argument list you would
  give to a program. In general this is used to supply the kernel with
  information about hardware parameters that the kernel would not be
  able to determine on its own, or to avoid/override the values that the
  kernel would otherwise detect.

  However, if you just copy a kernel image directly to a floppy, (e.g.
  cp zImage /dev/fd0) then you are not given a chance to specify any
  arguments to that kernel. So most Linux users will use software like
  LILO or loadlin that takes care of handing these arguments to the
  kernel, and then booting it.

  This present revision covers kernels up to and including v2.2.9.  Some
  features that are unique to development/testing kernels up to v2.3.2
  are also documented.

  The BootPrompt-Howto is by:

       Paul Gortmaker, p_gortmaker@yahoo.com



  1.1.  Disclaimer and Copyright

  This document is Copyright (c) 1995-1999 by Paul Gortmaker.  Please
  see the Disclaimer and Copying information at the end of this document
  (``copyright'') for information about redistribution of this document
  and the usual `we are not responsible for what you manage to break...'
  type legal stuff.


  1.2.  Intended Audience and Applicability

  Most Linux users should never have to even look at this document.
  Linux does an exceptionally good job at detecting most hardware and
  picking reasonable default settings for most parameters.  The
  information in this document is aimed at users who might want to
  change some of the default settings to optimize the kernel to their
  particular machine, or to a user who has `rolled their own' kernel to
  support a not so common piece of hardware for which automatic
  detection is currently not available.

  IMPORTANT NOTE: Driver related boot prompt arguments only apply to
  hardware drivers that are compiled directly into the kernel. They have
  no effect on drivers that are loaded as modules. Most Linux
  distributions come with a basic `bare-bones' kernel, and the drivers
  are small modules that are loaded after the kernel has initialized.
  If you are unsure if you are using modules then look at man depmod and
  man modprobe along with the contents of your /etc/conf.modules.


  1.3.  Related Documentation

  The most up-to-date documentation will always be the kernel source
  itself. Hold on! Don't get scared. You don't need to know any
  programming to read the comments in the source files.  For example, if
  you were looking for what arguments could be passed to the AHA1542
  SCSI driver, then you would go to the linux/drivers/scsi directory,
  and look at the file aha1542.c -- and within the first 100 lines, you
  would find a plain english description of the boot time arguments that
  the 1542 driver accepts.

  The linux directory is usually found in /usr/src/ for most
  distributions.  All references in this document to files that come
  with the kernel will have their pathname abbreviated to start with
  linux - you will have to append the /usr/src/ or whatever is
  appropriate for your system.  (If you can't find the file in question,
  then make use of the find and locate commands.)

  The next best thing will be any documentation files that are
  distributed with the kernel itself. There are now quite a few of
  these, and most of them can be found in the directory
  linux/Documentation and subdirectories from there.  Sometimes there
  will be README.foo files that can be found in the related driver
  directory (e.g. linux/drivers/???/, where examples of ??? could be
  scsi, char, or net).

  If you have figured out what boot-args you intend to use, and now want
  to know how to get that information to the kernel, then look at the
  documentation that comes with the software that you use to boot the
  kernel (e.g. LILO or loadlin). A brief overview is given below, but it
  is no substitute for the documentation that comes with the booting
  software.


  1.4.  The Linux Newsgroups

  If you have questions about passing boot arguments to the kernel,
  please check this document first. If this and the related
  documentation mentioned above does not answer your question(s) then
  you can try the Linux newsgroups.  General questions on how to
  configure your system should be directed to the comp.os.linux.setup
  newsgroup.  We ask that you please respect this general guideline for
  content, and don't cross-post your request to other groups.

  Of course you should try checking the group before blindly posting
  your question, as it may even be a Frequently Asked Question (a FAQ).
  A quick browse of the Linux FAQ before posting is a good idea. You
  should be able to find the FAQ somewhere close to where you found this
  document.  If it is not a FAQ then use newsgroup archives, such as
  those at http://www.dejanews.com to quickly search years worth of
  postings for your topic. Chances are someone else has already asked
  (and another person answered!) the question that you now have.


  1.5.  New Versions of this Document

  New versions of this document can be retrieved via anonymous FTP from
  most Linux FTP sites in the directory /pub/Linux/docs/HOWTO/. Updates
  will be made as new information and/or drivers becomes available. If
  this copy that you are presently reading is more than six months old,
  then you should probably check to see if a newer copy exists.  I would
  recommend viewing this via a WWW browser or in the Postscript/dvi
  format. Both of these contain cross-references that are lost in a
  simple plain text version.

  If you want to get the official copy, here is URL.

  BootPrompt-HOWTO <http://metalab.unc.edu/mdw/HOWTO/BootPrompt-
  HOWTO.html>


  2.  Overview of Boot Prompt Arguments


  This section gives some examples of software that can be used to pass
  kernel boot-time arguments to the kernel itself.  It also gives you an
  idea of how the arguments are processed, what limitations there are on
  the boot args, and how they filter down to each appropriate device
  that they are intended for.

  It is important to note that spaces should not be used in a boot
  argument, but only between separate arguments.  A list of values that
  are for a single argument are to be separated with a comma between the
  values, and again without any spaces. See the following examples
  below.


  ______________________________________________________________________
          ether=9,0x300,0xd0000,0xd4000,eth0  root=/dev/hda1            *RIGHT*
          ether = 9, 0x300, 0xd0000, 0xd4000, eth0  root = /dev/hda1    *WRONG*
  ______________________________________________________________________



  Once the Linux kernel is up and running, one can view the command line
  arguments that were in place at boot by simply typing cat
  /proc/cmdline at a shell prompt.


  2.1.  LILO (LInux LOader)

  The LILO program (LInux LOader) written by Werner Almesberger is the
  most commonly used. It has the ability to boot various kernels, and
  stores the configuration information in a plain text file. Most
  distributions ship with LILO as the default boot-loader. LILO can boot
  DOS, OS/2, Linux, FreeBSD, etc. without any difficulties, and is quite
  flexible.

  A typical configuration will have LILO stop and print LILO: shortly
  after you turn on your computer. It will then wait for a few seconds
  for any optional input from the user, and failing that it will then
  boot the default system. Typical system labels that people use in the
  LILO configuration files are linux and backup and msdos. If you want
  to type in a boot argument, you type it in here, after typing in the
  system label that you want LILO to boot from, as shown in the example
  below.


  ______________________________________________________________________
          LILO: linux root=/dev/hda1
  ______________________________________________________________________



  LILO comes with excellent documentation, and for the purposes of boot
  args discussed here, the LILO append= command is of significant
  importance when one wants to add a boot time argument as a permanent
  addition to the LILO config file.  You simply add something like
  append = "foo=bar" to the /etc/lilo.conf file. It can either be added
  at the top of the config file, making it apply to all sections, or to
  a single system section by adding it inside an image= section.  Please
  see the LILO documentation for a more complete description.


  2.2.  LoadLin

  The other commonly used Linux loader is `LoadLin' which is a DOS
  program that has the capability to launch a Linux kernel from the DOS
  prompt (with boot-args) assuming that certain resources are available.
  This is good for people that use DOS and want to launch into Linux
  from DOS.

  It is also very useful if you have certain hardware which relies on
  the supplied DOS driver to put the hardware into a known state. A
  common example is `SoundBlaster Compatible' sound cards that require
  the DOS driver to set a few proprietary registers to put the card into
  a SB compatible mode. Booting DOS with the supplied driver, and then
  loading Linux from the DOS prompt with LOADLIN.EXE avoids the reset of
  the card that happens if one rebooted instead. Thus the card is left
  in a SB compatible mode and hence is useable under Linux.

  There are also other programs that can be used to boot Linux.  For a
  complete list, please look at the programs available on your local
  Linux ftp mirror, under system/Linux-boot/.


  2.3.  The ``rdev'' utility

  There are a few of the kernel boot parameters that have their default
  values stored in various bytes in the kernel image itself.  There is a
  utility called rdev that is installed on most systems that knows where
  these values are, and how to change them.  It can also change things
  that have no kernel boot argument equivalent, such as the default
  video mode used.

  The rdev utility is usually also aliased to swapdev, ramsize, vidmode
  and rootflags. These are the five things that rdev can change, those
  being the root device, the swap device, the RAM disk parameters, the
  default video mode, and the readonly/readwrite setting of root device.

  More information on rdev can be found by typing rdev -h or by reading
  the supplied man page (man rdev).


  2.4.  How the Kernel Sorts the Arguments

  Most of the boot args take the form of:

  ______________________________________________________________________
          name[=value_1][,value_2]...[,value_11]
  ______________________________________________________________________



  where `name' is a unique keyword that is used to identify what part of
  the kernel the associated values (if any) are to be given to. Multiple
  boot args are just a space separated list of the above format. Note
  the limit of 11 is real, as the present code only handles 11 comma
  separated parameters per keyword. (However, you can re-use the same
  keyword with up to an additional 11 parameters in unusually
  complicated situations, assuming the setup function supports it.)
  Also note that the kernel splits the list into a maximum of ten
  integer arguments, and a following string, so you can't really supply
  11 integers unless you convert the 11th arg from a string to an int in
  the driver itself.

  Most of the sorting goes on in linux/init/main.c.  First, the kernel
  checks to see if the argument is any of the special arguments `root=',
  `ro', `rw', or `debug'.  The meaning of these special arguments is
  described further on in the document.

  Then it walks a list of setup functions (contained in the bootsetups
  array) to see if the specified argument string (such as `foo') has
  been associated with a setup function (foo_setup()) for a particular
  device or part of the kernel. If you passed the kernel the line
  foo=3,4,5,6,bar then the kernel would search the bootsetups array to
  see if `foo' was registered. If it was, then it would call the setup
  function associated with `foo' (foo_setup()) and hand it the integer
  arguments 3, 4, 5 and 6 as given on the kernel command line, and also
  hand it the string argument bar.


  2.5.  Setting Environment Variables.

  Anything of the form `foo=bar' that is not accepted as a setup
  function as described above is then interpreted as an environment
  variable to be set. An example would be to use TERM=vt100 or
  BOOT_IMAGE=vmlinuz.bak as a boot argument.  These environment
  variables are typically tested for in the initialization scripts to
  enable or disable a wide range of things.


  2.6.  Passing Arguments to the `init' program

  Any remaining arguments that were not picked up by the kernel and were
  not interpreted as environment variables are then passed onto process
  one, which is usually the init program. The most common argument that
  is passed to the init process is the word single which instructs init
  to boot the computer in single user mode, and not launch all the usual
  daemons. Check the manual page for the version of init installed on
  your system to see what arguments it accepts.


  3.  General Non-Device Specific Boot Args

  These are the boot arguments that are not related to any specific
  device or peripheral. They are instead related to certain internal
  kernel parameters, such as memory handling, ramdisk handling, root
  file system handling and others.


  3.1.  Root Filesystem options

  The following options all pertain to how the kernel selects and
  handles the root filesystem.


  3.1.1.  The `root=' Argument

  This argument tells the kernel what device is to be used as the root
  filesystem while booting. The default of this setting is the value of
  the root device of the system that the kernel was built on.  For
  example, if the kernel in question was built on a system that used
  `/dev/hda1' as the root partition, then the default root device would
  be `/dev/hda1'.  To override this default value, and select the second
  floppy drive as the root device, one would use `root=/dev/fd1'.

  Valid root devices are any of the following devices:


  (1) /dev/hdaN to /dev/hddN, which is partition N on ST-506 compatible
  disk `a to d'.

  (2) /dev/sdaN to /dev/sdeN, which is partition N on SCSI compatible
  disk `a to e'.

  (3) /dev/xdaN to /dev/xdbN, which is partition N on XT compatible disk
  `a to b'.

  (4) /dev/fdN, which is floppy disk drive number N. Having N=0 would be
  the DOS `A:' drive, and N=1 would be `B:'.

  (5) /dev/nfs, which is not really a device, but rather a flag to tell
  the kernel to get the root fs via the network.

  The more awkward and less portable numeric specification of the above
  possible disk devices in major/minor format is also accepted. (e.g.
  /dev/sda3 is major 8, minor 3, so you could use root=0x803 as an
  alternative.)

  This is one of the few kernel boot arguments that has its default
  stored in the kernel image, and which can thus be altered with the
  rdev utility.



  3.1.2.  The `ro' Argument

  When the kernel boots, it needs a root filesystem to read basic things
  off of. This is the root filesystem that is mounted at boot. However,
  if the root filesystem is mounted with write access, you can not
  reliably check the filesystem integrity with half-written files in
  progress. The `ro' option tells the kernel to mount the root
  filesystem as `readonly' so that any filesystem consistency check
  programs (fsck) can safely assume that there are no half-written files
  in progress while performing the check. No programs or processes can
  write to files on the filesystem in question until it is `remounted'
  as read/write capable.

  This is one of the few kernel boot arguments that has its default
  stored in the kernel image, and which can thus be altered with the
  rdev utility.


  3.1.3.  The `rw' Argument

  This is the exact opposite of the above, in that it tells the kernel
  to mount the root filesystem as read/write. The default is to mount
  the root filesystem as read/write anyway. Do not run any `fsck' type
  programs on a filesystem that is mounted read/write.

  The same value stored in the image file mentioned above is also used
  for this parameter, accessible via rdev.


  3.2.  Options Relating to RAM Disk Management

  The following options all relate to how the kernel handles the RAM
  disk device, which is usually used for bootstrapping machines during
  the install phase, or for machines with modular drivers that need to
  be installed to access the root filesystem.



  3.2.1.  The `ramdisk_start=' Argument

  To allow a kernel image to reside on a floppy disk along with a
  compressed ramdisk image, the `ramdisk_start=<offset>' command was
  added. The kernel can't be included into the compressed ramdisk
  filesystem image, because it needs to be stored starting at block zero
  so that the BIOS can load the bootsector and then the kernel can
  bootstrap itself to get going.

  Note: If you are using an uncompressed ramdisk image, then the kernel
  can be a part of the filesystem image that is being loaded into the
  ramdisk, and the floppy can be booted with LILO, or the two can be
  separate as is done for the compressed images.

  If you are using a two-disk boot/root setup (kernel on disk 1, ramdisk
  image on disk 2) then the ramdisk would start at block zero, and an
  offset of zero would be used. Since this is the default value, you
  would not need to actually use the command at all.


  3.2.2.  The `load_ramdisk=' Argument

  This parameter tells the kernel whether it is to try to load a ramdisk
  image or not. Specifying `load_ramdisk=1' will tell the kernel to load
  a floppy into the ramdisk. The default value is zero, meaning that the
  kernel should not try to load a ramdisk.

  Please see the file linux/Documentation/ramdisk.txt for a complete
  description of the new boot time arguments, and how to use them. A
  description of how this parameter can be set and stored in the kernel
  image via `rdev' is also described.


  3.2.3.  The `prompt_ramdisk=' Argument

  This parameter tells the kernel whether or not to give you a prompt
  asking you to insert the floppy containing the ramdisk image. In a
  single floppy configuration the ramdisk image is on the same floppy as
  the kernel that just finished loading/booting and so a prompt is not
  needed. In this case one can use `prompt_ramdisk=0'. In a two floppy
  configuration, you will need the chance to switch disks, and thus
  `prompt_ramdisk=1' can be used. Since this is the default value, it
  doesn't really need to be specified. ( (Historical note: Sneaky people
  used to use the `vga=ask' LILO option to temporarily pause the boot
  process and allow a chance to switch from boot to root floppy.)

  Please see the file linux/Documentation/ramdisk.txt for a complete
  description of the new boot time arguments, and how to use them. A
  description of how this parameter can be set and stored in the kernel
  image via `rdev' is also described.


  3.2.4.  The `ramdisk_size=' Argument

  While it is true that the ramdisk grows dynamically as required, there
  is an upper bound on its size so that it doesn't consume all available
  RAM and leave you in a mess. The default is 4096 (i.e. 4MB) which
  should be large enough for most needs. You can override the default to
  a bigger or smaller size with this boot argument.

  Please see the file linux/Documentation/ramdisk.txt for a complete
  description of the new boot time arguments, and how to use them. A
  description of how this parameter can be set and stored in the kernel
  image via `rdev' is also described.


  3.2.5.  The `ramdisk=' Argument (obsolete)

  (NOTE: This argument is obsolete, and should not be used except on
  kernels v1.3.47 and older. The commands that should be used for the
  ramdisk device are documented above.)

  This specifies the size in kB of the RAM disk device.  For example, if
  one wished to have a root filesystem on a 1.44MB floppy loaded into
  the RAM disk device, they would use:


  ______________________________________________________________________
          ramdisk=1440
  ______________________________________________________________________



  This is one of the few kernel boot arguments that has its default
  stored in the kernel image, and which can thus be altered with the
  rdev utility.



  3.2.6.  The `noinitrd' (initial RAM disk) Argument

  The v2.x and newer kernels have a feature where the root filesystem
  can be initially a RAM disk, and the kernel executes /linuxrc on that
  RAM image. This feature is typically used to allow loading of modules
  needed to mount the real root filesystem (e.g. load the SCSI driver
  modules stored in the RAM disk image, and then mount the real root
  filesystem on a SCSI disk.)

  The actual `noinitrd' argument determines what happens to the initrd
  data after the kernel has booted.  When specified, instead of
  converting it to a RAM disk, it is accessible via /dev/initrd, which
  can be read once before the RAM is released back to the system. For
  full details on using the initial RAM disk, please consult
  linux/Documentation/initrd.txt. In addition, the most recent versions
  of LILO and LOADLIN should have additional useful information.


  3.3.  Boot Arguments Related to Memory Handling

  The following arguments alter how Linux detects or handles the
  physical and virtual memory of your system.


  3.3.1.  The `mem=' Argument

  This argument has two purposes: The original purpose was to specify
  the amount of installed memory (or a value less than that if you
  wanted to limit the amount of memory available to linux). The second
  (and hardly used) purpose is to specify mem=nopentium which tells the
  Linux kernel to not use the 4MB page table performance feature.

  The original BIOS call defined in the PC specification  that returns
  the amount of installed memory was only designed to be able to report
  up to 64MB. (Yes, another lack of foresight, just like the 1024
  cylinder disks... sigh.) Linux uses this BIOS call at boot to
  determine how much memory is installed.  If you have more than 64MB of
  RAM installed, you can use this boot argument to tell Linux how much
  memory you have.  Here is a quote from Linus on the usage of the mem=
  parameter.

  ``The kernel will accept any `mem=xx' parameter you give it, and if it
  turns out that you lied to it, it will crash horribly sooner or later.
  The parameter indicates the highest addressable RAM address, so
  `mem=0x1000000' means you have 16MB of memory, for example.  For a
  96MB machine this would be `mem=0x6000000'.  If you tell Linux that it
  has more memory than it actually does have, bad things will happen:
  maybe not at once, but surely eventually.''

  Note that the argument does not have to be in hex, and the suffixes
  `k' and `M' (case insensitive) can be used to specify kilobytes and
  Megabytes, respectively. (A `k' will cause a 10 bit shift on your
  value, and a `M' will cause a 20 bit shift.)  A typical example for a
  128MB machine would be "mem=128m".


  3.3.2.  The `swap=' Argument

  This allows the user to tune some of the virtual memory (VM)
  parameters that are related to swapping to disk. It accepts the
  following eight parameters:


  ______________________________________________________________________
          MAX_PAGE_AGE
          PAGE_ADVANCE
          PAGE_DECLINE
          PAGE_INITIAL_AGE
          AGE_CLUSTER_FRACT
          AGE_CLUSTER_MIN
          PAGEOUT_WEIGHT
          BUFFEROUT_WEIGHT
  ______________________________________________________________________



  Interested hackers are advised to have a read of linux/mm/swap.c and
  also make note of the goodies in /proc/sys/vm. Kernels come with some
  useful documentation on this in the linux/Documentation/vm/ directory.


  3.3.3.  The `buff=' Argument

  Similar to the `swap=' argument, this allows the user to tune some of
  the parameters related to buffer memory management.  It accepts the
  following six parameters:


  ______________________________________________________________________
          MAX_BUFF_AGE
          BUFF_ADVANCE
          BUFF_DECLINE
          BUFF_INITIAL_AGE
          BUFFEROUT_WEIGHT
          BUFFERMEM_GRACE
  ______________________________________________________________________



  Interested hackers are advised to have a read of linux/mm/swap.c and
  also make note of the goodies in /proc/sys/vm.  Kernels come with some
  useful documentation on this in the linux/Documentation/vm/ directory.


  3.4.  Boot Arguments for NFS Root Filesystem

  Linux supports systems such as diskless workstations via having their
  root filesystem as NFS (Network FileSystem).  These arguments are used
  to tell the diskless workstation which machine it is to get its system
  from. Also note that the argument root=/dev/nfs is required. Detailed
  information on using an NFS root fs is in the file
  linux/Documentation/nfsroot.txt. You should read that file, as the
  following is only a quick summary taken directly from that file.


  3.4.1.  The `nfsroot=' Argument

  This argument tells the kernel which machine, what directory and what
  NFS options to use for the root filesystem. The form of the argument
  is as follows:


  ______________________________________________________________________
          nfsroot=[<server-ip>:]<root-dir>[,<nfs-options>]
  ______________________________________________________________________



  If the nfsroot parameter is not given on the command line, the default
  `/tftpboot/%s' will be used. The other options are as follows:

  <server-ip> -- Specifies the IP address of the NFS server. If this
  field is not given, the default address as determined by the nfsaddrs
  variable (see below) is used. One use of this parameter is for example
  to allow using different servers for RARP and NFS. Usually you can
  leave this blank.

  <root-dir> -- Name of the directory on the server to mount as root. If
  there is a `%s' token in the string, the token will be replaced by the
  ASCII-representation of the client's IP address.

  <nfs-options> -- Standard NFS options. All options are separated by
  commas.  If the options field is not given, the following defaults
  will be used:


          port            = as given by server portmap daemon
          rsize           = 1024
          wsize           = 1024
          timeo           = 7
          retrans         = 3
          acregmin        = 3
          acregmax        = 60
          acdirmin        = 30
          acdirmax        = 60
          flags           = hard, nointr, noposix, cto, ac



  3.4.2.  The `nfsaddrs=' Argument

  This boot argument sets up the various network interface addresses
  that are required to communicate over the network. If this argument is
  not given, then the kernel tries to use RARP and/or BOOTP to figure
  out these parameters. The form is as follows:


  ______________________________________________________________________
          nfsaddrs=<my-ip>:<serv-ip>:<gw-ip>:<netmask>:<name>:<dev>:<auto>
  ______________________________________________________________________



  <my-ip> -- IP address of the client. If empty, the address will either
  be determined by RARP or BOOTP. What protocol is used de- pends on
  what has been enabled during kernel configuration and on the <auto>
  parameter. If this parameter is not empty, neither RARP nor BOOTP will
  be used.

  <serv-ip> -- IP address of the NFS server. If RARP is used to
  determine the client address and this parameter is NOT empty only
  replies from the specified server are accepted. To use different RARP
  and NFS server, specify your RARP server here (or leave it blank), and
  specify your NFS server in the nfsroot parameter (see above). If this
  entry is blank the address of the server is used which answered the
  RARP or BOOTP request.

  <gw-ip> -- IP address of a gateway if the server in on a different
  subnet. If this entry is empty no gateway is used and the server is
  assumed to be on the local network, unless a value has been received
  by BOOTP.

  <netmask> -- Netmask for local network interface. If this is empty,
  the netmask is derived from the client IP address, unless a value has
  been received by BOOTP.

  <name> -- Name of the client. If empty, the client IP address is used
  in ASCII-notation, or the value received by BOOTP.

  <dev> -- Name of network device to use. If this is empty, all devices
  are used for RARP requests, and the first one found for BOOTP. For NFS
  the device is used on which either RARP or BOOTP replies have been
  received. If you only have one device you can safely leave this blank.

  <auto> -- Method to use for autoconfiguration. If this is either
  `rarp' or `bootp' the specified protocol is being used.  If the value
  is `both' or empty, both protocols are used so far as they have been
  enabled during kernel configuration Using 'none' means no
  autoconfiguration. In this case you have to specify all necessary
  values in the fields before.

  The <auto> parameter can appear alone as the value to the nfsaddrs
  parameter (without all the `:' characters before) in which case
  autoconfiguration is used. However, the `none' value is not available
  in that case.


  3.5.  Other Misc. Kernel Boot Arguments

  These various boot arguments let the user tune certain internal kernel
  parameters.


  3.5.1.  The `debug' Argument

  The kernel communicates important (and not-so important) messages to
  the operator via the printk() function.  If the message is considered
  important, the printk() function will put a copy on the present
  console as well as handing it off to the klogd() facility so that it
  gets logged to disk. The reason for printing important messages to the
  console as well as logging them to disk is because under unfortunate
  circumstances (e.g. a disk failure) the message won't make it to disk
  and will be lost.

  The threshold for what is and what isn't considered important is set
  by the console_loglevel variable. The default is to log anything more
  important than DEBUG (level 7) to the console. (These levels are
  defined in the include file kernel.h) Specifying debug as a boot
  argument will set the console loglevel to 10, so that all kernel
  messages appear on the console.

  The console loglevel can usually also be set at run time via an option
  to the klogd() program. Check the man page for the version installed
  on your system to see how to do this.


  3.5.2.  The `init=' Argument

  The kernel defaults to starting the `init' program at boot, which then
  takes care of setting up the computer for users via launching getty
  programs, running `rc' scripts and the like.  The kernel first looks
  for /sbin/init, then /etc/init (depreciated), and as a last resort, it
  will try to use /bin/sh (possibly on /etc/rc).  If for example, your
  init program got corrupted and thus stopped you from being able to
  boot, you could simply use the boot prompt init=/bin/sh which would
  drop you directly into a shell at boot, allowing you to replace the
  corrupted program.


  3.5.3.  The `kbd-reset' Argument

  Normally on i386 based machines, the Linux kernel does not reset the
  keyboard controller at boot, since the BIOS is supposed to do this.
  But as usual, not all machines do what they should. Supplying this
  option may help if you are having problems with your keyboard
  behaviour.  It simply forces a reset at initialization time. (Some
  have argued that this should be the default behaviour anyways).


  3.5.4.  The `maxcpus=' Argument

  The number given with this argument limits the maximum number of CPUs
  activated in SMP mode.  Using a value of 0 is equivalent to the nosmp
  option.


  3.5.5.  The `mca-pentium' Argument

  The IBM model 95 Microchannel machines seem to lock up on the test
  that Linux usually does to detect the type of math chip coupling.
  Since all Pentium chips have a built in math processor, this test (and
  the lock up problem) can be avoided by using this boot option.


  3.5.6.  The `md=' Argument

  If your root filesystem is on a Multiple Device then you can use this
  (assuming you compiled in boot support) to tell the kernel the
  multiple device layout. The format (from the file
  linux/Documentation/md.txt) is:

  md=md_device_num,raid_level,chunk_size_factor,fault_level,dev0,dev1,...,devN

  Where md_device_num is the number of the md device, i.e. 0 means md0,
  1 means md1, etc.  For raid_level, use -1 for linear mode and 0 for
  striped mode.  Other modes are currently unsupported.  The
  chunk_size_factor is for  raid-0 and raid-1 only and sets the chunk
  size as PAGE_SIZE shifted left the specified amount.  The fault_level
  is only for raid-1 and sets the maximum fault number to the specified
  number.  (Currently unsupported due to lack of boot support for
  raid1.)  The dev0-devN are a commaseparated list of the devices that
  make up the individual md device: e.g. /dev/hda1,/dev/hdc1,/dev/sda1



  3.5.7.  The `no387' Argument

  Some i387 coprocessor chips have bugs that show up when used in 32 bit
  protected mode. For example, some of the early ULSI-387 chips would
  cause solid lockups while performing floating point calculations,
  apparently due to a bug in the FRSAV/FRRESTOR instructions.  Using the
  `no387' boot argument causes Linux to ignore the math coprocessor even
  if you have one. Of course you must then have your kernel compiled
  with math emulation support! This may also be useful if you have one
  of those really old 386 machines that could use an 80287 FPU, as Linux
  can't use an 80287.


  3.5.8.  The `no-hlt' Argument

  The i386 (and successors thereof) family of CPUs have a `hlt'
  instruction which tells the CPU that nothing is going to happen until
  an external device (keyboard, modem, disk, etc.) calls upon the CPU to
  do a task. This allows the CPU to enter a `low-power' mode where it
  sits like a zombie until an external device wakes it up (usually via
  an interrupt).  Some of the early i486DX-100 chips had a problem  with
  the `hlt' instruction, in that they couldn't reliably return to
  operating mode after this instruction was used. Using the `no-hlt'
  instruction tells Linux to just run an infinite loop when there is
  nothing else to do, and to not halt your CPU when there is no
  activity. This allows people with these broken chips to use Linux,
  although they would be well advised to seek a replacement through a
  warranty where possible.


  3.5.9.  The `no-scroll' Argument

  Using this argument at boot disables scrolling features that make it
  difficult to use Braille terminals.


  3.5.10.  The `noapic' Argument

  Using this option tells a SMP kernel to not use some of the advanced
  features of the interrupt controller on multi processor machines. See
  linux/Documentation/IO-APIC.txt for more information.


  3.5.11.  The `nosmp' Argument

  Use of this option will tell a SMP kernel on a SMP machine to operate
  single processor.  Typically only used for debugging and determining
  if a particular problem is SMP related.


  3.5.12.  The `panic=' Argument

  In the unlikely event of a kernel panic (i.e. an internal error that
  has been detected by the kernel, and which the kernel decides is
  serious enough to moan loudly and then halt everything), the default
  behaviour is to just sit there until someone comes along and notices
  the panic message on the screen and reboots the machine.  However if a
  machine is running unattended in an isolated location it may be
  desirable for it to automatically reset itself so that the machine
  comes back on line. For example, using panic=30 at boot would cause
  the kernel to try and reboot itself 30 seconds after the kernel panic
  happened. A value of zero gives the default behaviour, which is to
  wait forever.

  Note that this timeout value can also be read and set via the
  /proc/sys/kernel/panic sysctl interface.
  3.5.13.  The `pci=' Argument



  3.5.14.  The `pirq=' Argument

  Using this option tells a SMP kernel information on the PCI slot
  versus IRQ settings for SMP motherboards which are unknown (or known
  to be blacklisted).  See linux/Documentation/IO-APIC.txt for more
  information.


  3.5.15.  The `profile=' Argument

  Kernel developers can enable an option that allows them to profile how
  and where the kernel is spending its CPU cycles in an effort to
  maximize efficiency and performance. This option lets you set the
  profile shift count at boot. Typically it is set to two. You can also
  compile your kernel with profiling enabled by default. In either case,
  you need a tool such as readprofile.c that can make use of the
  /proc/profile output.


  3.5.16.  The `reboot=' Argument

  This option controls the type of reboot that Linux will do when it
  resets the computer (typically via /sbin/init handling a Control-Alt-
  Delete). The default as of v2.0 kernels is to do a `cold' reboot (i.e.
  full reset, BIOS does memory check, etc.) instead of a `warm' reboot
  (i.e. no full reset, no memory check). It was changed to be cold by
  default since that tends to work on cheap/broken hardware that fails
  to reboot when a warm reboot is requested. To get the old behaviour
  (i.e. warm reboots) use reboot=w or in fact any word that starts with
  w will work.

  Why would you bother? Some disk controllers with cache memory on board
  can sense a warm reboot, and flush any cached data to disk. Upon a
  cold boot, the card may be reset and the write-back data in your cache
  card's memory is lost. Others have reported systems that take a long
  time to go through the memory check, and/or SCSI BIOSes that take
  longer to initialize on a cold boot as a good reason to use warm
  reboots.


  3.5.17.  The `reserve=' Argument

  This is used to protect I/O port regions from probes.  The form of the
  command is:


       reserve=iobase,extent[,iobase,extent]...


  In some machines it may be necessary to prevent device drivers from
  checking for devices (auto-probing) in a specific region. This may be
  because of poorly designed hardware that causes the boot to freeze
  (such as some ethercards), hardware that is mistakenly identified,
  hardware whose state is changed by an earlier probe, or merely
  hardware you don't want the kernel to initialize.

  The reserve boot-time argument addresses this problem by specifying an
  I/O port region that shouldn't be probed. That region is reserved in
  the kernel's port registration table as if a device has already been
  found in that region (with the name reserved).  Note that this
  mechanism shouldn't be necessary on most machines.  Only when there is
  a problem or special case would it be necessary to use this.
  The I/O ports in the specified region are protected against device
  probes that do a check_region() prior to probing blindly into a region
  of I/O space. This was put in to be used when some driver was hanging
  on a NE2000, or misidentifying some other device as its own.  A
  correct device driver shouldn't probe a reserved region, unless
  another boot argument explicitly specifies that it do so.  This
  implies that reserve will most often be used with some other boot
  argument. Hence if you specify a reserve region to protect a specific
  device, you must generally specify an explicit probe for that device.
  Most drivers ignore the port registration table if they are given an
  explicit address.

  For example, the boot line


  ______________________________________________________________________
          reserve=0x300,32  blah=0x300
  ______________________________________________________________________



  keeps all device drivers except the driver for `blah' from probing
  0x300-0x31f.

  As usual with boot-time specifiers there is an 11 parameter limit,
  thus you can only specify 5 reserved regions per reserve keyword.
  Multiple reserve specifiers will work if you have an unusually
  complicated request.



  3.5.18.  The `vga=' Argument

  Note that this is not really a boot argument. It is an option that is
  interpreted by LILO and not by the kernel like all the other boot
  arguments are. However its use has become so common that it deserves a
  mention here. It can also be set via using rdev -v or equivalently
  vidmode on the vmlinuz file.  This allows the setup code to use the
  video BIOS to change the default display mode before actually booting
  the Linux kernel. Typical modes are 80x50, 132x44 and so on. The best
  way to use this option is to start with vga=ask which will prompt you
  with a list of various modes that you can use with your video adapter
  before booting the kernel. Once you have the number from the above
  list that you want to use, you can later put it in place of the `ask'.
  For more information, please see the file linux/Documentation/svga.txt
  that comes with all recent kernel versions.

  Note that newer kernels (v2.1 and up) have the setup code that changes
  the video mode as an option, listed as Video mode selection support so
  you need to enable this option if you want to use this feature.


  4.  Boot Arguments to Control PCI Bus Behaviour (`pci=')

  The `pci=' argument (not avail. in v2.0 kernels) can be used to change
  the behaviour of PCI bus device probing and device behaviour. Firstly
  the file linux/drivers/pci/pci.c checks for architecture independent
  pci= options.  The remaining allowed arguments are handled in
  linux/arch/???/kernel/bios32.c and are listed below for ???=i386.


  4.1.  The `pci=bios' and `pci=nobios' Arguments

  These are used to set/clear the flag indicating that the PCI probing
  is to take place via the PCI BIOS.  The default is to use the BIOS.

  4.2.  The `pci=conf1' and `pci=conf2' Arguments

  If PCI direct mode is enabled, the use of these enables either
  configuration Type 1 or Type 2.  These implicitly clear the PCI BIOS
  probe flag (i.e. `pci=nobios') too.


  4.3.  The `pci=io=' Argument

  If you get a message like PCI: Unassigned IO space for.../ then you
  may need to supply an I/O value with this option.  From the source:

  ``Several BIOS'es forget to assign addresses to I/O ranges.  We try to
  fix it here, expecting there are free addresses starting with 0x5800.
  Ugly, but until we come with better resource management, it's the only
  simple solution.''


  4.4.  The `pci=nopeer' Argument

  This disables the default peer bridge fixup, which according to the
  source does the following:

  ``In case there are peer host bridges, scan bus behind each of them.
  Although several sources claim that the host bridges should have
  header type 1 and be assigned a bus number as for PCI2PCI bridges, the
  reality doesn't pass this test and the bus number is usually set by
  BIOS to the first free value.''


  4.5.  The `pci=nosort' Argument

  Using this argument instructs the kernel to not sort the PCI devices
  during the probing phase.


  4.6.  The `pci=off' Argument

  Using this option disables all PCI bus probing. Any device drivers
  that make use of PCI functions to find and initialize hardware will
  most likely fail to work.


  4.7.  The `pci=reverse' Argument

  This option will reverse the ordering of the PCI devices on that PCI
  bus.


  5.  Boot Arguments for Video Frame Buffer Drivers

  The `video=' argument (not avail. in v2.0 kernels) is used when the
  frame buffer device abstraction layer is built into the kernel. If
  that sounds complicated, well it isn't really too bad.  It basically
  means that instead of having a different video program (the X11R6
  server) for each brand of video card (e.g. XF86_S3, XF86_SVGA, ...),
  the kernel would have a built in driver available for each video card
  and export a single interface for the video program so that only one
  X11R6 server (XF86_FBDev) would be required.  This is similar to how
  networking is now - the kernel has drivers available for each brand of
  network card and exports a single network interface so that just one
  version of a network program (like Netscape) will work for all
  systems, regardless of the underlying brand of network card.

  The typical format of this argument is video=name:option1,option2,...
  where name is the name of a generic option or of a frame buffer
  driver.  The video= option is passed from linux/init/main.c into
  linux/drivers/video/fbmem.c for further processing.  Here it is
  checked for some generic options before trying to match to a known
  driver name. Once a driver name match is made, the comma separated
  option list is then passed into that particular driver for final
  processing. The list of valid driver names can be found by reading
  down the fb_drivers array in the file fbmem.c mentioned above.

  Information on the options that each driver supports will eventually
  be found in linux/Documentation/fb/ but currently (v2.2) only a few
  are described there.  Unfortunately the number of video drivers and
  the number of options for each one is content for another document
  itself and hence too much to list here.

  If there is no Documentation file for your card, you will have to get
  the option information directly from the driver. Go to
  linux/drivers/video/ and look in the appropriate ???fb.c file (the ???
  will be based on the card name).  In there, search for a function with
  _setup in its name and you should see what options the driver tries to
  match, such as font or mode or...


  5.1.  The `video=map:...' Argument

  This option is used to set/override the console to frame buffer device
  mapping. A comma separated list of numbers sets the mapping, with the
  value of option N taken to be the frame buffer device number for
  console N.


  5.2.  The `video=scrollback:...' Argument

  A number after the colon will set the size of memory allocated for the
  scrollback buffer. (Use Shift and Page Up or Page Down keys to
  scroll.)  A suffix of `k' or `K' after the number will indicate that
  the number is to be interpreted as kilobytes instead of bytes.


  5.3.  The `video=vc:...' Argument


  A number, or a range of numbers (e.g. video=vc:2-5) will specify the
  first, or the first and last frame buffer virtual console(s). The use
  of this option also has the effect of setting the frame buffer console
  to not be the default console.


  6.  Boot Arguments for SCSI Peripherals.

  This section contains the descriptions of the boot args that are used
  for passing information about the installed SCSI host adapters, and
  SCSI devices.


  6.1.  Arguments for Mid-level Drivers

  The mid level drivers handle things like disks, CD-ROMs and tapes
  without getting into host adapter specifics.


  6.1.1.  Maximum Probed LUNs (`max_scsi_luns=')

  Each SCSI device can have a number of `sub-devices' contained within
  itself. The most common example is any of the SCSI CD-ROMs that handle
  more than one disk at a time.  Each CD is addressed as a `Logical Unit
  Number' (LUN) of that particular device. But most devices, such as
  hard disks, tape drives and such are only one device, and will be
  assigned to LUN zero.

  The problem arises with single LUN devices with bad firmware.  Some
  poorly designed SCSI devices (old and unfortunately new) can not
  handle being probed for LUNs not equal to zero. They will respond by
  locking up, and possibly taking the whole SCSI bus down with them.

  The kernel has a configuration option that allows you to set the
  maximum number of probed LUNs. The default is to only probe LUN zero,
  to avoid the problem described above.

  To specify the number of probed LUNs at boot, one enters
  `max_scsi_luns=n' as a boot arg, where n is a number between one and
  eight. To avoid problems as described above, one would use n=1 to
  avoid upsetting such broken devices


  6.1.2.  SCSI Logging (`scsi_logging=')

  Supplying a non-zero value to this boot argument turns on logging of
  all SCSI events (error, scan, mlqueue, mlcomplete, llqueue,
  llcomplete, hlqueue, hlcomplete).  Note that better control of which
  events are logged can be obtained via the /proc/scsi/scsi interface if
  you aren't interested in the events that take place at boot before the
  /proc/ filesystem is accessible.


  6.1.3.  Parameters for the SCSI Tape Driver (`st=')

  Some boot time configuration of the SCSI tape driver can be achieved
  by using the following:


  ______________________________________________________________________
          st=buf_size[,write_threshold[,max_bufs]]
  ______________________________________________________________________



  The first two numbers are specified in units of kB.  The default
  buf_size is 32kB, and the maximum size that can be specified is a
  ridiculous 16384kB.  The write_threshold is the value at which the
  buffer is committed to tape, with a default value of 30kB.  The
  maximum number of buffers varies with the number of drives detected,
  and has a default of two. An example usage would be:


  ______________________________________________________________________
          st=32,30,2
  ______________________________________________________________________



  Full details can be found in the README.st file that is in the scsi
  directory of the kernel source tree.


  6.2.  Arguments for SCSI Host Adapters

  General notation for this section:

  iobase -- the first I/O port that the SCSI host occupies.  These are
  specified in hexidecimal notation, and usually lie in the range from
  0x200 to 0x3ff.

  irq -- the hardware interrupt that the card is configured to use.
  Valid values will be dependent on the card in question, but will
  usually be 5, 7, 9, 10, 11, 12, and 15. The other values are usually
  used for common peripherals like IDE hard disks, floppies, serial
  ports, etc.

  dma -- the DMA (Direct Memory Access) channel that the card uses.
  Typically only applies to bus-mastering cards.  PCI and VLB cards are
  native bus-masters, and do not require and ISA DMA channel.

  scsi-id -- the ID that the host adapter uses to identify itself on the
  SCSI bus. Only some host adapters allow you to change this value, as
  most have it permanently specified internally. The usual default value
  is seven, but the Seagate and Future Domain TMC-950 boards use six.

  parity -- whether the SCSI host adapter expects the attached devices
  to supply a parity value with all information exchanges.  Specifying a
  one indicates parity checking is enabled, and a zero disables parity
  checking. Again, not all adapters will support selection of parity
  behaviour as a boot argument.


  6.2.1.  Adaptec aha151x, aha152x, aic6260, aic6360, SB16-SCSI
  (`aha152x=')

  The aha numbers refer to cards and the aic numbers refer to the actual
  SCSI chip on these type of cards, including the Soundblaster-16 SCSI.

  The probe code for these SCSI hosts looks for an installed BIOS, and
  if none is present, the probe will not find your card. Then you will
  have to use a boot argument of the form:


  ______________________________________________________________________
           aha152x=iobase[,irq[,scsi-id[,reconnect[,parity]]]]
  ______________________________________________________________________



  Note that if the driver was compiled with debugging enabled, a sixth
  value can be specified to set the debug level.

  All the parameters are as described at the top of this section, and
  the reconnect value will allow device disconnect/reconnect if a non-
  zero value is used. An example usage is as follows:


  ______________________________________________________________________
          aha152x=0x340,11,7,1
  ______________________________________________________________________



  Note that the parameters must be specified in order, meaning that if
  you want to specify a parity setting, then you will have to specify an
  iobase, irq, scsi-id and reconnect value as well.


  6.2.2.  Adaptec aha154x (`aha1542=')

  These are the aha154x series cards. The aha1542 series cards have an
  i82077 floppy controller onboard, while the aha1540 series cards do
  not. These are busmastering cards, and have parameters to set the
  ``fairness'' that is used to share the bus with other devices. The
  boot argument looks like the following.

  ______________________________________________________________________
          aha1542=iobase[,buson,busoff[,dmaspeed]]
  ______________________________________________________________________



  Valid iobase values are usually one of: 0x130, 0x134, 0x230, 0x234,
  0x330, 0x334.  Clone cards may permit other values.

  The buson, busoff values refer to the number of microseconds that the
  card dominates the ISA bus. The defaults are 11us on, and 4us off, so
  that other cards (such as an ISA LANCE Ethernet card) have a chance to
  get access to the ISA bus.

  The dmaspeed value refers to the rate (in MB/s) at which the DMA
  (Direct Memory Access) transfers proceed at. The default is 5MB/s.
  Newer revision cards allow you to select this value as part of the
  soft-configuration, older cards use jumpers. You can use values up to
  10MB/s assuming that your motherboard is capable of handling it.
  Experiment with caution if using values over 5MB/s.


  6.2.3.  Adaptec aha274x, aha284x, aic7xxx (`aic7xxx=')

  These boards can accept an argument of the form:


  ______________________________________________________________________
          aic7xxx=extended,no_reset
  ______________________________________________________________________



  The extended value, if non-zero, indicates that extended translation
  for large disks is enabled. The no_reset value, if non-zero, tells the
  driver not to reset the SCSI bus when setting up the host adaptor at
  boot.


  6.2.4.  AdvanSys SCSI Host Adaptors (`advansys=')

  The AdvanSys driver can accept up to four i/o addresses that will be
  probed for an AdvanSys SCSI card. Note that these values (if used) do
  not effect EISA or PCI probing in any way.  They are only used for
  probing ISA and VLB cards.  In addition, if the driver has been
  compiled with debugging enabled, the level of debugging output can be
  set by adding an 0xdeb[0-f] parameter. The 0-f allows setting the
  level of the debugging messages to any of 16 levels of verbosity.


  6.2.5.  Always IN2000 Host Adaptor (`in2000=')

  Unlike other SCSI host boot arguments, the IN2000 driver uses ASCII
  string prefixes for most of its integer arguments. Here is a list of
  the supported arguments:

  ioport:addr -- Where addr is IO address of a (usually ROM-less) card.

  noreset -- No optional args. Prevents SCSI bus reset at boot time.

  nosync:x -- x is a bitmask where the 1st 7 bits correspond with the 7
  possible SCSI devices (bit 0 for device #0, etc).  Set a bit to
  PREVENT sync negotiation on that device.  The driver default is sync
  DISABLED on all devices.


  period:ns -- ns is the minimum # of nanoseconds in a SCSI data
  transfer period. Default is 500; acceptable values are 250 to 1000.

  disconnect:x -- x = 0 to never allow disconnects, 2 to always allow
  them.  x = 1 does 'adaptive' disconnects, which is the default and
  generally the best choice.

  debug:x If `DEBUGGING_ON' is defined, x is a bitmask that causes
  various types of debug output to printed - see the DB_xxx defines in
  in2000.h

  proc:x -- If `PROC_INTERFACE' is defined, x is a bitmask that
  determines how the /proc interface works and what it does - see the
  PR_xxx defines in in2000.h


  Some example usages are listed below:


  ______________________________________________________________________
          in2000=ioport:0x220,noreset
          in2000=period:250,disconnect:2,nosync:0x03
          in2000=debug:0x1e
          in2000=proc:3
  ______________________________________________________________________



  6.2.6.  AMD AM53C974 based hardware (`AM53C974=')

  Unlike other drivers, this one does not use boot parameters to
  communicate i/o, IRQ or DMA channels. (Since the AM53C974 is a PCI
  device, there shouldn't be a need to do so.)  Instead, the parameters
  are used to communicate the transfer modes and rates that are to be
  used between the host and the target device. This is best described
  with an example:


  ______________________________________________________________________
          AM53C974=7,2,8,15
  ______________________________________________________________________



  This would be interpreted as follows: `For communication between the
  controller with SCSI-ID 7 and the device with SCSI-ID 2, a transfer
  rate of 8MHz in synchronous mode with max. 15 bytes offset should be
  negotiated.' More details can be found in the file
  linux/drivers/scsi/README.AM53C974


  6.2.7.  BusLogic SCSI Hosts with v1.2 kernels (`buslogic=')

  In older kernels, the buslogic driver accepts only one parameter, that
  being the I/O base. It expects that to be one of the following valid
  values: 0x130, 0x134, 0x230, 0x234, 0x330, 0x334.


  6.2.8.  BusLogic SCSI Hosts with v2.x kernels (`BusLogic=')

  With v2.x kernels, the BusLogic driver accepts many parameters.  (Note
  the case in the above; upper case B and L!!!).  There are simply too
  many to list here.  A complete description is tucked away in the
  middle of the driver linux/drivers/scsi/BusLogic.c and searching for
  the string BusLogic= will put you right on it.


  6.2.9.  EATA SCSI Cards (`eata=')

  As of late v2.0 kernels, the EATA drivers will accept a boot argument
  to specify the i/o base(s) to be probed. It is of the form:


  ______________________________________________________________________
          eata=iobase1[,iobase2][,iobase3]...[,iobaseN]
  ______________________________________________________________________



  The driver will probe the addresses in the order that they are listed.



  6.2.10.  Future Domain TMC-8xx, TMC-950 (`tmc8xx=')

  The probe code for these SCSI hosts looks for an installed BIOS, and
  if none is present, the probe will not find your card. Or, if the
  signature string of your BIOS is not recognized then it will also not
  be found. In either case, you will then have to use a boot argument of
  the form:


  ______________________________________________________________________
          tmc8xx=mem_base,irq
  ______________________________________________________________________



  The mem_base value is the value of the memory mapped I/O region that
  the card uses. This will usually be one of the following values:
  0xc8000, 0xca000, 0xcc000, 0xce000, 0xdc000, 0xde000.


  6.2.11.  Future Domain TMC-16xx, TMC-3260, AHA-2920 (`fdomain=')

  The driver detects these cards according to a list of known BIOS ROM
  signatures. For a full list of known BIOS revisions, please see
  linux/drivers/scsi/fdomain.c as it has a lot of information at the top
  of that file. If your BIOS is not known to the driver, you can use an
  override of the form:


  ______________________________________________________________________
          fdomain=iobase,irq[,scsi_id]
  ______________________________________________________________________



  6.2.12.  IOMEGA Parallel Port / ZIP drive (`ppa=')

  This driver is for the IOMEGA Parallel Port SCSI adapter which is
  embedded into the IOMEGA ZIP drives. It may also work with the
  original IOMEGA PPA3 device. The boot argument for this driver is of
  the form:



  ______________________________________________________________________
          ppa=iobase,speed_high,speed_low,nybble
  ______________________________________________________________________



  with all but iobase being optionally specified values. If you wish to
  alter any of the three optional parameters, you are advised to read
  linux/drivers/scsi/README.ppa for details of what they control.


  6.2.13.  NCR5380 based controllers (`ncr5380=')

  Depending on your board, the 5380 can be either i/o mapped or memory
  mapped. (An address below 0x400 usually implies i/o mapping, but PCI
  and EISA hardware use i/o addresses above 0x3ff.) In either case, you
  specify the address, the IRQ value and the DMA channel value. An
  example for an i/o mapped card would be: ncr5380=0x350,5,3. If the
  card doesn't use interrupts, then an IRQ value of 255 (0xff) will
  disable interrupts. An IRQ value of 254 means to autoprobe. More
  details can be found in the file linux/drivers/scsi/README.g_NCR5380


  6.2.14.  NCR53c400 based controllers (`ncr53c400=')

  The generic 53c400 support is done with the same driver as the generic
  5380 support mentioned above. The boot argument is identical to the
  above with the exception that no DMA channel is used by the 53c400.


  6.2.15.  NCR53c406a based controllers (`ncr53c406a=')

  This driver uses a boot argument of the form:


  ______________________________________________________________________
          ncr53c406a=PORTBASE,IRQ,FASTPIO
  ______________________________________________________________________



  where the IRQ and FASTPIO parameters are optional. An interrupt value
  of zero disables the use of interrupts. Using a value of one for the
  FASTPIO parameter enables the use of insl and outsl instructions
  instead of the single-byte inb and outb instructions. The driver can
  also use DMA as a compile-time option.


  6.2.16.  Pro Audio Spectrum (`pas16=')

  The PAS16 uses a NCR5380 SCSI chip, and newer models support jumper-
  less configuration. The boot argument is of the form:


  ______________________________________________________________________
          pas16=iobase,irq
  ______________________________________________________________________



  The only difference is that you can specify an IRQ value of 255, which
  will tell the driver to work without using interrupts, albeit at a
  performance loss. The iobase is usually 0x388.



  6.2.17.  Seagate ST-0x (`st0x=')

  The probe code for these SCSI hosts looks for an installed BIOS, and
  if none is present, the probe will not find your card. Or, if the
  signature string of your BIOS is not recognized then it will also not
  be found. In either case, you will then have to use a boot argument of
  the form:


  ______________________________________________________________________
          st0x=mem_base,irq
  ______________________________________________________________________



  The mem_base value is the value of the memory mapped I/O region that
  the card uses. This will usually be one of the following values:
  0xc8000, 0xca000, 0xcc000, 0xce000, 0xdc000, 0xde000.


  6.2.18.  Trantor T128 (`t128=')

  These cards are also based on the NCR5380 chip, and accept the
  following options:


  ______________________________________________________________________
          t128=mem_base,irq
  ______________________________________________________________________



  The valid values for mem_base are as follows: 0xcc000, 0xc8000,
  0xdc000, 0xd8000.


  6.2.19.  Ultrastor SCSI cards (`u14-34f=')

  Note that there appears to be two independent drivers for this card,
  namely CONFIG_SCSI_U14_34F that uses u14-34f.c and
  CONFIG_SCSI_ULTRASTOR that uses ultrastor.c. It is the u14-34f one
  that (as of late v2.0 kernels) accepts a boot argument of the form:


  ______________________________________________________________________
          u14-34f=iobase1[,iobase2][,iobase3]...[,iobaseN]
  ______________________________________________________________________



  The driver will probe the addresses in the order that they are listed.


  6.2.20.  Western Digital WD7000 cards (`wd7000=')

  The driver probe for the wd7000 looks for a known BIOS ROM string and
  knows about a few standard configuration settings.  If it doesn't come
  up with the correct values for your card, or you have an unrecognized
  BIOS version, you can use a boot argument of the form:


  ______________________________________________________________________
          wd7000=irq,dma,iobase
  ______________________________________________________________________


  6.3.  SCSI Host Adapters that don't Accept Boot Args

  At present, the following SCSI cards do not make use of any boot-time
  parameters. In some cases, you can hard-wire values by directly
  editing the driver itself, if required.


          Adaptec aha1740 (EISA probing),
          NCR53c7xx,8xx (PCI, both drivers)
          Qlogic Fast (0x230, 0x330)
          Qlogic ISP (PCI)



  7.  Hard Disks

  This section lists all the boot args associated with standard MFM/RLL,
  ST-506, XT, and IDE disk drive devices.  Note that both the IDE and
  the generic ST-506 HD driver both accept the `hd=' option.


  7.1.  IDE Disk/CD-ROM Driver Parameters

  The IDE driver accepts a number of parameters, which range from disk
  geometry specifications, to support for advanced or broken controller
  chips. The following is a summary of all the possible boot arguments.
  For full details, you really should consult the file ide.txt in the
  linux/Documentation directory, from which this summary was extracted.


  ______________________________________________________________________

   "hdx="  is recognized for all "x" from "a" to "h", such as "hdc".
   "idex=" is recognized for all "x" from "0" to "3", such as "ide1".

   "hdx=noprobe"              : drive may be present, but do not probe for it
   "hdx=none"         : drive is NOT present, ignore cmos and do not probe
   "hdx=nowerr"               : ignore the WRERR_STAT bit on this drive
   "hdx=cdrom"                : drive is present, and is a cdrom drive
   "hdx=cyl,head,sect"        : disk drive is present, with specified geometry
   "hdx=autotune"             : driver will attempt to tune interface speed
                                  to the fastest PIO mode supported,
                                  if possible for this drive only.
                                  Not fully supported by all chipset types,
                                  and quite likely to cause trouble with
                                  older/odd IDE drives.

   "idex=noprobe"             : do not attempt to access/use this interface
   "idex=base"                : probe for an interface at the addr specified,
                                  where "base" is usually 0x1f0 or 0x170
                                  and "ctl" is assumed to be "base"+0x206
   "idex=base,ctl"    : specify both base and ctl
   "idex=base,ctl,irq"        : specify base, ctl, and irq number
   "idex=autotune"    : driver will attempt to tune interface speed
                                  to the fastest PIO mode supported,
                                  for all drives on this interface.
                                  Not fully supported by all chipset types,
                                  and quite likely to cause trouble with
                                  older/odd IDE drives.
   "idex=noautotune"  : driver will NOT attempt to tune interface speed
                                  This is the default for most chipsets,
                                  except the cmd640.
   "idex=serialize"   : do not overlap operations on idex and ide(x^1)
  ______________________________________________________________________

  The following are valid ONLY on ide0, and the defaults for the
  base,ctl ports must not be altered.


  ______________________________________________________________________

   "ide0=dtc2278"             : probe/support DTC2278 interface
   "ide0=ht6560b"             : probe/support HT6560B interface
   "ide0=cmd640_vlb"  : *REQUIRED* for VLB cards with the CMD640 chip
                            (not for PCI -- automatically detected)
   "ide0=qd6580"              : probe/support qd6580 interface
   "ide0=ali14xx"             : probe/support ali14xx chipsets (ALI M1439/M1445)
   "ide0=umc8672"             : probe/support umc8672 chipsets
  ______________________________________________________________________



  Everything else is rejected with a "BAD OPTION" message.


  7.2.  Standard ST-506 Disk Driver Options (`hd=')

  The standard disk driver can accept geometry arguments for the disks
  similar to the IDE driver. Note however that it only expects three
  values (C/H/S) -- any more or any less and it will silently ignore
  you. Also, it only accepts `hd=' as an argument, i.e. `hda=', `hdb='
  and so on are not valid here. The format is as follows:


  ______________________________________________________________________
          hd=cyls,heads,sects
  ______________________________________________________________________



  If there are two disks installed, the above is repeated with the
  geometry parameters of the second disk.


  7.3.  XT Disk Driver Options (`xd=')

  If you are unfortunate enough to be using one of these old 8 bit cards
  that move data at a whopping 125kB/s then here is the scoop.  The
  probe code for these cards looks for an installed BIOS, and if none is
  present, the probe will not find your card. Or, if the signature
  string of your BIOS is not recognized then it will also not be found.
  In either case, you will then have to use a boot argument of the form:


  ______________________________________________________________________
          xd=type,irq,iobase,dma_chan
  ______________________________________________________________________



  The type value specifies the particular manufacturer of the card, and
  are as follows: 0=generic; 1=DTC; 2,3,4=Western Digital,
  5,6,7=Seagate; 8=OMTI. The only difference between multiple types from
  the same manufacturer is the BIOS string used for detection, which is
  not used if the type is specified.

  The xd_setup() function does no checking on the values, and assumes
  that you entered all four values. Don't disappoint it.  Here is an
  example usage for a WD1002 controller with the BIOS disabled/removed,
  using the `default' XT controller parameters:

  ______________________________________________________________________
          xd=2,5,0x320,3
  ______________________________________________________________________



  8.  CD-ROMs (Non-SCSI/ATAPI/IDE)

  This section lists all the possible boot args pertaining to CD-ROM
  devices. Note that this does not include SCSI or IDE/ATAPI CD-ROMs.
  See the appropriate section(s) for those types of CD-ROMs.

  Note that most of these CD-ROMs have documentation files that you
  should read, and they are all in one handy place:
  linux/Documentation/cdrom.


  8.1.  The Aztech Interface (`aztcd=')

  The syntax for this type of card is:


  ______________________________________________________________________
          aztcd=iobase[,magic_number]
  ______________________________________________________________________



  If you set the magic_number to 0x79 then the driver will try and run
  anyway in the event of an unknown firmware version. All other values
  are ignored.


  8.2.  The CDU-31A and CDU-33A Sony Interface (`cdu31a=')

  This CD-ROM interface is found on some of the Pro Audio Spectrum sound
  cards, and other Sony supplied interface cards.  The syntax is as
  follows:


  ______________________________________________________________________
          cdu31a=iobase,[irq[,is_pas_card]]
  ______________________________________________________________________



  Specifying an IRQ value of zero tells the driver that hardware
  interrupts aren't supported (as on some PAS cards). If your card
  supports interrupts, you should use them as it cuts down on the CPU
  usage of the driver.

  The `is_pas_card' should be entered as `PAS' if using a Pro Audio
  Spectrum card, and otherwise it should not be specified at all.


  8.3.  The CDU-535 Sony Interface (`sonycd535=')

  The syntax for this CD-ROM interface is:


  ______________________________________________________________________
          sonycd535=iobase[,irq]
  ______________________________________________________________________


  A zero can be used for the I/O base as a `placeholder' if one wishes
  to specify an IRQ value.


  8.4.  The GoldStar Interface (`gscd=')

  The syntax for this CD-ROM interface is:


  ______________________________________________________________________
          gscd=iobase
  ______________________________________________________________________



  8.5.  The ISP16 Interface (`isp16=')

  The syntax for this CD-ROM interface is:


  ______________________________________________________________________
          isp16=[port[,irq[,dma]]][[,]drive_type]
  ______________________________________________________________________



  Using a zero for irq or dma means that they are not used. The
  allowable values for drive_type are noisp16, Sanyo, Panasonic, Sony,
  and Mitsumi.  Using noisp16 disables the driver altogether.


  8.6.  The Mitsumi Standard Interface (`mcd=')

  The syntax for this CD-ROM interface is:


  ______________________________________________________________________
          mcd=iobase,[irq[,wait_value]]
  ______________________________________________________________________



  The wait_value is used as an internal timeout value for people who are
  having problems with their drive, and may or may not be implemented
  depending on a compile time DEFINE.


  8.7.  The Mitsumi XA/MultiSession Interface (`mcdx=')

  At present this `experimental' driver has a setup function, but no
  parameters are implemented yet (as of 1.3.15).  This is for the same
  hardware as above, but the driver has extended features.


  8.8.  The Optics Storage Interface (`optcd=')

  The syntax for this type of card is:


  ______________________________________________________________________
          optcd=iobase
  ______________________________________________________________________



  8.9.  The Phillips CM206 Interface (`cm206=')

  The syntax for this type of card is:


  ______________________________________________________________________
          cm206=[iobase][,irq]
  ______________________________________________________________________



  The driver assumes numbers between 3 and 11 are IRQ values, and
  numbers between 0x300 and 0x370 are I/O ports, so you can specify one,
  or both numbers, in any order.  It also accepts `cm206=auto' to enable
  autoprobing.


  8.10.  The Sanyo Interface (`sjcd=')

  The syntax for this type of card is:


  ______________________________________________________________________
          sjcd=iobase[,irq[,dma_channel]]
  ______________________________________________________________________



  8.11.  The SoundBlaster Pro Interface (`sbpcd=')

  The syntax for this type of card is:


  ______________________________________________________________________
          sbpcd=iobase,type
  ______________________________________________________________________



  where type is one of the following (case sensitive) strings:
  `SoundBlaster', `LaserMate', or `SPEA'.  The I/O base is that of the
  CD-ROM interface, and not that of the sound portion of the card.


  9.  Serial and ISDN Drivers

  9.1.  The ICN ISDN driver (`icn=')

  This ISDN driver expects a boot argument of the form:


  ______________________________________________________________________
          icn=iobase,membase,icn_id1,icn_id2
  ______________________________________________________________________



  where iobase is the i/o port address of the card, membase is the
  shared memory base address of the card, and the two icn_id are unique
  ASCII string identifiers.



  9.2.  The PCBIT ISDN driver (`pcbit=')

  This boot argument takes integer pair arguments of the form:


  ______________________________________________________________________
          pcbit=membase1,irq1[,membase2,irq2]
  ______________________________________________________________________



  where membaseN is the shared memory base of the N'th card, and irqN is
  the interrupt setting of the N'th card. The default is IRQ 5 and
  membase 0xD0000.


  9.3.  The Teles ISDN driver (`teles=')

  This ISDN driver expects a boot argument of the form:


  ______________________________________________________________________
          teles=iobase,irq,membase,protocol,teles_id
  ______________________________________________________________________



  where iobase is the i/o port address of the card, membase is the
  shared memory base address of the card, irq is the interrupt channel
  the card uses, and teles_id is the unique ASCII string identifier.


  9.4.  The DigiBoard Driver (`digi=')

  The DigiBoard driver accepts a string of six comma separated
  identifiers or integers.  The 6 values in order are:


          Enable/Disable this card
          Type of card: PC/Xi(0), PC/Xe(1), PC/Xeve(2), PC/Xem(3)
          Enable/Disable alternate pin arrangement
          Number of ports on this card
          I/O Port where card is configured (in HEX if using string identifiers)
          Base of memory window (in HEX if using string identifiers)



  An example of a correct boot prompt argument (in both identifier and
  integer form) is:


  ______________________________________________________________________
          digi=E,PC/Xi,D,16,200,D0000
          digi=1,0,0,16,512,851968
  ______________________________________________________________________



  Note that the driver defaults to an i/o of 0x200 and a shared memory
  base of 0xD0000 in the absence of a digi= boot argument.  There is no
  autoprobing performed. More details can be found in the file
  linux/Documentation/digiboard.txt.



  9.5.  The RISCom/8 Multiport Serial Driver (`riscom8=')

  Up to four boards can be supported by supplying four unique i/o port
  values for each individual board installed.  Other details can be
  found in the file linux/Documentation/riscom8.txt.


  9.6.  The Baycom Serial/Parallel Radio Modem (`baycom=')

  The format of the boot argument for these devices is:


  ______________________________________________________________________
          baycom=modem,io,irq,options[,modem,io,irq,options]
  ______________________________________________________________________



  Using modem=1 means you have the ser12 device, modem=2 means you have
  the par96 device. Using options=0 means use hardware DCD, and
  options=1 means use software DCD. The io and irq are the i/o port base
  and interrupt settings as usual.  There is more details in the file
  README.baycom which is currently in the /linux/drivers/char/
  directory.


  10.  Other Hardware Devices

  Any other devices that didn't fit into any of the above categories got
  lumped together here.


  10.1.  Ethernet Devices (`ether=')

  Different drivers make use of different parameters, but they all at
  least share having an IRQ, an I/O port base value, and a name. In its
  most generic form, it looks something like this:


  ______________________________________________________________________
          ether=irq,iobase[,param_1[,param_2,...param_8]]],name
  ______________________________________________________________________



  The first non-numeric argument is taken as the name.  The param_n
  values (if applicable) usually have different meanings for each
  different card/driver.  Typical param_n values are used to specify
  things like shared memory address, interface selection, DMA channel
  and the like.

  The most common use of this parameter is to force probing for a second
  ethercard, as the default is to only probe for one. This can be
  accomplished with a simple:


  ______________________________________________________________________
          ether=0,0,eth1
  ______________________________________________________________________



  Note that the values of zero for the IRQ and I/O base in the above
  example tell the driver(s) to autoprobe.


  IMPORTANT NOTE TO MODULE USERS: The above will not force a probe for a
  second card if you are using the driver(s) as run time loadable
  modules (instead of having them complied into the kernel).  Most Linux
  distributions use a bare bones kernel combined with a large selection
  of modular drivers.  The ether= only applies to drivers compiled
  directly into the kernel.

  The Ethernet-HowTo has complete and extensive documentation on using
  multiple cards and on the card/driver specific implementation of the
  param_n values where used.  Interested readers should refer to the
  section in that document on their particular card for more complete
  information.  Ethernet-HowTo
  <http://metalab.unc.edu/mdw/HOWTO/Ethernet-HOWTO.html>


  10.2.  The Floppy Disk Driver (`floppy=')

  There are many floppy driver options, and they are all listed in
  README.fd in linux/drivers/block.  There are too many options in that
  file to list here. Instead, only those options that may be required to
  get a Linux install to proceed on less than normal hardware are
  reprinted here.

  floppy=0,daring Tells the floppy driver that your floppy controller
  should be used with caution (disables all daring operations).

  floppy=thinkpad Tells the floppy driver that you have a Thinkpad.
  Thinkpads use an inverted convention for the disk change line.

  floppy=nodma Tells the floppy driver not to use DMA for data
  transfers.  This is needed on HP Omnibooks, which don't have a
  workable DMA channel for the floppy driver. This option is also useful
  if you frequently get "Unable to allocate DMA memory" messages.  Use
  of `nodma' is not recommended if you have a FDC without a FIFO (8272A
  or 82072). 82072A and later are OK). The FDC model is reported at
  boot.  You also need at least a 486 to use nodma.

  floppy=nofifo Disables the FIFO entirely. This is needed if you get
  `Bus master arbitration error' messages from your Ethernet card (or
  from other devices) while accessing the floppy.

  floppy=broken_dcl Don't use the disk change line, but assume that the
  disk was changed whenever the device node is reopened. Needed on some
  boxes where the disk change line is broken or unsupported.  This
  should be regarded as a stopgap measure, indeed it makes floppy
  operation less efficient due to unneeded cache flushings, and slightly
  more unreliable. Please verify your cable connection and jumper
  settings if you have any DCL problems. However, some older drives, and
  also some Laptops are known not to have a DCL.

  floppy=debug Print (additional) debugging messages.

  floppy=messages Print informational messages for some operations (disk
  change notifications, warnings about over and underruns, and about
  autodetection).


  10.3.  The Sound Driver (`sound=')

  The sound driver can also accept boot args to override the compiled in
  values. This is not recommended, as it is rather complex and the
  documentation for it in the kernel mysteriously vanished (a hint).
  You are better off to use sound as a module, or compile in your own
  values.


  If you choose to use it regardless, then processing of the argument
  takes place in the file dev_table.c in linux/drivers/sound. It accepts
  a boot arg of the form:


  ______________________________________________________________________
          sound=device1[,device2[,device3...[,device11]]]
  ______________________________________________________________________



  where each deviceN value is of the following format 0xDTaaaId and the
  bytes are used as follows:

  D - second DMA channel (zero if not applicable)

  T - device type: 1=FM, 2=SB, 3=PAS, 4=GUS, 5=MPU401, 6=SB16,
  7=SB16-MIDI,...  The listing of soundcard types up to 26 (don't forget
  to convert back to hex for command line use) are listed in the file
  linux/include/linux/soundcard.h and 27 to 999 (newer models) can be
  found in the file linux/drivers/sound/dev_table.h.

  aaa - I/O address in hex.

  I - interrupt line in hex (i.e 10=a, 11=b, ...)

  d - First DMA channel.

  As you can see it gets pretty messy, and you really are better off to
  use a modular driver or compile in your own personal values as
  recommended. Using a boot arg of `sound=0' will disable the sound
  driver entirely.


  10.4.  The Bus Mouse Driver (`bmouse=')

  The busmouse driver only accepts one parameter, that being the
  hardware IRQ value to be used.


  10.5.  The MS Bus Mouse Driver (`msmouse=')

  The MS mouse driver only accepts one parameter, that being the
  hardware IRQ value to be used.


  10.6.  The Printer Driver (`lp=')

  With this boot argument you can tell the printer driver what ports to
  use and what ports not to use. The latter comes in handy if you don't
  want the printer driver to claim all available parallel ports, so that
  other drivers (e.g. PLIP, PPA) can use them instead.

  The format of the argument is multiple i/o, IRQ pairs. For example,
  lp=0x3bc,0,0x378,7 would use the port at 0x3bc in IRQ-less (polling)
  mode, and use IRQ 7 for the port at 0x378. The port at 0x278 (if any)
  would not be probed, since autoprobing only takes place in the absence
  of a lp= argument. To disable the printer driver entirely, one can use
  lp=0.


  11.  Copying, Translations, Closing, etc.

  Hey, you made it to the end! (Phew...)  Now just the legal stuff.


  11.1.  Copyright and Disclaimer

  This document is Copyright (c) 1995-1999 by Paul Gortmaker.  Copying
  and redistribution is allowed under the conditions as outlined in the
  Linux Documentation Project Copyright, available from where you
  obtained this document, OR as outlined in the GNU General Public
  License, version 2 (see linux/COPYING).

  This document is not gospel. However, it is probably the most up to
  date info that you will be able to find. Nobody is responsible for
  what happens to your hardware but yourself. If your stuff goes up in
  smoke, or anything else bad happens, we take no responsibility. ie.
  THE AUTHOR IS NOT RESPONSIBLE FOR ANY DAMAGES INCURRED DUE TO ACTIONS
  TAKEN BASED ON THE INFORMATION INCLUDED IN THIS DOCUMENT.

  A hint to people considering doing a translation.  First, translate
  the SGML source (available via FTP from the HowTo main site) so that
  you can then generate other output formats.  Be sure to keep a copy of
  the original English SGML source that you translated from! When an
  updated HowTo is released, get the new SGML source for that version,
  and then a simple diff -u old.sgml new.sgml will show you exactly what
  has changed so that you can easily incorporate those changes into your
  translated SMGL source without having to re-read or re-translate
  everything.

  If you are intending to incorporate this document into a published
  work, please make contact (via e-mail) so that you can be supplied
  with the most up to date information available. In the past, out of
  date versions of the Linux HowTo documents have been published, which
  caused the developers undue grief from being plagued with questions
  that were already answered in the up to date versions.


  11.2.  Closing

  If you have found any glaring typos, or outdated info in this
  document, please let me know. It is easy to overlook stuff, as the
  kernel (and the number of drivers) is huge compared to what it was
  when I started this.

  Thanks,

  Paul Gortmaker, p_gortmaker@yahoo.com




The Linux Bootdisk HOWTO

Tom Fawcett

           fawcett+BH@croftj.net
   
   Copyright  1995,1996,1997,1998,1999,2000 by Tom Fawcett and Graham
   Chapman.
   
   v4.0, April 2000
   
   This document describes how to design and build your own boot/root
   diskettes for Linux. These disks can be used as rescue disks or to
   test new system components. You should be reasonably familiar with
   system administration tasks before attempting to build your own
   bootdisk. If you just want a rescue disk to have for emergencies, see
   [1]Appendix A.1.
     _________________________________________________________________
   
   Table of Contents
   [2]Preface
          
        [3]Version notes
        [4]Yet to do
        [5]Feedback and credits
        [6]Distribution policy
                
   [7]Introduction
   [8]Bootdisks and the boot process
          
        [9]The boot process
        [10]Disk types
                
   [11]Building a root filesystem
          
        [12]Overview
        [13]Creating the filesystem
        [14]Populating the filesystem
        [15]Providing for PAM and NSS
        [16]Modules
        [17]Some final details
        [18]Wrapping it up
                
   [19]Choosing a kernel
   [20]Putting them together: Making the diskette(s)
          
        [21]Transferring the kernel with LILO
        [22]Transferring the kernel without LILO
        [23]Setting the ramdisk word
        [24]Transferring the root filesystem
                
   [25]Troubleshooting, or The Agony of Defeat
   [26]Miscellaneous topics
          
        [27]Reducing root filesystem size
        [28]Non-ramdisk root filesystems
        [29]Building a utility disk
                
   [30]How the pros do it
   [31]Frequently Asked Question (FAQ) list
   Appendix A. [32]Resources and pointers
          
        [33]Pre-made Bootdisks
        [34]Rescue packages
        [35]LILO -- the Linux loader
        [36]Linux FAQ and HOWTOs
        [37]Ramdisk usage
        [38]The Linux boot process
                
   Appendix B. [39]LILO boot error codes
   Appendix C. [40]Sample root filesystem listings
   Appendix D. [41]Sample utility disk directory listing
          
Preface

     Important: This document may be outdated. If the date on the title
     page is more than six months ago, please check the
     [42]Bootdisk-HOWTO homepage to see if a more recent version exists.
     
   Although this document should be legible in its text form, it looks
   much better in Postscript, PDF or HTML forms because of the
   typographical conventions used.
     _________________________________________________________________
   
Version notes

   Graham Chapman wrote the original Bootdisk-HOWTO and he supported it
   through version 3.1. Tom Fawcett started as co-author around the time
   kernel v2 was introduced. He is the document's current maintainer.
   
   This information is intended for Linux on the Intel platform. Much of
   this information may be applicable to Linux on other processors, but
   we have no first-hand experience or information about this. If you
   have experience with bootdisks on other platforms, please contact us.
     _________________________________________________________________
   
Yet to do

   Any volunteers?
   
    1. Describe (or link to another document that describes) how to
       create other bootable disk-like things, such as CDROMs, ZIP disks
       and LS110 disks.
    2. Describe how to deal with the huge libc.so shared libraries. The
       options are basically to get older, smaller libraries or to cut
       down existing libraries.
    3. Re-analyze distribution bootdisks and update the "How the Pros do
       it" section.
    4. Delete section that describes how to upgrade existing distribution
       bootdisks. This is usually more trouble than it's worth.
    5. Rewrite/streamline the Troubleshooting section.
     _________________________________________________________________
   
Feedback and credits

   I welcome any feedback, good or bad, on the content of this document.
   I/we have done our best to ensure that the instructions and
   information herein are accurate and reliable. Please let me know if
   you find errors or omissions. When writing, please indicate the
   version number of the document you're referencing.
   
   We thank the many people who assisted with corrections and
   suggestions. Their contributions have made it far better than we could
   ever have done alone.
   
   Send comments, corrections and questions to the author at the email
   address above. I don't mind trying to answer questions, but if you
   have a specific question about why your bootdisk doesn't work, please
   read [43]the section called Troubleshooting, or The Agony of Defeat
   first.
     _________________________________________________________________
   
Distribution policy

   Copyright  1995,1996,1997,1998,1999,2000 by Tom Fawcett and Graham
   Chapman. This document may be distributed under the terms set forth in
   the [44]Linux Documentation Project License. Please contact the
   authors if you are unable to get the license.
   
   This is free documentation. It is distributed in the hope that it will
   be useful, but without any warranty; without even the implied warranty
   of merchantability or fitness for a particular purpose.
     _________________________________________________________________
   
Introduction

   Linux boot disks are useful in a number of situations, such as testing
   a new kernel, recovering from a disk failure (anything from a lost
   boot sector to a disk head crash), fixing a disabled system, or
   upgrading critical system files safely (such as libc.so).
   
   There are several ways of obtaining boot disks:
   
     * Use one from a distribution such as Slackware. This will at least
       allow you to boot.
     * Use a rescue package to set up disks designed to be used as rescue
       disks.
     * Learn what is required for each of the types of disk to operate,
       then build your own.
       
   Some people choose the last option so they can do it themselves. That
   way, if something breaks, they can work out what to do to fix it. Plus
   it's a great way to learn about how a Linux system works.
   
   This document assumes some basic familiarity with Linux system
   administration concepts. For example, you should know about
   directories, filesystems and floppy diskettes. You should know how to
   use mount and df. You should know what /etc/passwd and fstab files are
   for and what they look like. You should know that most of the commands
   in this HOWTO should be run as root.
   
   Constructing your own bootdisk from scratch can be complicated. If you
   haven't read the Linux FAQ and related documents, such as the Linux
   Installation HOWTO and the Linux Installation Guide, you should not be
   trying to build boot diskettes. If you just need a working bootdisk
   for emergencies, it is much easier to download a prefabricated one.
   See [45]Appendix A.1, below, for where to find these.
     _________________________________________________________________
   
Bootdisks and the boot process

   A bootdisk is basically a miniature, self-contained Linux system on a
   floppy diskette. It must perform many of the same functions that a
   complete full-size Linux system performs. Before trying to build one
   you should understand the basic Linux boot process. We present the
   basics here, which are sufficient for understanding the rest of this
   document. Many details and alternative options have been omitted.
     _________________________________________________________________
   
The boot process

   All PC systems start the boot process by executing code in ROM
   (specifically, the BIOS) to load the sector from sector 0, cylinder 0
   of the boot drive. The boot drive is usually the first floppy drive
   (designated A: in DOS and /dev/fd0 in Linux). The BIOS then tries to
   execute this sector. On most bootable disks, sector 0, cylinder 0
   contains either:
   
     * code from a boot loader such as LILO, which locates the kernel,
       loads it and executes it to start the boot proper.
     * the start of an operating system kernel, such as Linux.
       
   If a Linux kernel has been raw-copied to a diskette, the first sector
   of the disk will be the first sector of the Linux kernel itself. This
   first sector will continue the boot process by loading the rest of the
   kernel from the boot device.
   
   Once the kernel is completely loaded, it goes through some basic
   device initialization. It then tries to load and mount a root
   filesystem from some device. A root filesystem is simply a filesystem
   that is mounted as ``/''. The kernel has to be told where to look for
   the root filesystem; if it cannot find a loadable image there, it
   halts.
   
   In some boot situations - often when booting from a diskette - the
   root filesystem is loaded into a ramdisk, which is RAM accessed by the
   system as if it were a disk. There are two reasons why the system
   loads to ramdisk. First, RAM is several orders of magnitude faster
   than a floppy disk, so system operation is fast; and second, the
   kernel can load a compressed filesystem from the floppy and uncompress
   it onto the ramdisk, allowing many more files to be squeezed onto the
   diskette.
   
   Once the root filesystem is loaded and mounted, you see a message
   like:
        VFS: Mounted root (ext2 filesystem) readonly.

   At this point the system finds the init program on the root filesystem
   (in /bin or /sbin) and executes it. init reads its configuration file
   /etc/inittab, looks for a line designated sysinit, and executes the
   named script. The sysinit script is usually something like /etc/rc or
   /etc/init.d/boot. This script is a set of shell commands that set up
   basic system services, such as:
   
     * Running fsck on all the disks,
     * Loading necessary kernel modules,
     * Starting swapping,
     * Initializing the network,
     * Mounting disks mentioned in fstab.
       
   This script often invokes various other scripts to do modular
   initialization. For example, in the common SysVinit structure, the
   directory /etc/rc.d/ contains a complex structure of subdirectories
   whose files specify how to enable and shut down most system services.
   However, on a bootdisk the sysinit script is often very simple.
   
   When the sysinit script finishes control returns to init, which then
   enters the default runlevel, specified in inittab with the initdefault
   keyword. The runlevel line usually specifies a program like getty,
   which is responsible for handling commununications through the console
   and ttys. It is the getty program which prints the familiar ``login:''
   prompt. The getty program in turn invokes the login program to handle
   login validation and to set up user sessions.
     _________________________________________________________________
   
Disk types

   Having reviewed the basic boot process, we can now define various
   kinds of disks involved. We classify disks into four types. The
   discussion here and throughout this document uses the term ``disk'' to
   refer to floppy diskettes unless otherwise specified, though most of
   the discussion could apply equally well to hard disks.
   
   boot
          A disk containing a kernel which can be booted. The disk can be
          used to boot the kernel, which then may load a root file system
          on another disk. The kernel on a bootdisk usually must be told
          where to find its root filesystem.
          
          Often a bootdisk loads a root filesystem from another diskette,
          but it is possible for a bootdisk to be set up to load a hard
          disk's root filesystem instead. This is commonly done when
          testing a new kernel (in fact, ``make zdisk'' will create such
          a bootdisk automatically from the kernel source code).
          
   root
          A disk with a filesystem containing files required to run a
          Linux system. Such a disk does not necessarily contain either a
          kernel or a boot loader.
          
          A root disk can be used to run the system independently of any
          other disks, once the kernel has been booted. Usually the root
          disk is automatically copied to a ramdisk. This makes root disk
          accesses much faster, and frees up the disk drive for a utility
          disk.
          
   boot/root
          A disk which contains both the kernel and a root filesystem. In
          other words, it contains everything necessary to boot and run a
          Linux system without a hard disk. The advantage of this type of
          disk is that is it compact - everything required is on a single
          disk. However, the gradually increasing size of everything
          means that it is increasingly difficult to fit everything on a
          single diskette, even with compression.
          
   utility
          A disk which contains a filesystem, but is not intended to be
          mounted as a root file system. It is an additional data disk.
          You would use this type of disk to carry additional utilities
          where you have too much to fit on your root disk.
          
   In general, when we talk about ``building a bootdisk'' we mean
   creating both the boot (kernel) and root (files) portions. They may be
   either together (a single boot/root disk) or separate (boot + root
   disks). The most flexible approach for rescue diskettes is probably to
   use separate boot and root diskettes, and one or more utility
   diskettes to handle the overflow.
     _________________________________________________________________
   
Building a root filesystem

   Creating the root filesystem involves selecting files necessary for
   the system to run. In this section we describe how to build a
   compressed root filesystem. A less common option is to build an
   uncompressed filesystem on a diskette that is directly mounted as
   root; this alternative is described in [46]the section called
   Non-ramdisk root filesystems.
     _________________________________________________________________
   
Overview

   A root filesystem must contain everything needed to support a full
   Linux system. To be able to do this, the disk must include the minimum
   requirements for a Linux system:
   
     * The basic file system structure,
     * Minimum set of directories: /dev, /proc, /bin, /etc, /lib, /usr,
       /tmp,
     * Basic set of utilities: sh, ls, cp, mv, etc.,
     * Minimum set of config files: rc, inittab, fstab, etc.,
     * Devices: /dev/hd*, /dev/tty*, /dev/fd0, etc.,
     * Runtime library to provide basic functions used by utilities.
       
   Of course, any system only becomes useful when you can run something
   on it, and a root diskette usually only becomes useful when you can do
   something like:
   
     * Check a file system on another drive, for example to check your
       root file system on your hard drive, you need to be able to boot
       Linux from another drive, as you can with a root diskette system.
       Then you can run fsck on your original root drive while it is not
       mounted.
     * Restore all or part of your original root drive from backup using
       archive and compression utilities such as cpio, tar, gzip and
       ftape.
       
   We will describe how to build a compressed filesystem, so called
   because it is compressed on disk and, when booted, is uncompressed
   onto a ramdisk. With a compressed filesystem you can fit many files
   (approximately six megabytes) onto a standard 1440K diskette. Because
   the filesystem is much larger than a diskette, it cannot be built on
   the diskette. We have to build it elsewhere, compress it, then copy it
   to the diskette.
     _________________________________________________________________
   
Creating the filesystem

   In order to build such a root filesystem, you need a spare device that
   is large enough to hold all the files before compression. You will
   need a device capable of holding about four megabytes. There are
   several choices:
   
     * Use a ramdisk (DEVICE = /dev/ram0). In this case, memory is used
       to simulate a disk drive. The ramdisk must be large enough to hold
       a filesystem of the appropriate size. If you use LILO, check your
       configuration file (/etc/lilo.conf) for a line like RAMDISK = nnn
       which determines the maximum RAM that can be allocated to a
       ramdisk. The default is 4096K, which should be sufficient. You
       should probably not try to use such a ramdisk on a machine with
       less than 8MB of RAM. Check to make sure you have a device like
       /dev/ram0, /dev/ram or /dev/ramdisk. If not, create /dev/ram0 with
       mknod (major number 1, minor 0).
     * If you have an unused hard disk partition that is large enough
       (several megabytes), this is acceptable.
     * Use a loopback device, which allows a disk file to be treated as a
       device. Using a loopback device you can create a three megabyte
       file on your hard disk and build the filesystem on it.
       Type man losetup for instructions on using loopback devices. If
       you don't have losetup, you can get it along with compatible
       versions of mount and unmount from the util-linux package in the
       directory [47]ftp://ftp.win.tue.nl/pub/linux/utils/util-linux/.
       If you do not have a loop device (/dev/loop0, /dev/loop1, etc.) on
       your system, you will have to create one with ``mknod /dev/loop0 b
       7 0''. Once you've installed these special mount and umount
       binaries, create a temporary file on a hard disk with enough
       capacity (eg, /tmp/fsfile). You can use a command like:
       
        dd if=/dev/zero of=/tmp/fsfile bs=1k count=nnn

       to create an nnn-block file.
       Use the file name in place of DEVICE below. When you issue a mount
       command you must include the option -o loop to tell mount to use a
       loopback device. For example:
       
        mount -o loop -t ext2 /tmp/fsfile /mnt

       will mount /tmp/fsfile via a loopback device at the mount point
       /mnt. A df will confirm this.
       
   After you've chosen one of these options, prepare the DEVICE with:
        dd if=/dev/zero of=DEVICE bs=1k count=4096

   This command zeroes out the device.
   
     Important: Zeroing the device is critical because the filesystem
     will be compressed later, so all unused portions should be filled
     with zeroes to achieve maximum compression. Keep this fact in mind
     whenever you delete files from your root filesystem. The filesystem
     will correctly de-allocate the blocks, but it will not zero them
     out again. If you do a lot of deletions and copying, your
     compressed filesystem may end up much larger than necessary.
     
   Next, create the filesystem. The Linux kernel recognizes two file
   system types for root disks to be automatically copied to ramdisk.
   These are minix and ext2, of which ext2 is preferred. If using ext2,
   you may find it useful to use the -i option to specify more inodes
   than the default; -i 2000 is suggested so that you don't run out of
   inodes. Alternatively, you can save on inodes by removing lots of
   unnecessary /dev files. mke2fs will by default create 360 inodes on a
   1.44Mb diskette. I find that 120 inodes is ample on my current rescue
   root diskette, but if you include all the devices in the /dev
   directory then you will easily exceed 360. Using a compressed root
   filesystem allows a larger filesystem, and hence more inodes by
   default, but you may still need to either reduce the number of files
   or increase the number of inodes.
   
   So the command you use will look like:
        mke2fs -m 0 -i 2000 DEVICE

   (If you're using a loopback device, the disk file you're using should
   be supplied in place of this DEVICE.)
   
   The mke2fs command will automatically detect the space available and
   configure itself accordingly. The ``-m 0'' parameter prevents it from
   reserving space for root, and hence provides more usable space on the
   disk.
   
   Next, mount the device:
        mount -t ext2 DEVICE /mnt

   (You must create a mount point /mnt if it does not already exist.) In
   the remaining sections, all destination directory names are assumed to
   be relative to /mnt.
     _________________________________________________________________
   
Populating the filesystem

   Here is a reasonable minimum set of directories for your root
   filesystem [48][1]:
   
     * /dev -- Devices, required to perform I/O
     * /proc -- Directory stub required by the proc filesystem
     * /etc -- System configuration files
     * /sbin -- Critical system binaries
     * /bin -- Essential binaries considered part of the system
     * /lib -- Shared libraries to provide run-time support
     * /mnt -- A mount point for maintenance on other disks
     * /usr -- Additional utilities and applications
       
   Three of these directories will be empty on the root filesystem, so
   they only need to be created with mkdir. The /proc directory is
   basically a stub under which the proc filesystem is placed. The
   directories /mnt and /usr are only mount points for use after the
   boot/root system is running. Hence again, these directories only need
   to be created.
   
   The remaining four directories are described in the following
   sections.
     _________________________________________________________________
   
/dev

   A /dev directory containing a special file for all devices to be used
   by the system is mandatory for any Linux system. The directory itself
   is a normal directory, and can be created with mkdir in the normal
   way. The device special files, however, must be created in a special
   way, using the mknod command.
   
   There is a shortcut, though -- copy your existing /dev directory
   contents, and delete the ones you don't want. The only requirement is
   that you copy the device special files using -R option. This will copy
   the directory without attempting to copy the contents of the files. Be
   sure to use an upper case R. The command is:
        cp -dpR /dev /mnt

   assuming that the diskette is mounted at /mnt. The dp switches ensure
   that symbolic links are copied as links, rather than using the target
   file, and that the original file attributes are preserved, thus
   preserving ownership information.
   
   If you want to do it the hard way, use ls -l to display the major and
   minor device numbers for the devices you want, and create them on the
   diskette using mknod.
   
   However the devices are copied, it is worth checking that any special
   devices you need have been placed on the rescue diskette. For example,
   ftape uses tape devices, so you will need to copy all of these if you
   intend to access your floppy tape drive from the bootdisk.
   
   Note that one inode is required for each device special file, and
   inodes can at times be a scarce resource, especially on diskette
   filesystems. It therefore makes sense to remove any device special
   files that you don't need from the diskette /dev directory. For
   example, if you do not have SCSI disks you can safely remove all the
   device files starting with sd. Similarly, if you don't intend to use
   your serial port then all device files starting with cua can go.
   
     Important: Be sure to include the following files from this
     directory: console, kmem, mem, null, ram0 and tty1.
     _________________________________________________________________
   
/etc

   This directory contains important configuration files. On most
   systems, these can be divided into three groups:
   
    1. Required at all times, e.g. rc, fstab, passwd.
    2. May be required, but no one is too sure.
    3. Junk that crept in.
       
   Files which are not essential can usually be identified with the
   command:
        ls -ltru

   This lists files in reverse order of date last accessed, so if any
   files are not being accessed, they can be omitted from a root
   diskette.
   
   On my root diskettes, I have the number of config files down to 15.
   This reduces my work to dealing with three sets of files:
   
    1. The ones I must configure for a boot/root system:
         a. rc.d/* -- system startup and run level change scripts
         b. fstab -- list of file systems to be mounted
         c. inittab -- parameters for the init process, the first process
            started at boot time.
    2. The ones I should tidy up for a boot/root system:
         a. passwd -- Critical list of users, home directories, etc.
         b. group -- user groups.
         c. shadow -- passwords of users. You may not have this.
         d. termcap -- the terminal capability database.
       If security is important, passwd and shadow should be pruned to
       avoid copying user passwords off the system, and so that unwanted
       logins are rejected when you boot from diskette.
       Be sure that passwd contains at least root. If you intend other
       users to login, be sure their home directories and shells exist.
       termcap, the terminal database, is typically several hundred
       kilobytes. The version on your boot/root diskette should be pruned
       down to contain only the terminal(s) you use, which is usually
       just the linux or linux-console entry.
    3. The rest. They work at the moment, so I leave them alone.
       
   Out of this, I only really have to configure two files, and what they
   should contain is surprisingly small.
   
     * rc should contain:
       
        #!/bin/sh
        /bin/mount -av
        /bin/hostname Kangaroo

       Be sure the directories are right. You don't really need to run
       hostname -- it just looks nicer if you do.
     * fstab should contain at least:
       
        /dev/ram0       /               ext2    defaults
        /dev/fd0        /               ext2    defaults
        /proc           /proc           proc    defaults

       You can copy entries from your existing fstab, but you should not
       automatically mount any of your hard disk partitions; use the
       noauto keyword with them. Your hard disk may be damaged or dead
       when the bootdisk is used.
       
   Your inittab should be changed so that its sysinit line runs rc or
   whatever basic boot script will be used. Also, if you want to ensure
   that users on serial ports cannot login, comment out all the entries
   for getty which include a ttys or ttyS device at the end of the line.
   Leave in the tty ports so that you can login at the console.
   
   A minimal inittab file looks like this:
        id:2:initdefault:
        si::sysinit:/etc/rc
        1:2345:respawn:/sbin/getty 9600 tty1
        2:23:respawn:/sbin/getty 9600 tty2

   The inittab file defines what the system will run in various states
   including startup, move to multi-user mode, etc. Check carefully the
   filenames mentioned in inittab; if init cannot find the program
   mentioned the bootdisk will hang, and you may not even get an error
   message.
   
   Note that some programs cannot be moved elsewhere because other
   programs have hardcoded their locations. For example on my system,
   /etc/shutdown has hardcoded in it /etc/reboot. If I move reboot to
   /bin/reboot, and then issue a shutdown command, it will fail because
   it cannot find the reboot file.
   
   For the rest, just copy all the text files in your /etc directory,
   plus all the executables in your /etc directory that you cannot be
   sure you do not need. As a guide, consult the sample listing in
   [49]Appendix Appendix C. Probably it will suffice to copy only those
   files, but systems differ a great deal, so you cannot be sure that the
   same set of files on your system is equivalent to the files in the
   list. The only sure method is to start with inittab and work out what
   is required.
   
   Most systems now use an /etc/rc.d/ directory containing shell scripts
   for different run levels. The minimum is a single rc script, but it
   may be simpler just to copy inittab and the /etc/rc.d directory from
   your existing system, and prune the shell scripts in the rc.d
   directory to remove processing not relevent to a diskette system
   environment.
     _________________________________________________________________
   
/bin and /sbin

   The /bin directory is a convenient place for extra utilities you need
   to perform basic operations, utilities such as ls, mv, cat and dd. See
   [50]Appendix Appendix C for an example list of files that go in a /bin
   and /sbin directories. It does not include any of the utilities
   required to restore from backup, such as cpio, tar and gzip. That is
   because I place these on a separate utility diskette, to save space on
   the boot/root diskette. Once the boot/root diskette is booted, it is
   copied to the ramdisk leaving the diskette drive free to mount another
   diskette, the utility diskette. I usually mount this as /usr.
   
   Creation of a utility diskette is described below in [51]the section
   called Building a utility disk. It is probably desirable to maintain a
   copy of the same version of backup utilities used to write the backups
   so you don't waste time trying to install versions that cannot read
   your backup tapes.
   
     Important: Be sure to include the following programs: init, getty
     or equivalent, login, mount, some shell capable of running your rc
     scripts, a link from sh to the shell.
     _________________________________________________________________
   
/lib

   In /lib you place necessary shared libraries and loaders. If the
   necessary libraries are not found in your /lib directory then the
   system will be unable to boot. If you're lucky you may see an error
   message telling you why.
   
   Nearly every program requires at least the libc library, libc.so.N,
   where N is the current version number. Check your /lib directory. The
   file libc.so.N is usually a symlink to a filename with a complete
   version number:
   
% ls -l /lib/libc*
-rwxr-xr-x   1 root     root      4016683 Apr 16 18:48 libc-2.1.1.so*
lrwxrwxrwx   1 root     root           13 Apr 10 12:25 libc.so.6 -> libc-2.1.1.
so*

   In this case, you want libc-2.1.1.so. To find other libraries you
   should go through all the binaries you plan to include and check their
   dependencies with ldd. For example:
        % ldd /sbin/mke2fs
        libext2fs.so.2 => /lib/libext2fs.so.2 (0x40014000)
        libcom_err.so.2 => /lib/libcom_err.so.2 (0x40026000)
        libuuid.so.1 => /lib/libuuid.so.1 (0x40028000)
        libc.so.6 => /lib/libc.so.6 (0x4002c000)
        /lib/ld-linux.so.2 => /lib/ld-linux.so.2 (0x40000000)

   Each file on the right-hand side is required. The file may be a
   symbolic link.
   
   Note that some libraries are quite large and will not fit easily on
   your root filesystem. For example, the libc.so listed above is about 4
   meg. You will probably need to strip libraries when copying them to
   your root filesystem. See [52]the section called Reducing root
   filesystem size for instructions.
   
   In /lib you must also include a loader for the libraries. The loader
   will be either ld.so (for A.OUT libraries, which are no longer common)
   or ld-linux.so (for ELF libraries). Newer versions of ldd tell you
   exactly which loader is needed, as in the example above, but older
   versions may not. If you're unsure which you need, run the file
   command on the library. For example:
% file /lib/libc.so.4.7.2 /lib/libc.so.5.4.33 /lib/libc-2.1.1.so
/lib/libc.so.4.7.2: Linux/i386 demand-paged executable (QMAGIC), stripped
/lib/libc.so.5.4.33: ELF 32-bit LSB shared object, Intel 80386, version 1, stri
pped
/lib/libc-2.1.1.so: ELF 32-bit LSB shared object, Intel 80386, version 1, not s
tripped

   The QMAGIC indicates that 4.7.2 is for A.OUT libraries, and ELF
   indicates that 5.4.33 and 2.1.1 are for ELF.
   
   Copy the specific loader(s) you need to the root filesystem you're
   building. Libraries and loaders should be checked carefully against
   the included binaries. If the kernel cannot load a necessary library,
   the kernel may hang with no error message.
     _________________________________________________________________
   
Providing for PAM and NSS

   Your system may require dynamically loaded libraries that are not
   visible to ldd. If you don't provide for these, you may have trouble
   logging in or using your bootdisk.
     _________________________________________________________________
   
PAM (Pluggable Authentication Modules)

   If your system uses PAM (Pluggable Authentication Modules), you must
   make some provision for it on your bootdisk. Briefly, PAM is a
   sophisticated modular method for authenticating users and controlling
   their access to services. An easy way to determine if your system uses
   PAM is run ldd on your login executable; if the output includes
   libpam.so, you need PAM.
   
   Fortunately, security is usually of no concern with bootdisks since
   anyone who has physical access to a machine can usually do anything
   they want anyway. Therefore, you can effectively disable PAM by
   creating a simple /etc/pam.conf file in your root filesystem that
   looks like this:
OTHER   auth       optional     /lib/security/pam_permit.so
OTHER   account    optional     /lib/security/pam_permit.so
OTHER   password   optional     /lib/security/pam_permit.so
OTHER   session    optional     /lib/security/pam_permit.so

   Also copy the file /lib/security/pam_permit.so to your root
   filesystem. This library is only about 8K so it imposes minimal
   overhead.
   
   This configuration allows anyone complete access to the files and
   services on your machine. If you care about security on your bootdisk
   for some reason, you'll have to copy some or all of your hard disk's
   PAM setup to your root filesystem. Be sure to read the PAM
   documentation carefully, and copy any libraries needed in
   /lib/security onto your root filesystem.
   
   You must also include /lib/libpam.so on your bootdisk. But you already
   know this since you ran ldd on /bin/login, which showed this
   dependency.
     _________________________________________________________________
   
NSS (Name Service Switch)

   If you are using glibc (aka libc6), you will have to make provisions
   for name services or you will not be able to login. The file
   /etc/nsswitch.conf controls database lookups for various servies. If
   you don't plan to access services from the network (eg, DNS or NIS
   lookups), you need only prepare a simple nsswitch.conf file that looks
   like this:
     passwd:     files
     shadow:     files
     group:      files
     hosts:      files
     services:   files
     networks:   files
     protocols:  files
     rpc:        files
     ethers:     files
     netmasks:   files
     bootparams: files
     automount:  files
     aliases:    files
     netgroup:   files
     publickey:  files

   This specifies that every service be provided only by local files. You
   will also need to include /lib/libnss_files.so.X, where X is 1 for
   glibc 2.0 and 2 for glibc 2.1. This library will be loaded dynamically
   to handle the file lookups.
   
   If you plan to access the network from your bootdisk, you may want to
   create a more elaborate nsswitch.conf file. See the nsswitch man page
   for details. You must include a file /lib/libnss_service.so.1 for each
   service you specify.
     _________________________________________________________________
   
Modules

   If you have a modular kernel, you must consider which modules you may
   want to load from your bootdisk after booting. You might want to
   include ftape and zftape modules if your backup tapes are on floppy
   tape, modules for SCSI devices if you have them, and possibly modules
   for PPP or SLIP support if you want to access the net in an emergency.
   
   These modules may be placed in /lib/modules. You should also include
   insmod, rmmod and lsmod. Depending on whether you want to load modules
   automatically, you might also include modprobe, depmod and swapout. If
   you use kerneld, include it along with /etc/conf.modules.
   
   However, the main advantage to using modules is that you can move
   non-critical modules to a utility disk and load them when needed, thus
   using less space on your root disk. If you may have to deal with many
   different devices, this approach is preferable to building one huge
   kernel with many drivers built in.
   
     Important: In order to boot a compressed ext2 filesystem, you must
     have ramdisk and ext2 support built-in. They cannot be supplied as
     modules.
     _________________________________________________________________
   
Some final details

   Some system programs, such as login, complain if the file
   /var/run/utmp and the directory /var/log do not exist. So:
        mkdir -p /mnt/var/{log,run{
        touch /mnt/var/run/utmp

   Finally, after you have set up all the libraries you need, run
   ldconfig to remake /etc/ld.so.cache on the root filesystem. The cache
   tells the loader where to find the libraries. To remake ld.so.cache,
   issue the following commands:
        chdir /mnt; chroot /mnt /sbin/ldconfig

   The chroot is necessary because ldconfig always remakes the cache for
   the root filesystem.
     _________________________________________________________________
   
Wrapping it up

   Once you have finished constructing the root filesystem, unmount it,
   copy it to a file and compress it:
        umount /mnt
        dd if=DEVICE bs=1k | gzip -v9 > rootfs.gz

   When this finishes you will have a file rootfs.gz. This is your
   compressed root filesystem. You should check its size to make sure it
   will fit on a diskette; if it doesn't you'll have to go back and
   remove some files. Some suggestions for reducing root filesystem size
   appear in [53]the section called Reducing root filesystem size.
     _________________________________________________________________
   
Choosing a kernel

   At this point you have a complete compressed root filesystem. The next
   step is to build or select a kernel. In most cases it would be
   possible to copy your current kernel and boot the diskette from that.
   However, there may be cases where you wish to build a separate one.
   
   One reason is size. If you are building a single boot/root diskette,
   the kernel will be one of the largest files on the diskette so you
   will have to reduce the size of the kernel as much as possible. To
   reduce kernel size, build it with the minumum set of facilities
   necessary to support the desired system. This means leaving out
   everything you don't need. Networking is a good thing to leave out, as
   well as support for any disk drives and other devices which you don't
   need when running your boot/root system. As stated before, your kernel
   must have ramdisk and ext2 support built into it.
   
   Having worked out a minimum set of facilities to include in a kernel,
   you then need to work out what to add back in. Probably the most
   common uses for a boot/root diskette system would be to examine and
   restore a corrupted root file system, and to do this you may need
   kernel support. For example, if your backups are all held on tape
   using Ftape to access your tape drive, then, if you lose your current
   root drive and drives containing Ftape, then you will not be able to
   restore from your backup tapes. You will have to reinstall Linux,
   download and reinstall ftape, and then try to read your backups.
   
   The point here is that, whatever I/O support you have added to your
   kernel to support backups should also be added into your boot/root
   kernel.
   
   The procedure for actually building the kernel is described in the
   documentation that comes with the kernel. It is quite easy to follow,
   so start by looking in /usr/src/linux. If you have trouble building a
   kernel, you should probably not attempt to build boot/root systems
   anyway. Remember to compress the kernel with ``make zImage''.
     _________________________________________________________________
   
Putting them together: Making the diskette(s)

   At this point you have a kernel and a compressed root filesystem. If
   you are making a boot/root disk, check their sizes to make sure they
   will both fit on one disk. If you are making a two disk boot+root set,
   check the root filesystem to make sure it will fit on a single
   diskette.
   
   You should decide whether to use LILO to boot the bootdisk kernel. The
   alternative is to copy the kernel directly to the diskette and boot
   without LILO. The advantage of using LILO is that it enables you to
   supply some parameters to the kernel which may be necessary to
   initialize your hardware (Check the file /etc/lilo.conf on your
   system. If it exists and has a line like ``append=...'', you probably
   need this feature). The disadvantage of using LILO is that building
   the bootdisk is more complicated and takes slightly more space. You
   will have to set up a small separate filesystem, which we shall call
   the kernel filesystem, where you transfer the kernel and a few other
   files that LILO needs.
   
   If you are going to use LILO, read on; if you are going to transfer
   the kernel directly, skip ahead to [54]the section called Transferring
   the kernel without LILO.
     _________________________________________________________________
   
Transferring the kernel with LILO

   The first thing you must do is create a small configuration file for
   LILO. It should look like this:
        boot      =/dev/fd0
        install   =/boot/boot.b
        map       =/boot/map
        read-write
        backup    =/dev/null
        compact
        image     = KERNEL
        label     = Bootdisk
        root      =/dev/fd0

   For an explanation of these parameters, see LILO's user documentation.
   You will probably also want to add an append=... line to this file
   from your hard disk's /etc/lilo.conf file.
   
   Save this file as bdlilo.conf.
   
   You now have to create a small filesystem, which we shall call a
   kernel filesystem, to distinguish it from the root filesystem.
   
   First, figure out how large the filesystem should be. Take the size of
   your kernel in blocks (the size shown by ``ls -l KERNEL'' divided by
   1024 and rounded up) and add 50. Fifty blocks is approximately the
   space needed for inodes plus other files. You can calculate this
   number exactly if you want to, or just use 50. If you're creating a
   two-disk set, you may as well overestimate the space since the first
   disk is only used for the kernel anyway. Call this number
   KERNEL_BLOCKS.
   
   Put a floppy diskette in the drive (for simplicity we'll assume
   /dev/fd0) and create an ext2 kernel filesystem on it:
        mke2fs -i 8192 -m 0 /dev/fd0 KERNEL_BLOCKS

   The ``-i 8192'' specifies that we want one inode per 8192 bytes. Next,
   mount the filesystem, remove the lost+found directory, and create dev
   and boot directories for LILO:
        mount /dev/fd0 /mnt
        rm -rf /mnt/lost+found
        mkdir /mnt/{boot,dev}

   Next, create devices /dev/null and /dev/fd0. Instead of looking up the
   device numbers, you can just copy them from your hard disk using -R:
        cp -R /dev/{null,fd0} /mnt/dev

   LILO needs a copy of its boot loader, boot.b, which you can take from
   your hard disk. It is usually kept in the /boot directory.
        cp /boot/boot.b /mnt/boot

   Finally, copy in the LILO configuration file you created in the last
   section, along with your kernel. Both can be put in the root
   directory:
        cp bdlilo.conf KERNEL /mnt

   Everything LILO needs is now on the kernel filesystem, so you are
   ready to run it. LILO's -r flag is used for installing the boot loader
   on some other root:
        lilo -v -C bdlilo.conf -r /mnt

   LILO should run without error, after which the kernel filesystem
   should look something like this:
total 361
  1 -rw-r--r--   1 root     root          176 Jan 10 07:22 bdlilo.conf
  1 drwxr-xr-x   2 root     root         1024 Jan 10 07:23 boot/
  1 drwxr-xr-x   2 root     root         1024 Jan 10 07:22 dev/
358 -rw-r--r--   1 root     root       362707 Jan 10 07:23 vmlinuz
boot:
total 8
  4 -rw-r--r--   1 root     root         3708 Jan 10 07:22 boot.b
  4 -rw-------   1 root     root         3584 Jan 10 07:23 map
dev:
total 0
  0 brw-r-----   1 root     root       2,   0 Jan 10 07:22 fd0
  0 crw-r--r--   1 root     root       1,   3 Jan 10 07:22 null

   Do not worry if the file sizes are slightly different from yours.
   
   Now leave the diskette in the drive and go to [55]the section called
   Setting the ramdisk word.
     _________________________________________________________________
   
Transferring the kernel without LILO

   If you are not using LILO, transfer the kernel to the bootdisk with
   dd:
        % dd if=KERNEL of=/dev/fd0 bs=1k
        353+1 records in
        353+1 records out

   In this example, dd wrote 353 complete records + 1 partial record, so
   the kernel occupies the first 354 blocks of the diskette. Call this
   number KERNEL_BLOCKS and remember it for use in the next section.
   
   Finally, set the root device to be the diskette itself, then set the
   root to be loaded read/write:
        rdev /dev/fd0 /dev/fd0
        rdev -R /dev/fd0 0

   Be careful to use a capital -R in the second rdev command.
     _________________________________________________________________
   
Setting the ramdisk word

   Inside the kernel image is the ramdisk word that specifies where the
   root filesystem is to be found, along with other options. The word can
   be accessed and set via the rdev command, and its contents are
   interpreted as follows:
   
   Bit field Description
   0-10 Offset to start of ramdisk, in 1024 byte blocks
   11-13 unused
   14 Flag indicating that ramdisk is to be loaded
   15 Flag indicating to prompt before loading rootfs
   
   If bit 15 is set, on boot-up you will be prompted to place a new
   floppy diskette in the drive. This is necessary for a two-disk boot
   set.
   
   There are two cases, depending on whether you are building a single
   boot/root diskette or a double ``boot+root'' diskette set.
   
    1. If you are building a single disk, the compressed root filesystem
       will be placed right after the kernel, so the offset will be the
       first free block (which should be the same as KERNEL_BLOCKS). Bit
       14 will be set to 1, and bit 15 will be zero. For example, say
       you're building a single disk and the root filesystem will begin
       at block 253 (decimal). The ramdisk word value should be 253
       (decimal) with bit 14 set to 1 and bit 15 set to 0. To calculate
       the value you can simply add the decimal values. 253 + (2^14) =
       253 + 16384 = 16637. If you don't quite understand where this
       number comes from, plug it into a scientific calculator and
       convert it to binary,
    2. If you are building a two-disk set, the root filesystem will begin
       at block zero of the second disk, so the offset will be zero. Bit
       14 will be set to 1 and bit 15 will be 1. The decimal value will
       be 2^14 + 2^15 = 49152 in this case.
       
   After carefully calculating the value for the ramdisk word, set it
   with rdev -r. Be sure to use the decimal value. If you used LILO, the
   argument to rdev here should be the mounted kernel path, e.g.
   /mnt/vmlinuz; if you copied the kernel with dd, instead use the floppy
   device name (e.g., /dev/fd0).
        rdev -r KERNEL_OR_FLOPPY_DRIVE  VALUE

   If you used LILO, unmount the diskette now.
     _________________________________________________________________
   
Transferring the root filesystem

   The last step is to transfer the root filesystem.
   
     * If the root filesystem will be placed on the same disk as the
       kernel, transfer it using dd with the seek option, which specifies
       how many blocks to skip:
       
        dd if=rootfs.gz of=/dev/fd0 bs=1k seek=KERNEL_BLOCKS

     * If the root filesystem will be placed on a second disk, remove the
       first diskette, put the second diskette in the drive, then
       transfer the root filesystem to it:
       
        dd if=rootfs.gz of=/dev/fd0 bs=1k

   Congratulations, you are done!
   
     Important: You should always test a bootdisk before putting it
     aside for an emergency. If it fails to boot, read on.
     _________________________________________________________________
   
Troubleshooting, or The Agony of Defeat

   When building bootdisks, the first few tries often will not boot. The
   general approach to building a root disk is to assemble components
   from your existing system, and try and get the diskette-based system
   to the point where it displays messages on the console. Once it starts
   talking to you, the battle is half over because you can see what it is
   complaining about, and you can fix individual problems until the
   system works smoothly. If the system just hangs with no explanation,
   finding the cause can be difficult. To get a system to boot to the
   stage where it will talk to you requires several components to be
   present and correctly configured. The recommended procedure for
   investigating the problem where the system will not talk to you is as
   follows:
   
     * You may see a message like this:
       
Kernel panic: VFS: Unable to mount root fs on XX:YY

       This is a common problem and it has only a few causes. First,
       check the device XX:YY against the list of device codes; is it the
       correct root device? If not, you probably didn't do an rdev -R, or
       you did it on the wrong image. If the device code is correct, then
       check carefully the device drivers compiled into your kernel. Make
       sure it has floppy disk, ramdisk and ext2 filesystem support
       built-in.
     * If you see many errors like:
       
end_request: I/O error, dev 01:00 (ramdisk), sector NNN

       This is an I/O error being reported by the ramdisk driver,
       probably because the kernel is trying to write beyond the end of
       the device. Your ramdisk is too small to hold your root
       filesystem. Check your bootdisk kernel's initialization messages
       for a line like:
       
        Ramdisk driver initialized : 16 ramdisks of 4096K size

       Check this size against the uncompressed size of the root
       filesystem. If the ramdisks aren't large enough, make them larger.
     * Check that the root disk actually contains the directories you
       think it does. It is easy to copy at the wrong level so that you
       end up with something like /rootdisk/bin instead of /bin on your
       root diskette.
     * Check that there is a /lib/libc.so with the same link that appears
       in your /lib directory on your hard disk.
     * Check that any symbolic links in your /dev directory in your
       existing system also exist on your root diskette filesystem, where
       those links are to devices which you have included in your root
       diskette. In particular, /dev/console links are essential in many
       cases.
     * Check that you have included /dev/tty1, /dev/null, /dev/zero,
       /dev/mem, /dev/ram and /dev/kmem files.
     * Check your kernel configuration -- support for all resources
       required up to login point must be built in, not modules. So
       ramdisk and ext2 support must be built-in.
     * Check that your kernel root device and ramdisk settings are
       correct.
       
   Once these general aspects have been covered, here are some more
   specific files to check:
   
    1. Make sure init is included as /sbin/init or /bin/init. Make sure
       it is executable.
    2. Run ldd init to check init's libraries. Usually this is just
       libc.so, but check anyway. Make sure you included the necessary
       libraries and loaders.
    3. Make sure you have the right loader for your libraries -- ld.so
       for a.out or ld-linux.so for ELF.
    4. Check the /etc/inittab on your bootdisk filesystem for the calls
       to getty (or some getty-like program, such as agetty, mgetty or
       getty_ps). Double-check these against your hard disk inittab.
       Check the man pages of the program you use to make sure these make
       sense. inittab is possibly the trickiest part because its syntax
       and content depend on the init program used and the nature of the
       system. The only way to tackle it is to read the man pages for
       init and inittab and work out exactly what your existing system is
       doing when it boots. Check to make sure /etc/inittab has a system
       initialisation entry. This should contain a command to execute the
       system initialization script, which must exist.
    5. As with init, run ldd on your getty to see what it needs, and make
       sure the necessary library files and loaders were included in your
       root filesystem.
    6. Be sure you have included a shell program (e.g., bash or ash)
       capable of running all of your rc scripts.
    7. If you have a /etc/ld.so.cache file on your rescue disk, remake
       it.
       
   If init starts, but you get a message like:
        Id xxx respawning too fast: disabled for 5 minutes

   it is coming from init, usually indicating that getty or login is
   dying as soon as it starts up. Check the getty and login executables
   and the libraries they depend upon. Make sure the invocations in
   /etc/inittab are correct. If you get strange messages from getty, it
   may mean the calling form in /etc/inittab is wrong.
   
   If you get a login prompt, and you enter a valid login name but the
   system prompts you for another login name immediately, the problem may
   be with PAM or NSS. See [56]the section called Providing for PAM and
   NSS. The problem may also be that you use shadow passwords and didn't
   copy /etc/shadow to your bootdisk.
   
   If you try to run some executable, such as df, which is on your rescue
   disk but you yields a message like: df: not found, check two things:
   (1) Make sure the directory containing the binary is in your PATH, and
   (2) make sure you have libraries (and loaders) the program needs.
     _________________________________________________________________
   
Miscellaneous topics

Reducing root filesystem size

   Sometimes a root filesystem is too large to fit on a diskette even
   after compression. Here are some ways to reduce the filesystem size:
   
    1. Increase the diskette density. By default, floppy diskettes are
       formatted at 1440K, but higher density formats are available.
       fdformat will format disks for the following sizes: 1600, 1680,
       1722, 1743, 1760, 1840, and 1920. Most 1440K drives will support
       1722K, and this is what I always use for bootdisks. See the
       fdformat man page and /usr/src/linux/Documentation/devices.txt.
    2. Replace your shell. Some of the popular shells for Linux, such as
       bash and tcsh, are large and require many libraries. Light-weight
       alternatives exist, such as ash, lsh, kiss and smash, which are
       much smaller and require few (or no) libraries. Most of these
       replacement shells are available from
       [57]http://metalab.unc.edu/pub/Linux/system/shells/. Make sure any
       shell you use is capable of running commands in all the rc files
       you include on your bootdisk.
    3. Strip libraries and binaries. Many libraries and binaries are
       distributed with debugging information. Running file on these
       files will tell you ``not stripped'' if so. When copying binaries
       to your root filesystem, it is good practice to use:
       
      objcopy --strip-all FROM TO

     Important: When copying libraries, be sure to use strip-debug
     instead of strip-all.
     
    4. If you deleted or moved files much when you were creating the root
       filesystem, create it again. See the NOTE ABOVE on the importance
       of not having dirty blocks in the filesystem.
    5. Move non-critical files to a utility disk. If some of your
       binaries are not needed immediately to boot or login, you can move
       them to a utility disk. See [58]the section called Building a
       utility disk for details. You may also consider moving modules to
       a utility disk as well.
     _________________________________________________________________
   
Non-ramdisk root filesystems

   [59]the section called Building a root filesystem gave instructions
   for building a compressed root filesystem which is loaded to ramdisk
   when the system boots. This method has many advantages so it is
   commonly used. However, some systems with little memory cannot afford
   the RAM needed for this, and they must use root filesystems mounted
   directly from the diskette.
   
   Such filesystems are actually easier to build than compressed root
   filesystems because they can be built on a diskette rather than on
   some other device, and they do not have to be compressed. We will
   outline the procedure as it differs from the instructions above. If
   you choose to do this, keep in mind that you will have much less space
   available.
   
    1. Calculate how much space you will have available for root files.
       If you are building a single boot/root disk, you must fit all
       blocks for the kernel plus all blocks for the root filesystem on
       the one disk.
    2. Using mke2fs, create a root filesystem on a diskette of the
       appropriate size.
    3. Populate the filesystem as described above.
    4. When done, unmount the filesystem and transfer it to a disk file
       but do not compress it.
    5. Transfer the kernel to a floppy diskette, as described above. When
       calculating the ramdisk word, set bit 14 to zero, to indicate that
       the root filesystem is not to be loaded to ramdisk. Run the rdev's
       as described.
    6. Transfer the root filesystem as before.
       
   There are several shortcuts you can take. If you are building a
   two-disk set, you can build the complete root filesystem directly on
   the second disk and you need not transfer it to a hard disk file and
   then back. Also, if you are building a single boot/root disk and using
   LILO, you can build a single filesystem on the entire disk, containing
   the kernel, LILO files and root files, and simply run LILO as the last
   step.
     _________________________________________________________________
   
Building a utility disk

   Building a utility disk is relatively easy -- simply create a
   filesystem on a formatted disk and copy files to it. To use it with a
   bootdisk, mount it manually after the system is booted.
   
   In the instructions above, we mentioned that the utility disk could be
   mounted as /usr. In this case, binaries could be placed into a /bin
   directory on your utility disk, so that placing /usr/bin in your path
   will access them. Additional libraries needed by the binaries are
   placed in /lib on the utility disk.
   
   There are several important points to keep in mind when designing a
   utility disk:
   
    1. Do not place critical system binaries or libraries onto the
       utility disk, since it will not be mountable until after the
       system has booted.
    2. You cannot access a floppy diskette and a floppy tape drive
       simultaneously. This means that if you have a floppy tape drive,
       you will not be able to access it while your utility disk is
       mounted.
    3. Access to files on the utility disk will be slow.
       
   [60]Appendix Appendix D shows a sample of files on a utility disk.
   Here are some ideas for files you may find useful: programs for
   examining and manipulating disks (format, fdisk) and filesystems
   (mke2fs, fsck, debugfs, isofs.o), a lightweight text editor (elvis,
   jove), compression and archive utilities (gzip, bzip, tar, cpio,
   afio), tape utilities (mt, ftmt, tob, taper), communications utilities
   (ppp.o, slip.o, minicom) and utilities for devices (setserial, mknod).
     _________________________________________________________________
   
How the pros do it

   You may notice that the bootdisks used by major distributions such as
   Slackware, RedHat or Debian seem more sophisticated than what is
   described in this document. Professional distribution bootdisks are
   based on the same principles outlined here, but employ various tricks
   because their bootdisks have additional requirements. First, they must
   be able to work with a wide variety of hardware, so they must be able
   to interact with the user and load various device drivers. Second,
   they must be prepared to work with many different installation
   options, with varying degrees of automation. Finally, distribution
   bootdisks usually combine installation and rescue capabilities.
   
   Some bootdisks use a feature called initrd (initial ramdisk). This
   feature was introduced around 2.0.x and allows a kernel to boot in two
   phases. When the kernel first boots, it loads an initial ramdisk image
   from the boot disk. This initial ramdisk is a root filesystem
   containing a program that runs before the real root fs is loaded. This
   program usually inspects the environment and/or asks the user to
   select various boot options, such as the device from which to load the
   real rootdisk. It typically loads additional modules not built in to
   the kernel. When this initial program exits, the kernel loads the real
   root image and booting continues normally. For further information on
   initrd, see your local file
   [61]/usr/src/linux/Documentation/initrd.txt and
   [62]ftp://elserv.ffm.fgan.de/pub/linux/loadlin-1.6/initrd-example.tgz
   
   The following are summaries of how each distribution's installation
   disks seem to work, based on inspecting their filesystems and/or
   source code. We do not guarantee that this information is completely
   accurate, or that they have not changed since the versions noted.
   
   Slackware (v.3.1) uses a straightforward LILO boot similar to what is
   described in [63]the section called Transferring the kernel with LILO.
   The Slackware bootdisk prints a bootup message ("Welcome to the
   Slackware Linux bootkernel disk!") using LILO's message parameter.
   This instructs the user to enter a boot parameter line if necessary.
   After booting, a root filesystem is loaded from a second disk. The
   user invokes a setup script which starts the installation. Instead of
   using a modular kernel, Slackware provides many different kernels and
   depends upon the user to select the one matching his or her hardware
   requirements.
   
   RedHat (v.4.0) also uses a LILO boot. It loads a compressed ramdisk on
   the first disk, which runs a custom init program. This program queries
   for drivers then loads additional files from a supplemental disk if
   necessary.
   
   Debian (v.1.3) is probably the most sophisticated of the installation
   disk sets. It uses the SYSLINUX loader to arrange various load
   options, then uses an initrd image to guide the user through
   installation. It appears to use both a customized init and a
   customized shell.
     _________________________________________________________________
   
Frequently Asked Question (FAQ) list

   Q: [64]I boot from my boot/root disks and nothing happens. What do I
          do?
          
   Q: [65]How does the Slackware/Debian/RedHat bootdisk work?
   Q: [66]How can I make a boot disk with a XYZ driver?
   Q: [67]How do I update my root diskette with new files?
   Q: [68]How do I remove LILO so that I can use DOS to boot again?
   Q: [69]How can I boot if I've lost my kernel and my boot disk?
   Q: [70]How can I make extra copies of boot/root diskettes?
   Q: [71]How can I boot without typing in "ahaxxxx=nn,nn,nn" every time?
   Q: [72]At boot time, I get error "A: cannot execute B". Why?
   Q: [73]My kernel has ramdisk support, but initializes ramdisks of 0K.
          Why?
          
   Q: I boot from my boot/root disks and nothing happens. What do I do?
   
   A: See [74]the section called Troubleshooting, or The Agony of Defeat,
   above.
   
   Q: How does the Slackware/Debian/RedHat bootdisk work?
   
   A: See [75]the section called How the pros do it, above.
   
   Q: How can I make a boot disk with a XYZ driver?
   
   A: The easiest way is to obtain a Slackware kernel from your nearest
   Slackware mirror site. Slackware kernels are generic kernels which
   atttempt to include drivers for as many devices as possible, so if you
   have a SCSI or IDE controller, chances are that a driver for it is
   included in the Slackware kernel.
   
   Go to the a1 directory and select either IDE or SCSI kernel depending
   on the type of controller you have. Check the xxxxkern.cfg file for
   the selected kernel to see the drivers which have been included in
   that kernel. If the device you want is in that list, then the
   corresponding kernel should boot your computer. Download the
   xxxxkern.tgz file and copy it to your boot diskette as described above
   in the section on making boot disks.
   
   You must then check the root device in the kernel, using the command
   rdev zImage. If this is not the same as the root device you want, use
   rdev to change it. For example, the kernel I tried was set to
   /dev/sda2, but my root SCSI partition is /dev/sda8. To use a root
   diskette, you would have to use the command rdev zImage /dev/fd0.
   
   If you want to know how to set up a Slackware root disk as well,
   that's outside the scope of this HOWTO, so I suggest you check the
   Linux Install Guide or get the Slackware distribution. See the section
   in this HOWTO titled ``References''.
   
   Q: How do I update my root diskette with new files?
   
   A: The easiest way is to copy the filesystem from the rootdisk back to
   the DEVICE you used (from [76]the section called Creating the
   filesystem, above). Then mount the filesystem and make the changes.
   You have to remember where your root filesystem started and how many
   blocks it occupied:
        dd if=/dev/fd0 bs=1k skip=ROOTBEGIN count=BLOCKS | gunzip > DEVICE
        mount -t ext2 DEVICE /mnt

   After making the changes, proceed as before (in [77]the section called
   Wrapping it up) and transfer the root filesystem back to the disk. You
   should not have to re-transfer the kernel or re-compute the ramdisk
   word if you do not change the starting position of the new root
   filesystem.
   
   Q: How do I remove LILO so that I can use DOS to boot again?
   
   A: This is not really a Bootdisk topic, but it is asked often. Within
   Linux, you can run:
        /sbin/lilo -u

   You can also use the dd command to copy the backup saved by LILO to
   the boot sector. Refer to the LILO documentation if you wish to do
   this.
   
   Within DOS and Windows you can use the DOS command:
        FDISK /MBR

   MBR stands for Master Boot Record. This command replaces the boot
   sector with a clean DOS one, without affecting the partition table.
   Some purists disagree with this, but even the author of LILO, Werner
   Almesberger, suggests it. It is easy, and it works.
   
   Q: How can I boot if I've lost my kernel and my boot disk?
   
   A: If you don't have a boot disk standing by, probably the easiest
   method is to obtain a Slackware kernel for your disk controller type
   (IDE or SCSI) as described above for ``How do I make a boot disk with
   a XXX driver?''. You can then boot your computer using this kernel,
   then repair whatever damage there is.
   
   The kernel you get may not have the root device set to the disk type
   and partition you want. For example, Slackware's generic SCSI kernel
   has the root device set to /dev/sda2, whereas my root Linux partition
   happens to be /dev/sda8. In this case the root device in the kernel
   will have to be changed.
   
   You can still change the root device and ramdisk settings in the
   kernel even if all you have is a kernel, and some other operating
   system, such as DOS.
   
   rdev changes kernel settings by changing the values at fixed offsets
   in the kernel file, so you can do the same if you have a hex editor
   available on whatever systems you do still have running -- for
   example, Norton Utilities Disk Editor under DOS. You then need to
   check and if necessary change the values in the kernel at the
   following offsets:
HEX     DEC  DESCRIPTION
0x01F8  504  Low byte of RAMDISK word
0x01F9  505  High byte of RAMDISK word
0x01FC  508  Root minor device number - see below
0X01FD  509  Root major device number - see below

   The interpretation of the ramdisk word was described in [78]the
   section called Setting the ramdisk word, above.
   
   The major and minor device numbers must be set to the device you want
   to mount your root filesystem on. Some useful values to select from
   are:
DEVICE          MAJOR MINOR
/dev/fd0            2     0   1st floppy drive
/dev/hda1           3     1   partition 1 on 1st IDE drive
/dev/sda1           8     1   partition 1 on 1st SCSI drive
/dev/sda8           8     8   partition 8 on 1st SCSI drive

   Once you have set these values then you can write the file to a
   diskette using either Norton Utilities Disk Editor, or a program
   called rawrite.exe. This program is included in all distributions. It
   is a DOS program which writes a file to the ``raw'' disk, starting at
   the boot sector, instead of writing it to the file system. If you use
   Norton Utilities you must write the file to a physical disk starting
   at the beginning of the disk.
   
   Q: How can I make extra copies of boot/root diskettes?
   
   A: Because magnetic media may deteriorate over time, you should keep
   several copies of your rescue disk, in case the original is
   unreadable.
   
   The easiest way of making copies of any diskettes, including bootable
   and utility diskettes, is to use the dd command to copy the contents
   of the original diskette to a file on your hard drive, and then use
   the same command to copy the file back to a new diskette. Note that
   you do not need to, and should not, mount the diskettes, because dd
   uses the raw device interface.
   
   To copy the original, enter the command:
        dd if=DEVICENAME of=FILENAME

   where DEVICENAME is the device name of the diskette drive and FILENAME
   is the name of the (hard-disk) output file. Omitting the count
   parameter causes dd to copy the whole diskette (2880 blocks if
   high-density).
   
   To copy the resulting file back to a new diskette, insert the new
   diskette and enter the reverse command:
        dd if=FILENAME of=DEVICENAME

   Note that the above discussion assumes that you have only one diskette
   drive. If you have two of the same type, you can copy diskettes using
   a command like:
        dd if=/dev/fd0 of=/dev/fd1

   Q: How can I boot without typing in "ahaxxxx=nn,nn,nn" every time?
   
   A: Where a disk device cannot be autodetected it is necessary to
   supply the kernel with a command device parameter string, such as:
        aha152x=0x340,11,3,1

   This parameter string can be supplied in several ways using LILO:
   
     * By entering it on the command line every time the system is booted
       via LILO. This is boring, though.
     * By using LILO's lock keyword to make it store the command line as
       the default command line, so that LILO will use the same options
       every time it boots.
     * By using the append= statement in the LILO config file. Note that
       the parameter string must be enclosed in quotes.
       
   For example, a sample command line using the above parameter string
   would be:
        zImage  aha152x=0x340,11,3,1 root=/dev/sda1 lock

   This would pass the device parameter string through, and also ask the
   kernel to set the root device to /dev/sda1 and save the whole command
   line and reuse it for all future boots.
   
   A sample APPEND statement is:
        APPEND = "aha152x=0x340,11,3,1"

   Note that the parameter string must not be enclosed in quotes on the
   command line, but it must be enclosed in quotes in the APPEND
   statement.
   
   Note also that for the parameter string to be acted on, the kernel
   must contain the driver for that disk type. If it does not, then there
   is nothing listening for the parameter string, and you will have to
   rebuild the kernel to include the required driver. For details on
   rebuilding the kernel, go to /usr/src/linux and read the README, and
   read the Linux FAQ and Installation HOWTO. Alternatively you could
   obtain a generic kernel for the disk type and install that.
   
   Readers are strongly urged to read the LILO documentation before
   experimenting with LILO installation. Incautious use of the BOOT
   statement can damage partitions.
   
   Q: At boot time, I get error "A: cannot execute B". Why?
   
   A: There are several cases of program names being hardcoded in various
   utilities. These cases do not occur everywhere, but they may explain
   why an executable apparently cannot be found on your system even
   though you can see that it is there. You can find out if a given
   program has the name of another hardcoded by using the strings command
   and piping the output through grep.
   
   Known examples of hardcoding are:
   
     * shutdown in some versions has /etc/reboot hardcoded, so reboot
       must be placed in the /etc directory.
     * init has caused problems for at least one person, with the kernel
       being unable to find init.
       
   To fix these problems, either move the programs to the correct
   directory, or change configuration files (e.g. inittab) to point to
   the correct directory. If in doubt, put programs in the same
   directories as they are on your hard disk, and use the same inittab
   and /etc/rc.d files as they appear on your hard disk.
   
   Q: My kernel has ramdisk support, but initializes ramdisks of 0K. Why?
   
   A: Where this occurs, a kernel message like this will appear as the
   kernel is booting:
        Ramdisk driver initialized : 16 ramdisks of 0K size

   This is probably because the size has been set to 0 by kernel
   parameters at boot time. This could possibly be because of an
   overlooked LILO configuration file parameter:
    ramdisk= 0

   This was included in sample LILO configuration files in some older
   distributions, and was put there to override any previous kernel
   setting. If you have such a line, remove it.
   
   Note that if you attempt to use a ramdisk of 0 size, the behaviour can
   be unpredictable, and can result in kernel panics.
     _________________________________________________________________
   
Appendix A. Resources and pointers

   When retrieving a package, always get the latest version unless you
   have good reasons for not doing so.
     _________________________________________________________________
   
Pre-made Bootdisks

   These are sources for distribution bootdisks. Please use one of the
   mirror sites to reduce the load on these machines.
   
     * [79]Slackware bootdisks, [80]rootdisks and [81]Slackware mirror
       sites
     * [82]RedHat bootdisks and [83]Red Hat mirror sites
     * [84]Debian bootdisks and [85]Debian mirror sites
       
   In addition to the distribution bootdisks, the following rescue disk
   images are available. Unless otherwise specified, these are available
   in the directory
   [86]http://metalab.unc.edu/pub/Linux/system/recovery/!INDEX.html
   
     * tomsrtbt, by Tom Oehser, is a single-disk boot/root disk based on
       kernel 2.0, with a large set of features and support programs. It
       supports IDE, SCSI, tape, network adaptors, PCMCIA and more. About
       100 utility programs and tools are included for fixing and
       restoring disks. The package also includes scripts for
       disassembling and reconstructing the images so that new material
       can be added if necessary.
     * rescue02, by John Comyns, is a rescue disk based on kernel 1.3.84,
       with support for IDE and Adaptec 1542 and NCR53C7,8xx. It uses ELF
       binaries but it has enough commands so that it can be used on any
       system. There are modules that can be loaded after booting for all
       other SCSI cards. It probably won't work on systems with 4 mb of
       ram since it uses a 3 mb ram disk.
     * resque_disk-2.0.22, by Sergei Viznyuk, is a full-featured
       boot/root disk based on kernel 2.0.22 with built-in support for
       IDE, many difference SCSI controllers, and ELF/AOUT. Also includes
       many modules and useful utilities for repairing and restoring a
       hard disk.
     * [87]cramdisk images, based on the 2.0.23 kernel, available for 4
       meg and 8 meg machines. They include math emulation and networking
       (PPP and dialin script, NE2000, 3C509), or support for the
       parallel port ZIP drive. These diskette images will boot on a 386
       with 4MB RAM. MSDOS support is included so you can download from
       the net to a DOS partition.
     _________________________________________________________________
   
Rescue packages

   Several packages for creating rescue disks are available on
   metalab.unc.edu. With these packages you specify a set of files for
   inclusion and the software automates (to varying degrees) the creation
   of a bootdisk. See
   [88]http://metalab.unc.edu/pub/Linux/system/recovery/!INDEX.html for
   more information. Check the file dates carefully. Some of these
   packages have not been updated in several years and will not support
   the creation of a compressed root filesystem loaded into ramdisk. To
   the best of our knowledge, [89]Yard is the only package that will.
     _________________________________________________________________
   
LILO -- the Linux loader

   Written by Werner Almesberger. Excellent boot loader, and the
   documentation includes information on the boot sector contents and the
   early stages of the boot process.
   
   Ftp from [90]ftp://tsx-11.mit.edu/pub/linux/packages/lilo/. It is also
   available on Metalab and mirrors.
     _________________________________________________________________
   
Linux FAQ and HOWTOs

   These are available from many sources. Look at the usenet newsgroups
   news.answers and comp.os.linux.announce.
   
   The FAQ is available from [91]http://linuxdoc.org/FAQ/Linux-FAQ.html
   and the HOWTOs from [92]http://linuxdoc.org/HOWTO/HOWTO-INDEX.html.
   Most documentation for Linux may be found at [93]The Linux
   Documentation Project homepage.
     _________________________________________________________________
   
Ramdisk usage

   An excellent description of the how the ramdisk code works may be
   found with the documentation supplied with the Linux kernel. See
   /usr/src/linux/Documentation/ramdisk.txt. It is written by Paul
   Gortmaker and includes a section on creating a compressed ramdisk.
     _________________________________________________________________
   
The Linux boot process

   For more detail on the Linux boot process, here are some pointers:
   
     * The [94]Linux System Administrators' Guide has a section on
       booting.
     * The [95]LILO ``Technical overview'' has the definitive technical,
       low-level description of the boot process, up to where the kernel
       is started.
     * The source code is the ultimate guide. Below are some kernel files
       related to the boot process. If you have the Linux kernel source
       code, you can find these under /usr/src/linux on your machine;
       alternatively, Shigio Yamaguchi (shigio@tamacom.com) has a very
       nice [96]hypertext kernel browser for reading kernel source files.
       Here are some relevant files to look at:
       
        arch/i386/boot/bootsect.S and setup.S
                Contain assembly code for the bootsector itself.
                
        arch/i386/boot/compressed/misc.c
                Contains code for uncompressing the kernel.
                
        arch/i386/kernel/
                Directory containing kernel initialization code. setup.c
                defines the ramdisk word.
                
        drivers/block/rd.c
                Contains the ramdisk driver. The procedures rd_load and
                rd_load_image load blocks from a device into a ramdisk.
                The procedure identify_ramdisk_image determines what kind
                of filesystem is found and whether it is compressed.
     _________________________________________________________________
   
Appendix B. LILO boot error codes

   Questions about these errors are asked so often on Usenet that we
   include them here as a public service. This summary is excerpted from
   Werner Almsberger's [97]LILO User Documentation.
   
   When LILO loads itself, it displays the word LILO. Each letter is
   printed before or after performing some specific action. If LILO fails
   at some point, the letters printed so far can be used to identify the
   problem.
   
   Output Problem
   (nothing) No part of LILO has been loaded. LILO either isn't installed
   or the partition on which its boot sector is located isn't active.
   L The first stage boot loader has been loaded and started, but it
   can't load the second stage boot loader. The two-digit error codes
   indicate the type of problem. (See also section ``Disk error codes''.)
   This condition usually indicates a media failure or a geometry
   mismatch (e.g. bad disk parameters).
   LI The first stage boot loader was able to load the second stage boot
   loader, but has failed to execute it. This can either be caused by a
   geometry mismatch or by moving /boot/boot.b without running the map
   installer.
   LIL The second stage boot loader has been started, but it can't load
   the descriptor table from the map file. This is typically caused by a
   media failure or by a geometry mismatch.
   LIL? The second stage boot loader has been loaded at an incorrect
   address. This is typically caused by a subtle geometry mismatch or by
   moving /boot/boot.b without running the map installer.
   LIL- The descriptor table is corrupt. This can either be caused by a
   geometry mismatch or by moving /boot/map without running the map
   installer.
   LILO All parts of LILO have been successfully loaded.
   
   If the BIOS signals an error when LILO is trying to load a boot image,
   the respective error code is displayed. These codes range from 0x00
   through 0xbb. See the LILO User Guide for an explanation of these.
     _________________________________________________________________
   
Appendix C. Sample root filesystem listings

/:
drwx--x--x   2 root     root         1024 Nov  1 15:39 bin
drwx--x--x   2 root     root         4096 Nov  1 15:39 dev
drwx--x--x   3 root     root         1024 Nov  1 15:39 etc
drwx--x--x   4 root     root         1024 Nov  1 15:39 lib
drwx--x--x   5 root     root         1024 Nov  1 15:39 mnt
drwx--x--x   2 root     root         1024 Nov  1 15:39 proc
drwx--x--x   2 root     root         1024 Nov  1 15:39 root
drwx--x--x   2 root     root         1024 Nov  1 15:39 sbin
drwx--x--x   2 root     root         1024 Nov  1 15:39 tmp
drwx--x--x   7 root     root         1024 Nov  1 15:39 usr
drwx--x--x   5 root     root         1024 Nov  1 15:39 var

/bin:
-rwx--x--x   1 root     root        62660 Nov  1 15:39 ash
-rwx--x--x   1 root     root         9032 Nov  1 15:39 cat
-rwx--x--x   1 root     root        10276 Nov  1 15:39 chmod
-rwx--x--x   1 root     root         9592 Nov  1 15:39 chown
-rwx--x--x   1 root     root        23124 Nov  1 15:39 cp
-rwx--x--x   1 root     root        23028 Nov  1 15:39 date
-rwx--x--x   1 root     root        14052 Nov  1 15:39 dd
-rwx--x--x   1 root     root        14144 Nov  1 15:39 df
-rwx--x--x   1 root     root        69444 Nov  1 15:39 egrep
-rwx--x--x   1 root     root          395 Nov  1 15:39 false
-rwx--x--x   1 root     root        69444 Nov  1 15:39 fgrep
-rwx--x--x   1 root     root        69444 Nov  1 15:39 grep
-rwx--x--x   3 root     root        45436 Nov  1 15:39 gunzip
-rwx--x--x   3 root     root        45436 Nov  1 15:39 gzip
-rwx--x--x   1 root     root         8008 Nov  1 15:39 hostname
-rwx--x--x   1 root     root        12736 Nov  1 15:39 ln
-rws--x--x   1 root     root        15284 Nov  1 15:39 login
-rwx--x--x   1 root     root        29308 Nov  1 15:39 ls
-rwx--x--x   1 root     root         8268 Nov  1 15:39 mkdir
-rwx--x--x   1 root     root         8920 Nov  1 15:39 mknod
-rwx--x--x   1 root     root        24836 Nov  1 15:39 more
-rws--x--x   1 root     root        37640 Nov  1 15:39 mount
-rwx--x--x   1 root     root        12240 Nov  1 15:39 mt
-rwx--x--x   1 root     root        12932 Nov  1 15:39 mv
-r-x--x--x   1 root     root        12324 Nov  1 15:39 ps
-rwx--x--x   1 root     root         5388 Nov  1 15:39 pwd
-rwx--x--x   1 root     root        10092 Nov  1 15:39 rm
lrwxrwxrwx   1 root     root            3 Nov  1 15:39 sh -> ash
-rwx--x--x   1 root     root        25296 Nov  1 15:39 stty
-rws--x--x   1 root     root        12648 Nov  1 15:39 su
-rwx--x--x   1 root     root         4444 Nov  1 15:39 sync
-rwx--x--x   1 root     root       110668 Nov  1 15:39 tar
-rwx--x--x   1 root     root        19712 Nov  1 15:39 touch
-rwx--x--x   1 root     root          395 Nov  1 15:39 true
-rws--x--x   1 root     root        19084 Nov  1 15:39 umount
-rwx--x--x   1 root     root         5368 Nov  1 15:39 uname
-rwx--x--x   3 root     root        45436 Nov  1 15:39 zcat

/dev:
lrwxrwxrwx   1 root     root            6 Nov  1 15:39 cdrom -> cdu31a
brw-rw-r--   1 root     root      15,   0 May  5  1998 cdu31a
crw-------   1 root     root       4,   0 Nov  1 15:29 console
crw-rw-rw-   1 root     uucp       5,  64 Sep  9 19:46 cua0
crw-rw-rw-   1 root     uucp       5,  65 May  5  1998 cua1
crw-rw-rw-   1 root     uucp       5,  66 May  5  1998 cua2
crw-rw-rw-   1 root     uucp       5,  67 May  5  1998 cua3
brw-rw----   1 root     floppy     2,   0 Aug  8 13:54 fd0
brw-rw----   1 root     floppy     2,  36 Aug  8 13:54 fd0CompaQ
brw-rw----   1 root     floppy     2,  84 Aug  8 13:55 fd0D1040
brw-rw----   1 root     floppy     2,  88 Aug  8 13:55 fd0D1120
brw-rw----   1 root     floppy     2,  12 Aug  8 13:54 fd0D360
brw-rw----   1 root     floppy     2,  16 Aug  8 13:54 fd0D720
brw-rw----   1 root     floppy     2, 120 Aug  8 13:55 fd0D800
brw-rw----   1 root     floppy     2,  32 Aug  8 13:54 fd0E2880
brw-rw----   1 root     floppy     2, 104 Aug  8 13:55 fd0E3200
brw-rw----   1 root     floppy     2, 108 Aug  8 13:55 fd0E3520
brw-rw----   1 root     floppy     2, 112 Aug  8 13:55 fd0E3840
brw-rw----   1 root     floppy     2,  28 Aug  8 13:54 fd0H1440
brw-rw----   1 root     floppy     2, 124 Aug  8 13:55 fd0H1600
brw-rw----   1 root     floppy     2,  44 Aug  8 13:55 fd0H1680
brw-rw----   1 root     floppy     2,  60 Aug  8 13:55 fd0H1722
brw-rw----   1 root     floppy     2,  76 Aug  8 13:55 fd0H1743
brw-rw----   1 root     floppy     2,  96 Aug  8 13:55 fd0H1760
brw-rw----   1 root     floppy     2, 116 Aug  8 13:55 fd0H1840
brw-rw----   1 root     floppy     2, 100 Aug  8 13:55 fd0H1920
lrwxrwxrwx   1 root     root            7 Nov  1 15:39 fd0H360 -> fd0D360
lrwxrwxrwx   1 root     root            7 Nov  1 15:39 fd0H720 -> fd0D720
brw-rw----   1 root     floppy     2,  52 Aug  8 13:55 fd0H820
brw-rw----   1 root     floppy     2,  68 Aug  8 13:55 fd0H830
brw-rw----   1 root     floppy     2,   4 Aug  8 13:54 fd0d360
brw-rw----   1 root     floppy     2,   8 Aug  8 13:54 fd0h1200
brw-rw----   1 root     floppy     2,  40 Aug  8 13:54 fd0h1440
brw-rw----   1 root     floppy     2,  56 Aug  8 13:55 fd0h1476
brw-rw----   1 root     floppy     2,  72 Aug  8 13:55 fd0h1494
brw-rw----   1 root     floppy     2,  92 Aug  8 13:55 fd0h1600
brw-rw----   1 root     floppy     2,  20 Aug  8 13:54 fd0h360
brw-rw----   1 root     floppy     2,  48 Aug  8 13:55 fd0h410
brw-rw----   1 root     floppy     2,  64 Aug  8 13:55 fd0h420
brw-rw----   1 root     floppy     2,  24 Aug  8 13:54 fd0h720
brw-rw----   1 root     floppy     2,  80 Aug  8 13:55 fd0h880
brw-rw----   1 root     disk       3,   0 May  5  1998 hda
brw-rw----   1 root     disk       3,   1 May  5  1998 hda1
brw-rw----   1 root     disk       3,   2 May  5  1998 hda2
brw-rw----   1 root     disk       3,   3 May  5  1998 hda3
brw-rw----   1 root     disk       3,   4 May  5  1998 hda4
brw-rw----   1 root     disk       3,   5 May  5  1998 hda5
brw-rw----   1 root     disk       3,   6 May  5  1998 hda6
brw-rw----   1 root     disk       3,  64 May  5  1998 hdb
brw-rw----   1 root     disk       3,  65 May  5  1998 hdb1
brw-rw----   1 root     disk       3,  66 May  5  1998 hdb2
brw-rw----   1 root     disk       3,  67 May  5  1998 hdb3
brw-rw----   1 root     disk       3,  68 May  5  1998 hdb4
brw-rw----   1 root     disk       3,  69 May  5  1998 hdb5
brw-rw----   1 root     disk       3,  70 May  5  1998 hdb6
crw-r-----   1 root     kmem       1,   2 May  5  1998 kmem
crw-r-----   1 root     kmem       1,   1 May  5  1998 mem
lrwxrwxrwx   1 root     root           12 Nov  1 15:39 modem -> ttyS1
lrwxrwxrwx   1 root     root           12 Nov  1 15:39 mouse -> psaux
crw-rw-rw-   1 root     root       1,   3 May  5  1998 null
crwxrwxrwx   1 root     root      10,   1 Oct  5 20:22 psaux
brw-r-----   1 root     disk       1,   1 May  5  1998 ram
brw-rw----   1 root     disk       1,   0 May  5  1998 ram0
brw-rw----   1 root     disk       1,   1 May  5  1998 ram1
brw-rw----   1 root     disk       1,   2 May  5  1998 ram2
brw-rw----   1 root     disk       1,   3 May  5  1998 ram3
brw-rw----   1 root     disk       1,   4 May  5  1998 ram4
brw-rw----   1 root     disk       1,   5 May  5  1998 ram5
brw-rw----   1 root     disk       1,   6 May  5  1998 ram6
brw-rw----   1 root     disk       1,   7 May  5  1998 ram7
brw-rw----   1 root     disk       1,   8 May  5  1998 ram8
brw-rw----   1 root     disk       1,   9 May  5  1998 ram9
lrwxrwxrwx   1 root     root            4 Nov  1 15:39 ramdisk -> ram0
***  I have only included devices for the IDE partitions I use.
***  If you use SCSI, then use the /dev/sdXX devices instead.
crw-------   1 root     root       4,   0 May  5  1998 tty0
crw-w-----   1 root     tty        4,   1 Nov  1 15:39 tty1
crw-------   1 root     root       4,   2 Nov  1 15:29 tty2
crw-------   1 root     root       4,   3 Nov  1 15:29 tty3
crw-------   1 root     root       4,   4 Nov  1 15:29 tty4
crw-------   1 root     root       4,   5 Nov  1 15:29 tty5
crw-------   1 root     root       4,   6 Nov  1 15:29 tty6
crw-------   1 root     root       4,   7 May  5  1998 tty7
crw-------   1 root     tty        4,   8 May  5  1998 tty8
crw-------   1 root     tty        4,   9 May  8 12:57 tty9
crw-rw-rw-   1 root     root       4,  65 Nov  1 12:17 ttyS1
crw-rw-rw-   1 root     root       1,   5 May  5  1998 zero

/etc:
-rw-------   1 root     root          164 Nov  1 15:39 conf.modules
-rw-------   1 root     root          668 Nov  1 15:39 fstab
-rw-------   1 root     root           71 Nov  1 15:39 gettydefs
-rw-------   1 root     root          389 Nov  1 15:39 group
-rw-------   1 root     root          413 Nov  1 15:39 inittab
-rw-------   1 root     root           65 Nov  1 15:39 issue
-rw-r--r--   1 root     root          746 Nov  1 15:39 ld.so.cache
-rw-------   1 root     root           32 Nov  1 15:39 motd
-rw-------   1 root     root          949 Nov  1 15:39 nsswitch.conf
drwx--x--x   2 root     root         1024 Nov  1 15:39 pam.d
-rw-------   1 root     root          139 Nov  1 15:39 passwd
-rw-------   1 root     root          516 Nov  1 15:39 profile
-rwx--x--x   1 root     root          387 Nov  1 15:39 rc
-rw-------   1 root     root           55 Nov  1 15:39 shells
-rw-------   1 root     root          774 Nov  1 15:39 termcap
-rw-------   1 root     root           78 Nov  1 15:39 ttytype
lrwxrwxrwx   1 root     root           15 Nov  1 15:39 utmp -> ../var/run/utmp
lrwxrwxrwx   1 root     root           15 Nov  1 15:39 wtmp -> ../var/log/wtmp

/etc/pam.d:
-rw-------   1 root     root          356 Nov  1 15:39 other

/lib:
-rwxr-xr-x   1 root     root        45415 Nov  1 15:39 ld-2.0.7.so
lrwxrwxrwx   1 root     root           11 Nov  1 15:39 ld-linux.so.2 -> ld-2.0.
7.so
-rwxr-xr-x   1 root     root       731548 Nov  1 15:39 libc-2.0.7.so
lrwxrwxrwx   1 root     root           13 Nov  1 15:39 libc.so.6 -> libc-2.0.7.
so
lrwxrwxrwx   1 root     root           17 Nov  1 15:39 libcom_err.so.2 -> libco
m_err.so.2.0
-rwxr-xr-x   1 root     root         6209 Nov  1 15:39 libcom_err.so.2.0
-rwxr-xr-x   1 root     root       153881 Nov  1 15:39 libcrypt-2.0.7.so
lrwxrwxrwx   1 root     root           17 Nov  1 15:39 libcrypt.so.1 -> libcryp
t-2.0.7.so
-rwxr-xr-x   1 root     root        12962 Nov  1 15:39 libdl-2.0.7.so
lrwxrwxrwx   1 root     root           14 Nov  1 15:39 libdl.so.2 -> libdl-2.0.
7.so
lrwxrwxrwx   1 root     root           16 Nov  1 15:39 libext2fs.so.2 -> libext
2fs.so.2.4
-rwxr-xr-x   1 root     root        81382 Nov  1 15:39 libext2fs.so.2.4
-rwxr-xr-x   1 root     root        25222 Nov  1 15:39 libnsl-2.0.7.so
lrwxrwxrwx   1 root     root           15 Nov  1 15:39 libnsl.so.1 -> libnsl-2.
0.7.so
-rwx--x--x   1 root     root       178336 Nov  1 15:39 libnss_files-2.0.7.so
lrwxrwxrwx   1 root     root           21 Nov  1 15:39 libnss_files.so.1 -> lib
nss_files-2.0.7.so
lrwxrwxrwx   1 root     root           14 Nov  1 15:39 libpam.so.0 -> libpam.so
.0.64
-rwxr-xr-x   1 root     root        26906 Nov  1 15:39 libpam.so.0.64
lrwxrwxrwx   1 root     root           19 Nov  1 15:39 libpam_misc.so.0 -> libp
am_misc.so.0.64
-rwxr-xr-x   1 root     root         7086 Nov  1 15:39 libpam_misc.so.0.64
-r-xr-xr-x   1 root     root        35615 Nov  1 15:39 libproc.so.1.2.6
lrwxrwxrwx   1 root     root           15 Nov  1 15:39 libpwdb.so.0 -> libpwdb.
so.0.54
-rw-r-r---   1 root     root       121899 Nov  1 15:39 libpwdb.so.0.54
lrwxrwxrwx   1 root     root           19 Nov  1 15:39 libtermcap.so.2 -> libte
rmcap.so.2.0.8
-rwxr-xr-x   1 root     root        12041 Nov  1 15:39 libtermcap.so.2.0.8
-rwxr-xr-x   1 root     root        12874 Nov  1 15:39 libutil-2.0.7.so
lrwxrwxrwx   1 root     root           16 Nov  1 15:39 libutil.so.1 -> libutil-
2.0.7.so
lrwxrwxrwx   1 root     root           14 Nov  1 15:39 libuuid.so.1 -> libuuid.
so.1.1
-rwxr-xr-x   1 root     root         8039 Nov  1 15:39 libuuid.so.1.1
drwx--x--x   3 root     root         1024 Nov  1 15:39 modules
drwx--x--x   2 root     root         1024 Nov  1 15:39 security

/lib/modules:
drwx--x--x   4 root     root         1024 Nov  1 15:39 2.0.35

/lib/modules/2.0.35:
drwx--x--x   2 root     root         1024 Nov  1 15:39 block
drwx--x--x   2 root     root         1024 Nov  1 15:39 cdrom

/lib/modules/2.0.35/block:
drwx------   1 root     root         7156 Nov  1 15:39 loop.o

/lib/modules/2.0.35/cdrom:
drwx------   1 root     root        24108 Nov  1 15:39 cdu31a.o

/lib/security:
-rwx--x--x   1 root     root         8771 Nov  1 15:39 pam_permit.so

***  Directory stubs for mounting
/mnt:
drwx--x--x   2 root     root         1024 Nov  1 15:39 cdrom
drwx--x--x   2 root     root         1024 Nov  1 15:39 floppy

/proc:

/root:
-rw-------   1 root     root          176 Nov  1 15:39 .bashrc
-rw-------   1 root     root          182 Nov  1 15:39 .cshrc
-rwx--x--x   1 root     root          455 Nov  1 15:39 .profile
-rw-------   1 root     root         4014 Nov  1 15:39 .tcshrc

/sbin:
-rwx--x--x   1 root     root        23976 Nov  1 15:39 depmod
-rwx--x--x   2 root     root       274600 Nov  1 15:39 e2fsck
-rwx--x--x   1 root     root        41268 Nov  1 15:39 fdisk
-rwx--x--x   1 root     root         9396 Nov  1 15:39 fsck
-rwx--x--x   2 root     root       274600 Nov  1 15:39 fsck.ext2
-rwx--x--x   1 root     root        29556 Nov  1 15:39 getty
-rwx--x--x   1 root     root         6620 Nov  1 15:39 halt
-rwx--x--x   1 root     root        23116 Nov  1 15:39 init
-rwx--x--x   1 root     root        25612 Nov  1 15:39 insmod
-rwx--x--x   1 root     root        10368 Nov  1 15:39 kerneld
-rwx--x--x   1 root     root       110400 Nov  1 15:39 ldconfig
-rwx--x--x   1 root     root         6108 Nov  1 15:39 lsmod
-rwx--x--x   2 root     root        17400 Nov  1 15:39 mke2fs
-rwx--x--x   1 root     root         4072 Nov  1 15:39 mkfs
-rwx--x--x   2 root     root        17400 Nov  1 15:39 mkfs.ext2
-rwx--x--x   1 root     root         5664 Nov  1 15:39 mkswap
-rwx--x--x   1 root     root        22032 Nov  1 15:39 modprobe
lrwxrwxrwx   1 root     root            4 Nov  1 15:39 reboot -> halt
-rwx--x--x   1 root     root         7492 Nov  1 15:39 rmmod
-rwx--x--x   1 root     root        12932 Nov  1 15:39 shutdown
lrwxrwxrwx   1 root     root            6 Nov  1 15:39 swapoff -> swapon
-rwx--x--x   1 root     root         5124 Nov  1 15:39 swapon
lrwxrwxrwx   1 root     root            4 Nov  1 15:39 telinit -> init
-rwx--x--x   1 root     root         6944 Nov  1 15:39 update

/tmp:

/usr:
drwx--x--x   2 root     root         1024 Nov  1 15:39 bin
drwx--x--x   2 root     root         1024 Nov  1 15:39 lib
drwx--x--x   3 root     root         1024 Nov  1 15:39 man
drwx--x--x   2 root     root         1024 Nov  1 15:39 sbin
drwx--x--x   3 root     root         1024 Nov  1 15:39 share
lrwxrwxrwx   1 root     root           10 Nov  1 15:39 tmp -> ../var/tmp

/usr/bin:
-rwx--x--x   1 root     root        37164 Nov  1 15:39 afio
-rwx--x--x   1 root     root         5044 Nov  1 15:39 chroot
-rwx--x--x   1 root     root        10656 Nov  1 15:39 cut
-rwx--x--x   1 root     root        63652 Nov  1 15:39 diff
-rwx--x--x   1 root     root        12972 Nov  1 15:39 du
-rwx--x--x   1 root     root        56552 Nov  1 15:39 find
-r-x--x--x   1 root     root         6280 Nov  1 15:39 free
-rwx--x--x   1 root     root         7680 Nov  1 15:39 head
-rwx--x--x   1 root     root         8504 Nov  1 15:39 id
-r-sr-xr-x   1 root     bin          4200 Nov  1 15:39 passwd
-rwx--x--x   1 root     root        14856 Nov  1 15:39 tail
-rwx--x--x   1 root     root        19008 Nov  1 15:39 tr
-rwx--x--x   1 root     root         7160 Nov  1 15:39 wc
-rwx--x--x   1 root     root         4412 Nov  1 15:39 whoami

/usr/lib:
lrwxrwxrwx   1 root     root           17 Nov  1 15:39 libncurses.so.4 -> libnc
urses.so.4.2
-rw-r-r---   1 root     root       260474 Nov  1 15:39 libncurses.so.4.2

/usr/sbin:
-r-x--x--x   1 root     root        13684 Nov  1 15:39 fuser
-rwx--x--x   1 root     root         3876 Nov  1 15:39 mklost+found

/usr/share:
drwx--x--x   4 root     root         1024 Nov  1 15:39 terminfo

/usr/share/terminfo:
drwx--x--x   2 root     root         1024 Nov  1 15:39 l
drwx--x--x   2 root     root         1024 Nov  1 15:39 v

/usr/share/terminfo/l:
-rw-------   1 root     root         1552 Nov  1 15:39 linux
-rw-------   1 root     root         1516 Nov  1 15:39 linux-m
-rw-------   1 root     root         1583 Nov  1 15:39 linux-nic

/usr/share/terminfo/v:
-rw-------   2 root     root         1143 Nov  1 15:39 vt100
-rw-------   2 root     root         1143 Nov  1 15:39 vt100-am

/var:
drwx--x--x   2 root     root         1024 Nov  1 15:39 log
drwx--x--x   2 root     root         1024 Nov  1 15:39 run
drwx--x--x   2 root     root         1024 Nov  1 15:39 tmp

/var/log:
-rw-------   1 root     root            0 Nov  1 15:39 wtmp

/var/run:
-rw-------   1 root     root            0 Nov  1 15:39 utmp

/var/tmp:
     _________________________________________________________________
   
Appendix D. Sample utility disk directory listing

total 579
-rwxr-xr-x   1 root     root        42333 Jul 28 19:05 cpio
-rwxr-xr-x   1 root     root        32844 Aug 28 19:50 debugfs
-rwxr-xr-x   1 root     root       103560 Jul 29 21:31 elvis
-rwxr-xr-x   1 root     root        29536 Jul 28 19:04 fdisk
-rw-r-r---   1 root     root       128254 Jul 28 19:03 ftape.o
-rwxr-xr-x   1 root     root        17564 Jul 25 03:21 ftmt
-rwxr-xr-x   1 root     root        64161 Jul 29 20:47 grep
-rwxr-xr-x   1 root     root        45309 Jul 29 20:48 gzip
-rwxr-xr-x   1 root     root        23560 Jul 28 19:04 insmod
-rwxr-xr-x   1 root     root          118 Jul 28 19:04 lsmod
lrwxrwxrwx   1 root     root            5 Jul 28 19:04 mt -> mt-st
-rwxr-xr-x   1 root     root         9573 Jul 28 19:03 mt-st
lrwxrwxrwx   1 root     root            6 Jul 28 19:05 rmmod -> insmod
-rwxr-xr-x   1 root     root       104085 Jul 28 19:05 tar
lrwxrwxrwx   1 root     root            5 Jul 29 21:35 vi -> elvis

  Notes
  
   [98][1]
   
   The directory structure presented here is for root diskette use only.
   Real Linux systems have a more complex and disciplined set of
   policies, called the [99]Filesystem Hierarchy Standard, for
   determining where files should go.)

References

   1. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#PREMADE
   2. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN21
   3. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN27
   4. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN36
   5. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN51
   6. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN59
   7. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN67
   8. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN87
   9. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN92
  10. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN168
  11. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#BUILDROOT
  12. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN204
  13. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#CREATINGROOTFS
  14. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN328
  15. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#PAMANDNSS
  16. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN649
  17. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN670
  18. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#WRAPPINGITUP
  19. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN697
  20. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN712
  21. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#TRANSFERRINGWITHLILO
  22. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#TRANSFERRINGWITHOUTLILO
  23. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#SETTINGRAMDISKWORD
  24. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN834
  25. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#TROUBLESHOOTING
  26. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN954
  27. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#SLIMFAST
  28. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#NONRAMDISKROOT
  29. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#UTILITYDISK
  30. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#PROS
  31. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1093
  32. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1277
  33. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#PREMADE
  34. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1316
  35. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1323
  36. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1328
  37. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1337
  38. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1343
  39. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1386
  40. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#LISTINGS
  41. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#UTILITYLIST
  42. http://www.croftj.net/~fawcett/Bootdisk-HOWTO/index.html
  43. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#TROUBLESHOOTING
  44. http://linuxdoc.org/copyright.html
  45. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#PREMADE
  46. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#NONRAMDISKROOT
  47. ftp://ftp.win.tue.nl/pub/linux/utils/util-linux/
  48. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#FTN.AEN331
  49. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#LISTINGS
  50. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#LISTINGS
  51. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#UTILITYDISK
  52. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#SLIMFAST
  53. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#SLIMFAST
  54. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#TRANSFERRINGWITHOUTLILO
  55. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#SETTINGRAMDISKWORD
  56. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#PAMANDNSS
  57. http://metalab.unc.edu/pub/Linux/system/shells/
  58. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#UTILITYDISK
  59. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#BUILDROOT
  60. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#UTILITYLIST
  61. file://localhost/usr/src/linux/Documentation/initrd.txt
  62. ftp://elserv.ffm.fgan.de/pub/linux/loadlin-1.6/initrd-example.tgz
  63. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#TRANSFERRINGWITHLILO
  64. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1097
  65. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1104
  66. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1111
  67. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1128
  68. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1143
  69. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1159
  70. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1182
  71. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1205
  72. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1238
  73. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN1265
  74. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#TROUBLESHOOTING
  75. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#PROS
  76. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#CREATINGROOTFS
  77. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#WRAPPINGITUP
  78. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#SETTINGRAMDISKWORD
  79. http://metalab.unc.edu/pub/Linux/distributions/slackware/bootdsks.144/
  80. http://metalab.unc.edu/pub/Linux/distributions/slackware/rootdsks/
  81. http://www.slackware.com/getslack/
  82. http://metalab.unc.edu/pub/Linux/distributions/redhat/current/i386/images/
  83. http://www.redhat.com/mirrors.html
  84. ftp://ftp.debian.org/pub/debian/dists/stable/main/disks-i386/current/
  85. ftp://ftp.debian.org/pub/debian/README.mirrors.html
  86. http://metalab.unc.edu/pub/Linux/system/recovery/!INDEX.html
  87. http://metalab.unc.edu/pub/Linux/system/recovery/images
  88. http://metalab.unc.edu/pub/Linux/system/recovery/!INDEX.html
  89. http://www.croftj.net/~fawcett/yard/index.html
  90. ftp://tsx-11.mit.edu/pub/linux/packages/lilo/
  91. http://linuxdoc.org/FAQ/Linux-FAQ.html
  92. http://linuxdoc.org/HOWTO/HOWTO-INDEX.html
  93. http://linuxdoc.org/
  94. http://linuxdoc.org/LDP/sag/c1596.html
  95. http://metalab.unc.edu/pub/Linux/system/boot/lilo/lilo-t-21.ps.gz
  96. http://www.tamacom.com/tour/linux/index.html
  97. http://metalab.unc.edu/pub/Linux/system/boot/lilo/lilo-u-21.ps.gz
  98. file://localhost/export/sunsite/users/gferg/work/00_Bootdisk-HOWTO.html#AEN331
  99. http://www.pathname.com/fhs/2.0/fhs-toc.html
  The Linux Busmouse HOWTO
  Chris Bagwell, cbagwell@sprynet.com
  v2.0, 14 Feb 2000

  This document describes how to install, configure and use a busmouse
  under Linux.  It lists the supported busmice and attempts to answer
  the most frequently asked questions with regards to busmice.  It also
  contains some pointers for serial mice as well.
  ______________________________________________________________________

  Table of Contents


  1. Introduction.

     1.1 Copyright and Disclaimer.
     1.2 Feedback.
     1.3 Acknowledgements.

  2. Determining your mouse type.

     2.1 Hardware interfaces.
        2.1.1 Inport mice.
        2.1.2 Logitech mice.
        2.1.3 PS/2 mice.
        2.1.4 ATI combo video/mice.
        2.1.5 IBM PC110 palmtop digitizer.
        2.1.6 Apple Desktop Mouse
        2.1.7 Hybrid Mice
     2.2 Mouse protocols.

  3. Getting your mouse working.

     3.1 Setting the mouse interrupt.
        3.1.1 Common IRQ usage. In most cases IRQ4 is used for the first serial port (
        3.1.2 Inport and Logitech mice.
        3.1.3 ATI-XL mice.
        3.1.4 PS/2 mice.
     3.2 Configuring the kernel.
        3.2.1 Compiling the kernel.
        3.2.2 Changing interrupts with newer kernels.
     3.3 The mouse devices.

  4. Using your mouse.

     4.1 Configuring Applications
        4.1.1 Redhat
        4.1.2 Other configurations
     4.2 gpm.
     4.3 XFree86.
     4.4 XFree86 and gpm.

  5. Still can't get your mouse going?

     5.1 Other resources
        5.1.1 3-Button Mouse HOWTO.
        5.1.2 Laptop-HOWTO.
        5.1.3 Wacom Tablet HOWTO The


  ______________________________________________________________________

  1.  Introduction.

  This document is a guide to getting your busmouse working with Linux.
  With the more advanced distributions available today setting up your
  busmouse is generally easy but when you do run into problems this
  document will help give you a better understanding of how to manually
  setup your busmouse hardware and software.

  Busmouse support has been in the kernel for as long as I can remember,
  and hasn't changed much in a long time, so this document should be
  relevant to any version of Linux you're likely to have.


  1.1.  Copyright and Disclaimer.

  This document is Copyright (c) 2000 by Chris Bagwell.  This document
  may be distributed under the terms set forth in the Linux
  Documentation Project License at http://linuxdoc.org/copyright.html.
  Please contact the author if you are unable to get the license.

  The author disclaims all warranties with regard to this document,
  including all implied warranties of merchantability and fitness for a
  certain purpose; in no event shall the author be liable for any
  special, indirect or consequential damages or any damages whatsoever
  resulting from loss of use, data or profits, whether in an action of
  contract, negligence or other tortious action, arising out of or in
  connection with the use of this document.


  1.2.  Feedback.

  If you find any mistakes in this document, have any comments about its
  contents or an update or addition, send them to me at the address
  listed at the top of this howto.


  1.3.  Acknowledgements.

  This howto has been, in the spirit of Linux, a community effort.
  Thanks goes out to Mike Battersby, mib@post.com as he started this
  FAQ.  Any errors are most likely added by me.

  Many thanks go to Johan Myreen for the sections on the PS/2 mice,
  Robert T. Harris for help on the ATI-XL sections and Reuben Sumner for
  miscellaneous info and constructive criticism.

  Thanks also to the multitudes of people who have sent me mouse
  information, fixes or words of encouragement.


  2.  Determining your mouse type.

  There are two separate but important characteristics you will need to
  know about your mouse before you go on: what hardware interface it
  uses and what mouse protocol it uses.


  The hardware interface is the hardware aspect of the mouse, taking
  into account things like which i/o ports it uses and how to check if
  it is installed.  This is the part which the kernel is concerned with
  so that it knows how to read data from the mouse.  For serial mice
  users, this part of the story is easy since their interface is always
  the serial port device drivers.

  The mouse protocol is the software aspect of the mouse.  Applications
  need to know the protocol to interpret the raw mouse data they receive
  from the kernel device driver.



  2.1.  Hardware interfaces.

  The Linux kernel up to the 2.2 series supports four different kinds of
  busmouse hardware interface : Inport (Microsoft), Logitech, PS/2 and
  ATI-XL.

  The 2.4 series of kernel also includes support for several new busmice
  including an IBM PC110 digitizer pad and Apple Desktop mouse.  It also
  contains drivers for USB mice which are sometimes discussed with
  busmice since they fall outside the more common serial driver
  interface.

  There is no sure-fire way of determining your hardware interface.  For
  those that have Windows 9x install, it sometimes helps to go to the
  Systems screen under the control panel and look for devices
  controlling your mouse.  This screen will often tell you the interface
  type and what I/O ports and interrupts this hardware uses.


  2.1.1.  Inport mice.

  Inport mice include most of the old style Microsoft mice which are
  shaped like a bar of dove soap.  Inport mice generally connect to an
  interface card which plugs into the bus on your motherboard.  If the
  plug which connects your mouse cord to the interface card is round,
  has 9 pins, and a notch in one side you likely have an Inport mouse.

  ATI currently claims that all ATI VGA + Busmouse combo cards used the
  Microsoft Inport hardware and thus owners of these cards should first
  attempt using the Microsoft Busmouse driver.

  The ATI-XL VGA+Busmouse card technically is compatible with the
  Microsoft Inport mouse driver but has slightly different provisioning
  for interrupts and therefore has a specific driver for it.  Avoid this
  driver if possible since its not supported very well.


  2.1.2.  Logitech mice.

  Logitech mice in general appear almost exactly the same as Inport
  mice.  They too connect to an interface card via a 9 pin mini-din
  connector.  Hopefully, it will have come in a Logitech box or have
  ``Logitech'' printed on the connector card so that you can tell it
  actually is a Logitech mouse.


  2.1.3.  PS/2 mice.

  The PS/2 mouse interface is not on an expansion card, the mouse is
  connected to the PS/2 port on the keyboard controller.  This is
  sometimes located on the keyboard or more often as a extra port
  somehow connected directly to the computer case.

  A PS/2 port uses a 6-pin mini DIN connector, similar to the keyboard
  connector.  Many laptops also use this kind of interface to their
  trackballs or touchpads; in this case the mouse is internally
  connected to the PS/2 port and needs no connector.


  2.1.4.  ATI combo video/mice.

  ATI-XL mice are a variant of Inport mice, with some slight differences
  in interrupt setup.  They come on the ATI-XL combined video
  adaptor/mouse card.  Unless you know you have an ATI-XL card (and thus
  an ATI-XL mouse), you probably don't have one of these. It is possible
  for ATI-XL mice to use either the ATI-XL or Inport kernel drivers,
  although the ATI-XL driver should give better results.

  There is also an older ATI video adaptor/mouse card called either ATI
  VGA1024 or ATI VGA Wonder.  These cards are setup the same as the ATI-
  XL but use the Logitech mouse protocol.  For these mice the hardware
  drivers are setup the same as for the ATI-XL but you setup the
  software the same as the Logitech mice.


  2.1.5.  IBM PC110 palmtop digitizer.

  The IBM PC110 palmtop contains a digitizer pad that can be used to
  emulate a mouse using the PS/2 mouse protocol.  In this case, you set
  up the hardware interface using the IBM PC110 device driver and setup
  software as you would a PS/2 mouse.


  2.1.6.  Apple Desktop Mouse

  This bus mouse is common on Macintoshes and uses a 4 pin connector.
  The hardware is controled by the kernel but application software will
  need to be aware of its unique mouse protocol.  I am currently unaware
  of what protocols this mouse uses.


  2.1.7.  Hybrid Mice

  The period of time that busmouse were popular, the hardware companies
  were experementing alot and cross-licensing each others designs.
  Therefore, you could have a ATI mouse that uses the Logitech hardware
  interface or a Logitech that uses the Inport interface.  If you know
  you have a busmouse but can't get it working with the standard
  interface/protocol setups, try experementing by using different
  interface device drivers with different mouse protocols.


  2.2.  Mouse protocols.

  The PC world is full of different and conflicting mouse protocols.
  Fortunately, the choice for bus mice is considerable smaller than that
  for serial mice.  Most Inport, Logitech and ATI-XL mice use the
  ``BusMouse'' protocol, although there are some ancient Logitech mice
  which use the ``MouseSystems'' protocol, and some even older Microsoft
  mice which use the ''Logitech'' protocol.  PS/2 mice will always use
  the ``PS/2'' protocol.


  3.  Getting your mouse working.

  Once you have figured out your mouse interface and protocol types,
  you're ready to proceed.


  3.1.  Setting the mouse interrupt.

  Now that you've found out what hardware interface your mouse uses
  you'll need to know which interrupt number your mouse is using, and
  make sure it doesn't conflict with any other peripherals you have
  installed.  That last part deserves to be repeated!  Make sure that it
  does not conflict with any other peripherals you have installed!

  You should make sure that your mouse is not trying to use the same
  interrupt as any of your other devices --- it is not possible for the
  mouse to share an interrupt under Linux, even though it may work fine
  under other operating systems.  Check the documentation for all your
  peripherals to see which interrupt they use.
  Under Linux, busmice don't register which IRQ they are using until
  after they have been opened by an application that makes use of them.
  Plug-n-Play hardware often registers their interrupts during bootup.
  This creates a possibility for a Plug-n-Play peice of hardware to
  steal the IRQ away from your mouse.  Bear in mind that other operating
  systems may be initialzing the P-n-P cards to an IRQ that is not in
  conflict with your busmouse but things may not work out as nicely
  under Linux.  It is up to you to make sure there are no IRQ conflicts
  between all of your equipment.

  3.1.1.  In most cases IRQ4 is used for the first serial port (
  /dev/ttyS0 ), IRQ3 for the second ( /dev/ttyS1 ) (these are assuming
  you actually have such devices --- if you don't you can happily use
  their IRQ's), IRQ5 for some SCSI adaptors, and IRQ12 for some network
  cards.  Having a other card use IRQ12 is a big problem for machines
  with PS/2 ports as you are forced to use IRQ12 only for the PS/2 port.
  For ATI-XL, Inport and Logitech mice the kernel default is to use
  IRQ5, so if you are stuck with a pre-compiled kernel (eg, CD-ROM
  users) you will have to use that.  If you are using an Inport or Log-
  itech mice with a newer kernel you may be able to pass a command line
  option to the kernel to tell it what interrupt to use without recom-
  piling.  Common IRQ usage.

  3.1.2.  Inport and Logitech mice.

  If you open up your computer's case and look at the card which your
  mouse plugs into, you should notice a block of jumpers on the card
  (hopefully labeled ``INTERRUPT'') with positions for interrupt
  (otherwise known as IRQ) numbers 2,3,4 and 5.  To change the interrupt
  simply move the jumper from its current position onto the correct pair
  of pins.



       ***     MAKE SURE YOUR COMPUTER IS TURNED OFF   ***
       ***     BEFORE CHANGING THE JUMPERS AROUND!!    ***



  3.1.3.  ATI-XL mice.

  ATI-XL and a few other ATI busmice have a software selectable IRQ -
  you should have received with your mouse a MS-DOS program (VSETUP.EXE)
  to set the IRQ.  In order to do so you must (temporarily) boot MS-DOS
  and run this program.  Note that the VSETUP program takes an optional
  parameter ``/70'' to increase the vertical refresh rate (which results
  in less flicker).  The VSETUP program also allows you to select either
  the primary or secondary mouse address - you should set this to the
  primary address or the kernel will not be able to detect your mouse.

  Once VSETUP has been run you must perform a hard reset for the new
  configuration to take effect.


  3.1.4.  PS/2 mice.

  The PS/2 mouse always uses IRQ12 -- there is no way of changing this
  (except with a soldering gun.)  In the rare case that some other
  device is using IRQ12, you'll have to rejumper that peripheral to use
  another IRQ number.



  3.2.  Configuring the kernel.

  In order for your busmouse to operate correctly you will need to
  configure your kernel to compile in busmouse support.  If you are
  using a pre-compiled kernel then it often comes with support for all
  three busmouse included.  This may still not be enough.  The kernel
  could be trying to use the wrong interrupt or the detection can get
  confused and treat your mouse as the wrong type.


  In pre-2.4 series kernel there is no support for auto-detection of the
  IRQ that a busmouse is using.  Therefore, if your card is set to
  anything but the kernel default value of IRQ 5 then you will need to
  let the kernel know what IRQ to use instead.  This can be done in one
  of two ways.  The easiest is to pass command line options to the
  kernel during the bootup process.  Please read thru the "Compiling the
  kernel" section for reference but attempt the directions in "Changing
  interrupts with newer kernels" first.  Also of great help in this area
  is the Bootprompt-HOWTO, avaliable at most sites that carry this
  HOWTO.


  3.2.1.  Compiling the kernel.

  Change to your kernel directory (here assumed to be (/usr/src/linux)
  and do a


       make config


  If you are unsure as to your mouse type, the first time you recompile
  the kernel you may wish to enable all of the busmouse options in the
  hope that the kernel will autodetect your mouse properly.  People have
  mixed success with this: it doesn't always work, but on the other hand
  it might save you any further compiles.

  Answer ``y'' or ''m'' to the question pertaining to your type of
  busmouse interface and ``n'' to all the other busmouse questions.  Use
  the ''m'' option if you have your system setup to support loading
  kernel modules if you do not or do not know what that means then it
  will be safe to always answer ''y'' to have the support directly
  compiled into your kernel.

  As an example, if you have an Inport mouse you should answer ``y'' to


       Microsoft busmouse support


  and ``n'' to all other busmouse questions.  Answer the non-mouse
  related questions as you usually would.

  To compile the kernel with PS/2 mouse support answer ``y'' to the
  question.


       PS/2 mouse (aka "auxiliary device") support


  The PS/2 mouse driver actually supports two kinds of devices: the
  standard PS/2 Auxiliary Device controller and a special PS/2 mouse
  interface chip from Chips & Technologies which is used in the Texas
  Instruments Travelmate and Gateway Nomad laptops.  To compile in
  support for the trackballs on these computers, answer ``y'' to the

       C&T 82C710 mouse port support (as on TI Travelmate)


  question.  Note that you will still have to answer ``y'' to the
  question about the standard PS/2 driver to even get a chance to answer
  this question, since the 82C710 driver is actually an add-on to the
  standard PS/2 mouse driver.

  When configured both for a standard PS/2 mouse device and the 82C710
  device, the driver first tries to locate a 82C710 chip at boot time.
  Failing this, the standard driver is used instead, so using a kernel
  configured for both types of interface on a machine with a standard
  PS/2 mouse port should work too.  However, there has been one report
  of a falsely detected 82C710 chip, so to be on the safe side do not
  configure in support for the 82C710 if you don't need it.

  You will now need to tell the kernel what interrupt your mouse uses.
  You can skip this step if your using a PS/2 mouse as it always uses
  IRQ 12.

  If you have a Logitech, Inport mouse, or an ATI mouse that uses the
  Logitech protocol, edit the file
  /usr/src/linux/include/linux/busmouse.h and change the line which says


       #define MOUSE_IRQ               5


  to reflect the interrupt number for your mouse (see the section
  ``Setting the mouse interrupt'' for details on finding your interrupt
  number).

  If you have an ATI-XL mouse, edit the file
  /usr/src/linux/drivers/char/atixlmouse.c and change the line which
  says


       #define ATIXL_MOUSE_IRQ         5


  to reflect your mouse's interrupt number.

  Due to the vagaries of the PC architecture, if you have set your mouse
  to use interrupt 2, you must set the #define to use interrupt 9.

  Examples

  For a mouse on interrupt 3, you should change the line to read


       #define MOUSE_IRQ               3


  For a mouse on interrupt 2, you should change the line to read


       #define MOUSE_IRQ               9


  Next, compile your kernel as per the instructions which come with it,
  and boot from the new kernel.  You should now have the busmouse
  support correctly compiled in.



  3.2.2.  Changing interrupts with newer kernels.

  The steps to compile into the kernel what interrupt it uses works with
  any version of the kernel to date.  Newer kernels (starting somewhere
  in  the 2.x.x's) allow you to pass arguments to the kernel during load
  time using something like LILO or LOADLIN to specify the interrupt
  number for Logitech and Microsoft Inport mice. This can be a real time
  saver as you do not need to recompiler your kernel (or know how to).
  If you've configured your kernel to load the mouse drivers as modules
  then you will need to pass this information when loading the module.

  You can add the following options to your boot line in LILO to change
  interrupt:



       bmouse=3  (Logitech Busmice)
       msmouse=3 (for Microsoft Inport mice)



  Substitute the 3 with your mouse's actual interrupt. An example of
  using this with lilo is:


       LILO:linux msmouse=3


  You can consult your LILO or LOADLIN docs to see how to add this type
  information to their configuration files so that you do not need to
  type it.

  If your system uses kerneld to auto load modules, you can edit your
  /etc/conf.modules or /etc/modules.conf file, which ever your system
  uses, and add one of the following lines.



       options msbusmouse mouse_irq=3
       options busmouse mouse_irq=3



  3.3.  The mouse devices.

  Mice under Linux are accessed via the devices in the /dev directory.
  The following table gives a list of interface types and which device
  you should use.



       INTERFACE        DEVICE        MAJOR    MINOR
       ---------------------------------------------
       Logitech        /dev/logibm      10      0
       PS/2            /dev/psaux       10      1
       Inport          /dev/inportbm    10      2
       ATI-XL          /dev/atibm       10      3



  Note: If you are using your ATI-XL mouse with the Inport driver, you
  should use the inportbm device, not the /dev/atibm device.
  The major and minor entries are the device numbers for that particular
  device.

  If you find that you do not have these devices, you should create them
  first.  To do so, execute the following as root.



       mknod /dev/logimm    c 10 0
       mknod /dev/psaux     c 10 1
       mknod /dev/inportbm  c 10 2
       mknod /dev/atibm     c 10 3



  Note: Some time in the (progressively less) recent history of Linux
  the names for the busmouse devices have changed.  The following device
  names have been superceded by those above and should be removed:
  bmousems, bmouseps2, bmouseatixl, and bmouselogitech..

  Many people like to create a symbolic link from their mouse device to
  /dev/mouse so that they don't have to remember which device they need
  to be using.  If you have one of the current Linux distributions you
  will almost certainly find that you have such a link.  If you have
  such a link, or create one, you should make sure that it is pointing
  to the correct device for your mouse.


  4.  Using your mouse.

  This section deals with the general use of your mouse with various
  applications.


  4.1.  Configuring Applications

  Most Linux Distributions will prompt you for what type of mouse you
  have during installation and then setup both gpm and X windows for
  you.  If you do not select the correct mouse protocol or install new
  mouse hardware you can usually run a command line program that will
  reconfigure both gpm and X windows to use the new protocol.


  4.1.1.  Redhat

  Under RedHat you can run /usr/sbin/mouseconfig.


  4.1.2.  Other configurations

  Consult your installation documents for what to run under other
  distributions.


  4.2.  gpm.

  Gpm is a program which allows you to do mouse based 'cut- and-paste'
  between Linux virtual consoles, much like you can under X, and is a
  good way of testing your mouse out.  The most current version of gpm
  can be found at ftp://ftp.prosa.it/pub/gpm.  Most Linux distributions
  come with a precompiled gpm binary.

  When invoking gpm, use the -t switch to indicate which protocol your
  mouse is using and the -m option to indicate which mouse device you
  are using.  Three protocols useful for most busmice are logi, bm, and
  ps2.  The default for mouse device is to use /dev/mouse, so you can
  omit the -m option if you have the appropriate symbolic link.  An
  example for a Microsoft Inport mouse is:


       gpm -t bm


  or if you use the PS/2 protocol:


       gpm -t ps2


  You should then be able move your mouse and see a block move around
  the screen and also be able to cut and paste text between virtual
  consoles using the mouse buttons.  Read the documentation with gpm, or
  do a ``man gpm'' for more information on how to operate it.


  4.3.  XFree86.

  To use your busmouse under XFree86, you will need to set your mouse
  protocol type in your Xconfig file. If you have a BusMouse protocol
  mouse, your Xconfig should contain (including the quotes)



       Section "Pointer"
         Protocol "Busmouse"
         Device   "/dev/mouse"

         # Any other options such as Emulate3Buttons
       EndSection



  For PS/2 mice change the protocol line to:



         Protocol        "PS/2"



  If you have a two button mouse, it should also contain the line



         Emulate3Buttons



  which will allow you to emulate the use of the middle mouse button by
  pressing both mouse buttons simultaneously.  All other mouse related
  lines, such as ``BaudRate'' and ``SampleRate'' should be commented
  out, as these have no effect on bus mice.


  4.4.  XFree86 and gpm.

  For a long period of the kernel developement, it was not possible to
  share busmice between processes.  Because of this it was hard to run
  both XFree86 and gpm at the same time. If you try to run X with gpm
  running and you get errors like the following then you know you are
  using one of these older kernels.



       Fatal server error:
       Cannot open mouse (Device or resource busy)



  There are two meathods of getting gpm working with XFree86 with these
  kernels.  The first is to kill any copy of gpm you have running before
  you start up XFree86.  The second is to use gpm's "repeater" option
  (it takes mouse data and repeats the information to multiple
  applications).

  I would recommend upgrading your kernel if possible so that you can
  share busmice between processes.  For this document, I will only
  explain the simplest meathod of using XFree86 and gpm together with
  older kernels.  Please see gpm's documentation if you would like to
  use the repeater meathod.

  Gpm allows you to terminate running copies of itself by executing:


       gpm -k


  This should be done before starting up X11.  Take whatever script you
  use to start up your X session, such as startx, and add the above
  command to the top of the script so that gpm is shut down
  automatically.  You may wish to also put a command that restarts gpm
  at the bottom of the script so that it restarts upon exiting your X
  session.


  5.  Still can't get your mouse going?

  So you've read through this howto a dozen times, done everything
  exactly as you think you should have, and your mouse still doesn't
  work?  The best advice I can give you is this: experiment.  Sure, it's
  a pain in the posterior, but in the end the only way to find out what
  is going to work with your mouse is to try all of the alternatives
  until you have success.

  As always, if there is something you don't understand, try reading the
  manual page first and see if that helps.  If you have a specific
  question, or a problem you think I might be able to help with, feel
  free to contact me at the address listed at the top of this howto, and
  I'll see if I can help you out or point you to someone who can.

  The comp.os.linux.setup newsgroup or comp.os.linux.hardware is the
  appropriate forum for discussion and/or questions regarding setup ---
  please don't post questions to other groups, and especially don't
  crosspost questions to two or more of the Linux groups, they are more
  than cluttered enough as it is!  When posting, you will get a much
  better response (and much fewer flames) if you use appropriate
  Subject: and Keywords: lines.  For example:



       Subject: BUSMICE - Gateway 2000 mouse wont work.
       Keywords: mouse busmouse gateway

  5.1.  Other resources

  The following are useful resources for information regarding Linux and
  Mice.


  5.1.1.  3-Button Mouse HOWTO.

  The 3-Button Mouse HOWTO is of great use if you have a combination
  3-Button Serial/PS2 mouse.  It gives pointers on how to get the middle
  button working.


  5.1.2.  Laptop-HOWTO.

  The Linux Laptop-HOWTO gives pointers on getting external mice working
  along with your built in mouse.


  5.1.3.  TheWacom Tablet HOWTO gives pointers on using the Wacom Tabel
  as a mouse.  Wacom Tablet HOWTO



  C++ Programming HOW-TO
  Al Dev (Alavoor Vasudevan)        alavoor@yahoo.com
  v12.0, 10 July 2000

  This document discusses methods to avoid memory problems in C++ and
  also will help you to program properly in C++ language. The informa
  tion in this document applies to all the operating systems that is -
  Linux, MS DOS, BeOS, Apple Macintosh OS, Windows 95/98/NT/2000, OS/2,
  IBM OSes (MVS, AS/400 etc..), VAX VMS, Novell Netware, all flavors of
  Unix like Solaris, HPUX, AIX, SCO, Sinix, BSD, etc.. and to all other
  operating systems which support "C++" compiler (it means almost all
  the operating systems on this planet!).
  ______________________________________________________________________

  Table of Contents



  1. Introduction

     1.1 Problems facing the current C++ compilers
     1.2 Which one "C", "C++" or Java ?

  2. Download String

  3. Usage of String class

     3.1 Operators
     3.2 Functions
     3.3 Miscellaneous Functions

  4. C++ Zap (Delete) command

  5. Pointers are problems

  6. Usage of my_malloc and my_free

  7. Debug files

  8. C++ Online-Docs

     8.1 C++ Tutorials
     8.2 C++ Coding Standards
     8.3 C++ Quick-Reference
     8.4 C++ Usenet Newsgroups

  9. Memory Tools

  10. Related URLs

  11. Other Formats of this Document

  12. Copyright

  13. Appendix A example_String.cpp

  14. Appendix B String.h

  15. Appendix C String.cpp

  16. Appendix D my_malloc.cpp

  17. Appendix E my_malloc.h

  18. Appendix F debug.h

  19. Appendix G debug.cpp

  20. Appendix H Makefile



  ______________________________________________________________________

  1.  Introduction

  C++ is the most popular language and will be used for a long time in
  the future inspite of emergence of Java. C++ runs extremely fast and
  is in fact  10 to 20 times FASTER than  Java. Java runs very slow
  because it is an byte-code-interpreted language running on top of
  "virtual machine".  Java runs faster with JIT compiler but is still
  slower than C++. And optimized C++ program is about 3 to 4 times
  faster than Java using the JIT (Just-In-Time) compiler!! The memory
  management in Java is automated, so that programmers do not directly
  deal with memory allocations. This document attempts to automate the
  memory management in C++ to make it much more easy to use.  A neat
  feature of Java is that memory allocations are taken care of
  automatically.  This howto will enable "C++" to "compete/imitate" with
  Java language in memory management.

  Because of manual memory allocations, debugging the C++ programs
  consumes a major portion of time. The information in this document
  will give you some better ideas and tips to reduce the debugging time.

  1.1.  Problems facing the current C++ compilers

  Since C++ is super-set of C, it got all the bad features of "C"
  language.

  For example, in "C" programming - memory leaks, memory overflows are
  very common due to usage of features like -

  ______________________________________________________________________
          Datatype  char * and char[]
          String functions like strcpy, strcat, strncpy, strncat, etc..
          Memory functions like malloc, realloc, strdup, etc..
  ______________________________________________________________________



  The usage of char * and strcpy causes horrible memory problems due to
  "overflow", "fence past errors", "step-on-others-toe" (hurting other
  variable's memory locations) or  "memory leaks".  The memory problems
  are extremely hard to debug and are very time consuming to fix and
  trouble-shoot. Memory problems bring down the productivity of
  programmers. This document helps in increasing the productivity of
  programmers via different methods addressed to solve the memory
  defects in "C++".  Memory related bugs are very tough to crack, and
  even experienced programmers take several days, weeks or months to
  debug memory related problems. Many times memory bugs will be "hiding"
  in the code for several months and can cause unexpected program
  crashes!! The usage of char * in C++ is costing USA and Japan $2
  billion every year in time lost in debugging and downtime of programs.
  If you use char * in C++ then it is a very costly affair especially if
  your programs have more than 50,000 lines of code.

  Hence, the following techniques are proposed to overcome the faults of
  "C" language.

  It is proposed that C++ compilers should prevent the programmers from
  using the "char *" , "char[]" datatypes and functions like strcpy,
  strcat, strncpy, strncat.  The datatypes like char *, char[] and
  functions like strcpy, strcat are evil and must be completetly BANNED
  from usage in C++!!  The "char *" is like smallpox virus and it must
  be eradicated from C++ world!!  If you want to use "char *" as in some
  system functions than you should use "C" language. You would put all
  your "C" programs in a separate file and link to "C++" programs using
  the linkage-specification statement extern "C"  -

  ______________________________________________________________________
  extern "C" {
  #include <stdlib.h>
  }

  extern "C" {
          comp();
          some_c_function();
  }
  ______________________________________________________________________

  The extern "C" statement says that everything within the brace-sur
  rounded block - in this case, everything in the header file and
  comp(), some_c_function() is compiled by a C compiler.

  Instead of using char * and char[] all the C++ programmers MUST use
  the which is given in this document and included in the STDLIB.  The
  utilises the constructor and destructor to automate the memory
  management and also provides many functions like ltrim, substring,
  etc..

  See also related in the C++ compiler. The string class is part of the
  standard GNU C++ library and provides lot of string manipulation
  functions. The can remove the need of char * datatype.  Also, C++
  programmers must be encouraged to use 'new', 'delete' features instead
  of using 'malloc' or 'free'.

  The does everything that char * or char [] does. It can completely
  replace char datatype. Plus added benefit is that programmers do not
  have to worry about the memory problems and memory allocation at all!!

  The GNU C++ compiler MUST drop off the support of char *, char[]
  datatypes and in order to compile older programs using char datatype,
  the compiler should provide a additional option called "-fchar-
  datatype" to g++ command.  Over the next 2 years all the C++ programs
  will use and and there will be no char * and char[]. The compiler
  should try to prevent bad programming practices!

  1.2.  Which one "C", "C++" or Java ?

  It is recommended you do programming in object-oriented "C++" for all
  your application programming or general purpose programming. You can
  take full advantage of object oriented facilities of C++. The C++
  compiler is lot more complex than "C" compiler and C++ programs may
  run bit slower than "C" programs. But speed difference between "C" and
  "C++" is very minute - it could be few milli-seconds which may have
  little impact for real-time programming.  Since computer hardware is
  becoming cheaper and faster and memory "C" as time saved in clarity
  and re-usability of C++ code offsets the slow speed.  Compiler
  optimizer options like -O or -O3 can speed up C++/C which is not
  available in Java.

  Nowadays, "C" language is primarily used for "systems programming" to
  develop operating systems, device drivers etc..

  Java is platform independent language more suitable for developing GUI
  running inside web-browsers (Java applets) but runs very slow. Prefer
  to use web-server-side programming "Fast-CGI" with C++ and HTML,
  DHTML, XML to get better performance. Hence, the golden rule is "Web-
  server side programming use C++ and web-client side (browser)
  programming use Java applets". The reason is - the server-side OS
  (Linux) is under your control and never changes and you will never
  know what the client side web-browser OS is. It can be Internet
  appliance device (embedded linux+netscape) or computers running
  Windows 95/98/NT/2000 or Linux, Apple Mac, OS/2, Netware, Solaris
  etc..

  The greatness of Java language is that you can create "Applets (GUI)"
  which can run on any client OS platform!  Java was created to replace
  the Microsoft Windows 95/NT GUI APIs like MS Visual Basic or MS Visual
  C++.  In other words - "Java is the cross-platform Windows-GUI API
  language of next century".  Many web-browsers like Netscape supports
  Java applents and web-browser like Hot Java is written in java itself.
  But the price you pay for cross-platform portability is the
  performance, applications written in Java run very slow.


  Hence, Java runs on "client" and C++ runs on servers!!

  2.  Download String

  All the programs, examples are given in Appendix of this document.
  You can download as a single tar zip, the String class, libraries and
  example programs from

    Go here and click on C++Programming howto.tar.gz file
     <http://www.aldev.8m.com>

    Mirror site :  <http://aldev.webjump.com>

  3.  Usage of String class

  To use String class, you should first refer to a sample program
  "example_String.cpp" given in ``Appendix A'' and the String class
  which is given in ``Appendix B''.

  The is a complete replacement for char and char * datatype.  You can
  use just like char and get much more functionalities.  You should link
  with the library 'libString.a' which you can build from the makefile
  given in ``Appendix H'' and copy the library to /usr/lib or /lib
  directory where all the "C++" libraries are located. To use the
  'libString.a' compile your programs like -

  ______________________________________________________________________
          g++ example.cpp -lString
  ______________________________________________________________________


  See illustration sample code as given below -

  ______________________________________________________________________
          String aa;

          aa = " Washington DC is the capital of USA ";

          // You can use aa.val() like a 'char *' variable in programs !!
          for (unsigned long tmpii = 0; tmpii < aa.length(); tmpii++)
          {
                  //fprintf(stdout, "aa.val()[%ld]=%c ", tmpii, aa.val()[tmpii]);
                  fprintf(stdout, "aa[%ld]=%c ", tmpii, aa[tmpii]);
          }

          // Using pointers on 'char *' val ...
          for (char *tmpcc = aa.val(); *tmpcc != 0; tmpcc++)
          {
                  fprintf(stdout, "aa.val()=%c ", *tmpcc);
          }
  ______________________________________________________________________



  3.1.  Operators

  The provides these operators :-

    Equal to ==

    Not equal to !=

    Assignment =

    Add to itself and Assignment +=

    String concatenation or addition +

     For example to use operators -

     ___________________________________________________________________
             String aa;
             String bb("Bill Clinton");

             aa = "put some value string";  // assignment operator
             aa += "add some more"; // Add to itself and assign operator
             aa = "My name is" + " Alavoor Vasudevan "; // string cat operator

             if (bb == "Bill Clinton")  // boolean equal to operator
                     cout << "bb is eqaul to 'Bill Clinton' " << endl;

             if (bb != "Al Gore")   // boolean 'not equal' to operator
                     cout << "bb is not equal to 'Al Gore'" << endl;
     ___________________________________________________________________



  3.2.  Functions

  The functions provided by String class has the same name as that Java
  language's String class. The function names and the behaviour is
  exactly same as that of Java's string class!! This will facilitate
  portability of code between Java and C++ (you can cut and paste and do
  minimum changes to code).

  The provides these Java like functions :-

    Current string length length()

    char charAt(int where);

    void getChars(int sourceStart, int sourceEnd, char target, int
     targetStart);

    char* toCharArray();

    bool equals(String str2); // See also == operator

    bool equals(char *str2); // See also == operator

    bool equalsIgnoreCase(String str2);

    bool regionMatches(int startIndex, String str2, int str2StartIndex,
     int numChars);

    bool regionMatches(bool ignoreCase, int startIndex, String str2,
     int str2StartIndex, int numChars);

    String toUpperCase();

    String toLowerCase();

    bool startsWith(String str2);

    bool startsWith(char *str2);

    bool endsWith(String str2);

    bool endsWith(char *str2);

    int compareTo(String str2);

    int compareTo(char *str2);

    int compareToIgnoreCase(String str2);

    int compareToIgnoreCase(char *str2);

    int indexOf(char ch, int startIndex = 0);

    int indexOf(char *str2, int startIndex = 0);

    int indexOf(String str2, int startIndex = 0);

    int lastIndexOf(char ch, int startIndex = 0);

    int lastIndexOf(char *str2, int startIndex = 0);

    int lastIndexOf(String str2, int startIndex = 0);

    String substring(int startIndex, int endIndex = 0);

    String replace(char original, char replacement);

    String replace(char *original, char *replacement);

    String trim(); // See also overloaded trim()

    String concat(String str2);  // See also operator +

    String concat(char *str2); // See also operator +

    String append(String str2) {return concat(str2);} // See also
     operator +

    String append(char *str2) {return concat(str2);} // See also
     operator +

    String append(int bb) {return (*this + bb);} // See also operator +

    String append(unsigned long bb) {return (*this + bb);} // See also
     operator +

    String append(float bb) {return (*this + bb);} // See also operator
     +

    String insert(int index, String str2);

    String insert(int index, char ch);

    String reverse(); // See also overloaded reverse()

    String deleteCharAt(int loc);

    String deleteStr(int startIndex, int endIndex); // Java's
     "delete()"

  These are addional functions which are not available in Java.

    Left trim the string. Remove leading white-spaces - newlines, tabs
     ltrim()

    Right trim the string. Remove trailing white-spaces - newlines,
     tabs rtrim()

    Remove trailing and leading white-spaces trim()


    Remove trailing newlines chop()

    Change string to upper case to_upper()

    Change string to lower case to_lower()

    Truncate or round-off the float value roundf(float input_val, short
     precision)

    Truncate or round-off the double value roundd(double input_val,
     short precision)

    Find position, matching substr beginning from start pos(char
     *substr, unsigned long start)

    Explodes the string and returns the list in the list-head pointer
     explodeH explode(char *seperator)

    Implodes the strings in the list-head pointer explodeH and returns
     the String variable implode(char *glue)

    Joins the strings in the list-head pointer explodeH and returns the
     String variable join(char *glue)

    Repeat the input string n times repeat(char *input, unsigned int
     multiplier)

    Replace all occurences of string 'needle' with 'str' in the
     haystack 'val' replace(char *needle, char *str)

    Translate certain chars str_tr(char *from, char *to)

    Center the text string center(int length, char padchar = ' ')

    Formats the original string by placing 'number' of 'padchar'
     characters between each set of blank-delimited words. Leading and
     Trailing blanks are always removed. If 'number' is omitted or is 0,
     then all spaces are in the string are removed. The default number
     is 0 and default padchar ' ' space(int number = 0, char padchar = '
     ')

    The result is string comprised of all characters between and
     including 'start' and 'end' xrange(char start, char end)

    Removes any characters contained in 'list'. The default character
     for 'list' is a blank ' ' compress(char *list)

    Deletes a portion of string of 'length' characters from 'start'
     position.  If start is greater than the string length than string
     is unchanged delstr(int start, int length)

    The 'newstr' in inserted into val beginning at 'start'. The
     'newstr' will be padded or truncated to 'length' characters. The
     default 'length' is string length of newstr insert(char *newstr,
     int start = 0, int length = 0, char padchar = ' ')

    The result is string of 'length' chars madeup of leftmost chars in
     val.  Quick way to left justify a string left(int length = 0, char
     padchar = ' ')

    The result is string of 'length' chars madeup of rightmost chars in
     val.  Quick way to right justify a string right(int length = 0,
     char padchar = ' ')

    The 'newstr' in overlayed into val beginning at 'start'. The
     'newstr' will be padded or truncated to 'length' characters. The
     default 'length' is string length of newstr overlay(char *newstr,
     int start = 0, int length = 0, char padchar = ' ')

    Sub-string, extract a portion of string substr(int start, int
     length = 0)

    matches first match of regx at(char *regx)

    Returns string before regx before(char *regx)

    Returns string after regx after(char *regx)

    Returns true if string is NULL value bool isnull()

    Resets the string to NULL clear()

  3.3.  Miscellaneous Functions

  Some miscellaneous String functions are given here, but DO NOT USE
  these, and instead use operators  like '+', '+=', '==' etc.. These are
  'private' members of the 'String' class.

    Copy string str_cpy(char *bb)

    Long integer converted to string str_cpy(unsigned long bb)

    Integer converted to string str_cpy(int bb)

    Float converted to string str_cpy(float bb)

    String concatenate a char * str_cat(char *bb)

    String concatenate a int str_cat(int bb)

    String concatenate a int str_cat(unsigned long bb)

    String concatenate a float str_cat(float bb)

    Is equal to String ? bool equalto(const String & rhs, bool type =
     false)

    Is equal to char* ? bool equalto(const char *rhs, bool type =
     false)

     For example to convert integer to string do -

     ___________________________________________________________________
             String  aa;

             aa = 34;  // The '=' operator will convert int to string
             cout << "The value of aa is : " << aa.val() << endl;

             aa = 234.878;  // The '=' operator will convert float to string
             cout << "The value of aa is : " << aa.val() << endl;

             aa = 34 + 234.878;
             cout << "The value of aa is : " << aa.val() << endl;
             // The output aa will be '268.878'

             // You must cast String to convert
             aa = (String) 34 + " Honourable President Ronald Reagan " + 234.878;
             cout << "The value of aa is : " << aa.val() << endl;
             // The output aa will be '34 Honourable President Ronald Reagan 234.878'
     ___________________________________________________________________


  4.  C++ Zap (Delete) command

  The delete and new commands in C++ are much better than the malloc and
  free functions of "C".  Consider using new and zap (delete command)
  instead of malloc and free as much as possible.

  To make delete command even more cleaner, make a Zap() command. Define
  a zap() command like this:

  ______________________________________________________________________
  /*
  ** Use do while to make it robust and bullet-proof macro.
  ** For example, if "do-while" is NOT used then results will be
  ** something else just as in -
  ** if (bbint == 4)
  **              aa = 0
  ** else
  **              zap(aptr);  // Problem!! aptr will be always set to NULL
  */

  #define zap(x) do { delete(x); x = NULL; } while (0)
  ______________________________________________________________________



  The zap() command will delete the pointer and set it NULL.  This will
  ensure that even if multiple zap()'s are called on the same deleted
  pointer then the program will not crash. For example -


  ______________________________________________________________________
          zap(pFirstname);
          zap(pFirstname); // no core dumps !! Because pFirstname is NULL now
          zap(pFirstname); // no core dumps !! Because pFirstname is NULL now

          zap(pLastname);
          zap(pJobDescription);
  ______________________________________________________________________



  There is nothing magical about this, it just saves repetative code,
  saves typing time and makes programs more readable. The C++
  programmers often forget to reset the deleted pointer to NULL, and
  this causes annoying problems causing core dumps and crashes. The
  zap() takes care of this automatically.  Do not stick a typecast in
  the zap() command -- if something errors out on the above zap()
  command it likely has another error somewhere.

  Also ``my_malloc()'' , my_realloc() and my_free() should be used
  instead of malloc(), realloc() and free(), as they are much cleaner
  and have additional checks.  For an example, see the file "String.h"
  which is using the ``my_malloc()'' and my_free() functions.

  WARNING : Do not use free() to free memory allocated with 'new' or
  'delete' to free memory allocated with malloc. If you do, then results
  will be unpredictable!!

  5.  Pointers are problems

  Pointers are not required for general purpose programming. In modern
  languages like Java there is no support for pointers!! Pointers make
  the programs messy and programs using pointers are very hard to read.

  Avoid using pointers as much as possible and use references. Pointers
  are really a great pain. It is possible to write a application without
  using pointers.

  A reference is an alias; when you create a reference, you initialize
  it with the name of another object, the target. From the moment on,
  the reference acts as an alternative name of the target, and anything
  you do to the reference is really done to the target.

  Syntax of References: Declare a reference by writing the type,
  followed by the reference operator (&), followed by the reference
  name. References MUST be initialized at the time of creation.  For
  example -

  ______________________________________________________________________
          int             weight;
          int     & rweight = weight;

          DOG             aa;
          DOG & rDogRef = aa;
  ______________________________________________________________________



  Do's of references -

    Do use references to create an alias to an object

    Do initialize all references

    Do use references for high efficiency and performance of program.

    Do use const to protect references and pointers whenever possible.

  Do not's of references -

    IMPORTANT: Don't use references to NULL objects !!!!

    Don't confuse the address of operator & with reference operator !!
     The references are used in the declarations section (see Syntax of
     References above).

    Don't try to reassign a reference

    Don't use pointers if references will work

    Don't return a reference to a local object

    Don't pass by reference if the item referred to may go out of scope


  6.  Usage of my_malloc and my_free

  Try to avoid using malloc and realloc as much as possible and use new
  and ``zap''(delete). But sometimes you may need to use the "C" style
  memory allocations in "C++". Use the functions my_malloc() ,
  my_realloc() and my_free().  These functions do proper allocations and
  initialisations and try to prevent memory problems. Also these
  functions (in DEBUG mode) can keep track of memory allocated and print
  total memory usage before and after the program is run. This tells you
  if there are any memory leaks.

  The my_malloc and my_realloc is defined as below. It allocates little
  more memory (SAFE_MEM = 5) and initializes the space and if it cannot
  allocate it exits the program. The 'call_check(), remove_ptr()'
  functions are active only when DEBUG is defined in makefile and are
  assigned to ((void)0) i.e. NULL for non-debug production release. They
  enable the total-memory used tracing.
  ______________________________________________________________________
  void *local_my_malloc(size_t size, char fname[], int lineno)
  {
          size_t  tmpii = size + SAFE_MEM;
          void *aa = NULL;
          aa = (void *) malloc(tmpii);
          if (aa == NULL)
                  raise_error_exit(MALLOC, VOID_TYPE, fname, lineno);
          memset(aa, 0, tmpii);
          call_check(aa, tmpii, fname, lineno);
          return aa;
  }

  char *local_my_realloc(char *aa, size_t size, char fname[], int lineno)
  {
          remove_ptr(aa, fname, lineno);
          unsigned long tmpjj = 0;
          if (aa) // aa !=  NULL
                  tmpjj = strlen(aa);
          unsigned long tmpqq = size + SAFE_MEM;
          size_t  tmpii = sizeof (char) * (tmpqq);
          aa = (char *) realloc(aa, tmpii);
          if (aa == NULL)
                  raise_error_exit(REALLOC, CHAR_TYPE, fname, lineno);

          // do not memset!! memset(aa, 0, tmpii);
          aa[tmpqq-1] = 0;
          unsigned long kk = tmpjj;
          if (tmpjj > tmpqq)
                  kk = tmpqq;
          for ( ; kk < tmpqq; kk++)
                  aa[kk] = 0;
          call_check(aa, tmpii, fname, lineno);
          return aa;
  }

  ______________________________________________________________________